{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Sparse Kernel Machines\n",
    "author: Emmanuel Towner\n",
    "date: '2025-04-29'\n",
    "description: \"Blog Post 7\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Sparse Kernel Machines\n",
    "\n",
    "## Abstract\n",
    "\n",
    "[Link to source code](https://github.com/EpicET/EpicET.github.io/blob/main/posts/blog7/kernel_logistic.py)\n",
    "\n",
    "In this post, we explore sparse kernel logistic regression using radial basis function (RBF) kernels. We demonstrate how sparsity naturally emerges in these models, meaning only a subset of training points — the support vectors — contribute significantly to the final decision function. We examine how key hyperparameters like the regularization strength (𝜆) and the kernel sharpness (𝛾) influence model behavior, including sparsity, decision boundaries, and overfitting. Through several experiments, we visualize decision surfaces, investigate the impact of parameter choices, and show how kernel methods can capture nonlinear patterns effectively. To evaluate generalization, we conclude with an overfitting case study using ROC curves to compare training and test performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Classification and kernel code adapted from Prof. Phil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "\n",
    "def classification_data(n_points=300, noise=0.2, p_dims=2):\n",
    "\n",
    "    y = torch.arange(n_points) >= int(n_points / 2)\n",
    "    y = 1.0 * y\n",
    "    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n",
    "    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n",
    "\n",
    "    X = X - X.mean(dim=0, keepdim=True)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_classification_data(X, y, ax):\n",
    "    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n",
    "    targets = [0, 1]\n",
    "    markers = [\"o\", \",\"]\n",
    "    for i in range(2):\n",
    "        ix = y == targets[i]\n",
    "        ax.scatter(\n",
    "            X[ix, 0],\n",
    "            X[ix, 1],\n",
    "            s=20,\n",
    "            c=y[ix],\n",
    "            facecolors=\"none\",\n",
    "            edgecolors=\"darkgrey\",\n",
    "            cmap=\"BrBG\",\n",
    "            vmin=-1,\n",
    "            vmax=2,\n",
    "            alpha=0.8,\n",
    "            marker=markers[i],\n",
    "        )\n",
    "    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "X, y = classification_data(n_points=100, noise=0.4)\n",
    "plot_classification_data(X, y, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X_1, X_2, gamma):\n",
    "    return torch.exp(-gamma * torch.cdist(X_1, X_2) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Defined a rbf kernel machine based on Prof. Phil's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernel_logistic import KernelLogisticRegression\n",
    "\n",
    "KR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\n",
    "KR.fit(X, y, m_epochs = 500000, lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.0 * (KR.a > 0.001)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.abs(KR.a) > 0.001\n",
    "\n",
    "x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n",
    "x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n",
    "\n",
    "X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n",
    "\n",
    "x1 = X1.ravel()\n",
    "x2 = X2.ravel()\n",
    "\n",
    "X_ = torch.stack((x1, x2), dim=1)\n",
    "\n",
    "preds = KR.prediction(X_, recompute_kernel=True)\n",
    "preds = 1.0 * torch.reshape(preds, X1.size())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.contourf(\n",
    "    X1,\n",
    "    X2,\n",
    "    preds,\n",
    "    origin=\"lower\",\n",
    "    cmap=\"BrBG\",\n",
    "    vmin=2 * preds.min() - preds.max(),\n",
    "    vmax=2 * preds.max() - preds.min(),\n",
    ")\n",
    "plot_classification_data(X, y, ax)\n",
    "plt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Basic Experiments\n",
    "\n",
    "### Experiment 1: Large lambda\n",
    "\n",
    "This experiment shows that when 𝜆 is very large, there may be only one point in the training data with a wieght distinguishable from zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "KR = KernelLogisticRegression(rbf_kernel, lam=166, gamma=1)\n",
    "KR.fit(X, y, m_epochs=500000, lr=0.0001)\n",
    "\n",
    "ix = torch.abs(KR.a) > 0.001\n",
    "\n",
    "x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n",
    "x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n",
    "\n",
    "X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n",
    "\n",
    "x1 = X1.ravel()\n",
    "x2 = X2.ravel()\n",
    "\n",
    "X_ = torch.stack((x1, x2), dim=1)\n",
    "\n",
    "preds = KR.prediction(X_, recompute_kernel=True)\n",
    "preds = 1.0 * torch.reshape(preds, X1.size())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.contourf(\n",
    "    X1,\n",
    "    X2,\n",
    "    preds,\n",
    "    origin=\"lower\",\n",
    "    cmap=\"BrBG\",\n",
    "    vmin=2 * preds.min() - preds.max(),\n",
    "    vmax=2 * preds.max() - preds.min(),\n",
    ")\n",
    "plot_classification_data(X, y, ax)\n",
    "plt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "At a lambda of 166, there was only 1 data point in the center of blue region that had a distinguishable weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Experiment 2: Changing gamma\n",
    "\n",
    "This experiment is to whow that changing 𝛄 can result in wigglier descision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [1, 10, 20, 50, 100]  # List of gamma values to try\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4))  # One row, 5 subplots\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "\n",
    "    # Create and train the Kernel Logistic Regression model\n",
    "    KR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=gamma)\n",
    "    KR.fit(X, y, m_epochs=10000, lr=0.0001)\n",
    "\n",
    "    # Find support vectors (or approximate support vectors)\n",
    "    ix = torch.abs(KR.a) > 0.001\n",
    "\n",
    "    # Create grid for predictions\n",
    "    x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n",
    "    x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n",
    "    x1_r = X1.ravel()\n",
    "    x2_r = X2.ravel()\n",
    "    X_ = torch.stack((x1_r, x2_r), dim=1)\n",
    "\n",
    "    # Predict on the grid\n",
    "    preds = KR.prediction(X_, recompute_kernel=True)\n",
    "    preds = 1.0 * torch.reshape(preds, X1.size())\n",
    "\n",
    "    # Plot in the corresponding subplot\n",
    "    axs[i].set_title(f\"Gamma = {gamma}\")\n",
    "    axs[i].contourf(\n",
    "        X1,\n",
    "        X2,\n",
    "        preds,\n",
    "        origin=\"lower\",\n",
    "        cmap=\"BrBG\",\n",
    "        vmin=2 * preds.min() - preds.max(),\n",
    "        vmax=2 * preds.max() - preds.min(),\n",
    "    )\n",
    "    plot_classification_data(X, y, axs[i])\n",
    "    axs[i].scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "In this experiment, I test my model with a gammas of 1, 10, 20, 50, 100 to see with the decision boundaries looked like. I observed that is the gamma increased the boundaries became less clear and less circular. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Experiment 3: Dealing with nonlinear data\n",
    "\n",
    "This experiment shows that the kernelized model can still find the pattern within nonlinear data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "First, I generate nonlinear data for the model to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "X_nonlinear, y_nonlinear = classification_data(n_points=400, noise=0.9)\n",
    "plot_classification_data(X_nonlinear, y_nonlinear, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Then I run the usual graphing code to visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "KR = KernelLogisticRegression(rbf_kernel, lam=50, gamma=0.01)\n",
    "KR.fit(X_nonlinear, y_nonlinear, m_epochs=200000, lr=0.0001)\n",
    "\n",
    "ix = torch.abs(KR.a) > 0.001\n",
    "\n",
    "x1 = torch.linspace(X_nonlinear[:, 0].min() - 0.2, X_nonlinear[:, 0].max() + 0.2, 101)\n",
    "x2 = torch.linspace(X_nonlinear[:, 1].min() - 0.2, X_nonlinear[:, 1].max() + 0.2, 101)\n",
    "\n",
    "X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n",
    "\n",
    "x1 = X1.ravel()\n",
    "x2 = X2.ravel()\n",
    "\n",
    "X_ = torch.stack((x1, x2), dim=1)\n",
    "\n",
    "preds = KR.prediction(X_, recompute_kernel=True)\n",
    "preds = 1.0 * torch.reshape(preds, X1.size())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.contourf(\n",
    "    X1,\n",
    "    X2,\n",
    "    preds,\n",
    "    origin=\"lower\",\n",
    "    cmap=\"BrBG\",\n",
    "    vmin=2 * preds.min() - preds.max(),\n",
    "    vmax=2 * preds.max() - preds.min(),\n",
    ")\n",
    "plot_classification_data(X_nonlinear, y_nonlinear, ax)\n",
    "plt.scatter(X_nonlinear[ix, 0], X_nonlinear[ix, 1], facecolors=\"none\", edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate and split data\n",
    "X_over, y_over = classification_data(n_points=1000, noise=0.8)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.4, random_state=42)\n",
    "\n",
    "# Fit a model with high gamma (likely to overfit)\n",
    "KR = KernelLogisticRegression(rbf_kernel, lam=1e-5, gamma=1000)\n",
    "KR.fit(X_train, y_train, m_epochs=30000, lr=0.0001)\n",
    "\n",
    "# Predict probabilities on train and test\n",
    "with torch.no_grad():\n",
    "    y_train_scores = KR.prediction(X_train, recompute_kernel=True)\n",
    "    y_test_scores = KR.prediction(X_test, recompute_kernel=True)\n",
    "   \n",
    "# Convert to NumPy for sklearn\n",
    "y_train_np = y_train.numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "train_scores_np = y_train_scores.numpy()\n",
    "test_scores_np = y_test_scores.numpy()\n",
    "\n",
    "# Compute ROC curves\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train_np, train_scores_np)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test_np, test_scores_np)\n",
    "\n",
    "# Compute AUC\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr_train, tpr_train, label=f\"Train ROC (AUC = {roc_auc_train:.2f})\")\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test ROC (AUC = {roc_auc_test:.2f})\", linestyle='--')\n",
    "plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves: Overfitting Example\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "In this experiment, we simulate the problem of overfitting by training a kernel logistic regression model with extremely high gamma of 1000. This allows the model to fit the training data very closely, but it generalizes poorly to unseen test data.\n",
    "\n",
    "The (AUC) score for training set was near with perfect with 0.99 but the testing set was near 0.72. This large gap between train and test AUCs is a sign that there is overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this post, I explored the behavior of sparse kernel logistic regression with RBF kernels through several experiments:\n",
    "\n",
    "- Sparsity and Regularization: When using a large regularization parameter (𝜆 = 166), only 1 out of 100 training points had a weight (α) greater than 0.001, demonstrating extreme sparsity. In contrast, with a smaller 𝜆 (e.g., 0.1), a larger fraction of points acted as support vectors, contributing to the decision boundary.\n",
    "\n",
    "- Effect of Gamma (𝛾): By varying 𝛾 from 1 to 100, we observed that the decision boundary became increasingly complex and less smooth. For example, at 𝛾 = 1, the boundary was broad and circular, while at 𝛾 = 100, it became highly irregular, closely fitting the training data.\n",
    "\n",
    "- Nonlinear Data: On a challenging, noisy dataset (400 points, noise = 0.9), the kernelized model with 𝜆 = 50 and 𝛾 = 0.01 was still able to capture the underlying class structure, showing the flexibility of kernel methods for nonlinear patterns.\n",
    "\n",
    "- Overfitting: Training with a very high 𝛾 (1000) and low 𝜆 (1e-5) on a larger dataset (1000 points, noise = 0.8), the model achieved a near-perfect training AUC of 0.99, but the test AUC dropped to 0.72. This large gap quantitatively demonstrates overfitting: the model fits the training data extremely well but fails to generalize.\n",
    "\n",
    "\n",
    "I spent a lot of time readjusting and making the kernel logistic regression especially with the grad function. So I would say my debugging skills with dimension/shape related errors. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
