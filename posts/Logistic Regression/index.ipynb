{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Implementing Logistic Regression\n",
    "author: Emmanuel Towner\n",
    "date: '2025-04-9'\n",
    "description: \"Blog Post 5\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Implementing Logistic Regression\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Link to [logistic source code](https://github.com/EpicET/EpicET.github.io/blob/main/posts/blog5/logistic.py).\n",
    "\n",
    "This notebook implements a logistic regression model with a gradient descent optimizer. The model is trained on a synthetic dataset, and in where it is optimized at each iteration and also updates loss. There are four experiments conducted. The first experiment tested the model on with vanilla gradient descent plotting the loss per iteration and a decision boundary. The second experiment compared the loss per iterations between the model when using the vanilla descent and when using momentum descent. The third experiment was to overfit the model to the training data and compare it to the accuracy of the model on the test data. The fourth experiment was to test the model on a heart disease prediction dateset. The dataset was split into training, validation, and test data.  The model was trained on the training data and the loss computed for both training and validation. The model was then evaluated on the test data, and the accuracy was reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from logistic import LogisticRegression, GradientDescentOptimizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n",
    "    \n",
    "    y = torch.arange(n_points) >= int(n_points/2)\n",
    "    y = 1.0*y\n",
    "    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n",
    "    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = classification_data(noise = 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "This code was adapted from previous notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "def plot_classification_data(X, y, ax):\n",
    "    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n",
    "    targets = [0, 1]\n",
    "    markers = [\"o\" , \",\"]\n",
    "    for i in range(2):\n",
    "        ix = y == targets[i]\n",
    "        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n",
    "    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n",
    "\n",
    "def draw_line(w, X, y, x_min, x_max, **kwargs):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    plot_classification_data(X, y, ax)\n",
    "    x = torch.linspace(x_min, x_max, 101)\n",
    "    y = -(w[0]*x + w[2])/w[1]\n",
    "    l = ax.plot(x, y, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Experiment 1: Vanilla Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression() \n",
    "opt = GradientDescentOptimizer(LR)\n",
    "\n",
    "loss_vec = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    loss = LR.loss(X, y) \n",
    "    loss_vec.append(loss)\n",
    "     \n",
    "    opt.step(X, y, alpha = 0.1, beta = 0)\n",
    "\n",
    "  \n",
    "plt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\n",
    "plt.semilogx()\n",
    "labs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_line(opt.model.w, X, y, x_min = -0.5, x_max = 1.5, color = \"black\", linestyle = \"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The code above implements a vanilla gradient descent with logistic regression. It is run through a training loop, while keeping track of the loss and storing it in an array called loss_vec. Using loss_vec, I plot a graph showing the loss over gradient iterations. The second graph shows the decision boundary of the data. \n",
    "- Loss: This appears to hit convergence with a loss <0.2 at ~10,000 iterations. \n",
    "- Decision Boundary: The decision boundary appears to correctly classify the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Experiment 2: Benefits of Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = classification_data(n_points=700, noise = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Vanilla gradient descent\n",
    "LR = LogisticRegression() \n",
    "opt = GradientDescentOptimizer(LR)\n",
    "\n",
    "loss_vec1 = []\n",
    "iterations = 5000\n",
    "alpha = 0.1\n",
    "for _ in range(iterations):\n",
    "    loss = LR.loss(X, y)\n",
    "    loss_vec1.append(loss)\n",
    "    opt.step(X, y, alpha, beta=0.0)  \n",
    "\n",
    "# Momentum version\n",
    "LR = LogisticRegression() \n",
    "opt = GradientDescentOptimizer(LR)\n",
    "\n",
    "loss_vec2 = []\n",
    "for _ in range(iterations):\n",
    "    loss = LR.loss(X, y)\n",
    "    loss_vec2.append(loss)\n",
    "    opt.step(X, y, alpha, beta=0.9) \n",
    "\n",
    "\n",
    "plt.plot(torch.arange(1, len(loss_vec1) + 1), loss_vec1, color=\"black\", label=f\"alpha={alpha}, beta=0.0\")\n",
    "plt.plot(torch.arange(1, len(loss_vec2) + 1), loss_vec2, color=\"orange\", label=f\"alpha={alpha}, beta=0.9\")\n",
    "plt.semilogx()\n",
    "plt.xlabel(\"Number of gradient descent iterations\")\n",
    "plt.ylabel(\"Loss (binary cross entropy)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "The code above plots the loss over iterations for both vanilla gradient descent and momentum gradient descent (beta = 0.9) over 5000 iterations with an alpha of 0.1. The loss of both descents begin at ~0.64, but momentum gradient descent drops lower a lot qucker and reaches convergence at ~1000 iterations. Vanilla gradient descent takes ~5000 iterations to reach convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Experiment 3: Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = classification_data(n_points= 60, noise = 0.3, p_dims = 100)\n",
    "X_test, y_test = classification_data(n_points= 60, noise = 0.3, p_dims = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression() \n",
    "opt = GradientDescentOptimizer(LR)\n",
    "\n",
    "for _ in range(200): \n",
    "    opt.step(X_train, y_train, alpha = 0.01, beta = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training accuracy\n",
    "train_predictions = LR.predict(X_train)\n",
    "train_accuracy = (train_predictions == y_train).float().mean().item()\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_predictions = LR.predict(X_test)\n",
    "test_accuracy = (test_predictions == y_test).float().mean().item()\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "This experiments creates equal data for train and test set with 60 points, 100 dimensions and a noise of 0.3. The model(alpha = 0.01, beta = 0.9) is trained on the training set for 200 iterations,leading to overfitting and a training accuracy of a 100%. The accuracy of the model on the test set is 96.67%, which is rather good given the model was overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Experiment 4: Performance ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍on ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍empirical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data\n",
    "\n",
    "For my external dataset, I used a [heart prediction dataset](https://www.kaggle.com/datasets/shantanugarg274/heart-prediction-dataset-quantum) from Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"shantanugarg274/heart-prediction-dataset-quantum\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "data_path = path + \"/Heart Prediction Quantum Dataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Here I found a dataset on heart disease prediction from Kaggle. The data was in 1 csv file with 7 columns representing age, gender, blood pressure, cholesterol, heart rate, quantum pattern feature, and heart disease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df.drop(\"HeartDisease\", axis=1).values\n",
    "y_data = df[\"HeartDisease\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Since I intend to predict heart disease, I removed that column from the main dataset in part in the target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "X_data = torch.tensor(X_data, dtype=torch.float32)\n",
    "y_data = torch.tensor(y_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "The data across features widely varied in range and so I used sci-kit learn's StandardScaler to standardize the both datasets and then converted them into tensors.  The model was trained on the training set and the loss computed for both training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "I also used train_test_split to split the 60% data into training, 20% in validation, and test sets. I had to do this in 2 steps since the you cannot split the data into 3 sets at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Vanilla gradient descent\n",
    "LRV = LogisticRegression() \n",
    "optv = GradientDescentOptimizer(LRV)\n",
    "\n",
    "v_loss_train = []\n",
    "v_loss_val = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.01\n",
    "for _ in range(iterations):\n",
    "    train_loss = LRV.loss(X_train, y_train)\n",
    "    val_loss = LRV.loss(X_val, y_val)\n",
    "    v_loss_train.append(train_loss.item())\n",
    "    v_loss_val.append(val_loss.item())\n",
    "    optv.step(X_train, y_train, alpha, beta=0.0) \n",
    "\n",
    "# Momentum version\n",
    "LRM = LogisticRegression() \n",
    "optm = GradientDescentOptimizer(LRM)\n",
    "\n",
    "m_loss_train = []\n",
    "m_loss_val = []\n",
    "for _ in range(iterations):\n",
    "    train_loss = LRM.loss(X_train, y_train)\n",
    "    val_loss = LRM.loss(X_val, y_val)\n",
    "    m_loss_train.append(train_loss.item())\n",
    "    m_loss_val.append(val_loss.item())\n",
    "    optm.step(X_train, y_train, alpha, beta=0.9)  \n",
    "\n",
    "ax[0].plot(torch.arange(1, iterations + 1), v_loss_train, color=\"black\", label=\"Vanilla\")\n",
    "ax[0].plot(torch.arange(1, iterations + 1), m_loss_train, color=\"orange\", label=\"Momentum\")\n",
    "ax[0].set_xlabel(\"Iterations\")\n",
    "ax[0].set_ylabel(\"Training Loss\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Training Loss\")\n",
    "\n",
    "ax[1].plot(torch.arange(1, iterations + 1), v_loss_val, color=\"black\", label=\"Vanilla\")\n",
    "ax[1].plot(torch.arange(1, iterations + 1), m_loss_val, color=\"orange\", label=\"Momentum\")\n",
    "ax[1].set_xlabel(\"Iterations\")\n",
    "ax[1].set_ylabel(\"Validation Loss\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Validation Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "The 2 graphs above show the loss over iterations for both training and validation. The model was trained for 100 iterations with an alpha of 0.01 and mommentum beta of 0.9. The graphs were similar to each other and the only the momentum loss(~0.2 for training and ~0.3 for validation) reached convergence at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = LRM.loss(X_test, y_test)\n",
    "print(f\"Testing Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# Testing Accuracy\n",
    "test_predictions = LRM.predict(X_test)\n",
    "test_accuracy = (test_predictions == y_test).float().mean().item()\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "My model had a losss of 0.38 and accuracy of 81% on the test set. While the accuracy is was ok, the loss was a bit high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this blog post, I implemented a logistic regression model with a momentum based optimizer. It successfully achieved convergence with vanilla gradient(beta = 0), then proved the with momentum it could increase speedup. The model was able to overfit the training data and achieve a high accuracy on the test set. The model was also able to learn the heart disease dataset, reach convergence, and achieve a good accuracy. The loss was a bit high, but maybe with more iterations it could be improved.\n",
    "In this blog post, I improved my data preprocesssing skills. I learned the purpose of and how to use sci-kit learn's StandardScaler to standardize features. I gained experience converting the recieved data into a useful data types. As always my graphing skills improved with matplotlib and creating line plots. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
