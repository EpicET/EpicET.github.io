{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Advanced Optimization\n",
    "author: Emmanuel Towner\n",
    "date: '2025-05-07'\n",
    "description: \"Blog Post 8\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Advanced Optimization: Newtonâ€™s Method and Adam\n",
    "\n",
    "## Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from newton_logistic import LogisticRegression, NewtonOptimizer, GradientDescentOptimizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Newton Experiments\n",
    "\n",
    "### Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "For my external dataset, I used a [heart prediction dataset](https://www.kaggle.com/datasets/shantanugarg274/heart-prediction-dataset-quantum) from Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"shantanugarg274/heart-prediction-dataset-quantum\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "data_path = path + \"/Heart Prediction Quantum Dataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Here I found a dataset on heart disease prediction from Kaggle. The data was in 1 csv file with 7 columns representing age, gender, blood pressure, cholesterol, heart rate, quantum pattern feature, and heart disease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df.drop(\"HeartDisease\", axis=1).values\n",
    "y_data = df[\"HeartDisease\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Since I intend to predict heart disease, I removed that column from the main dataset in part in the target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "X_data = torch.tensor(X_data, dtype=torch.float32)\n",
    "y_data = torch.tensor(y_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The data across features widely varied in range and so I used sci-kit learn's StandardScaler to standardize the both datasets and then converted them into tensors.  The model was trained on the training set and the loss computed for both training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "I also used train_test_split to split the 60% data into training and 40% in test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Experiment 1: Alpha Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LR = LogisticRegression() \n",
    "opt = NewtonOptimizer(LR)\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    train_loss = LR.loss(X_train, y_train)\n",
    "    loss_train.append(train_loss.item())\n",
    "    \n",
    "    test_loss = LR.loss(X_test, y_test)\n",
    "    loss_test.append(test_loss.item())\n",
    "    \n",
    "    opt.step(X_train, y_train, alpha)\n",
    "\n",
    "    \n",
    "# Plotting the loss\n",
    "fig, ax = plt.subplots(figsize=(6, 6))  # Single axes, adjust figsize if needed\n",
    "ax.plot(torch.arange(1, iterations + 1), loss_train, color=\"black\")\n",
    "ax.plot(torch.arange(1, iterations + 1), loss_test, color=\"orange\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss vs Iterations\")\n",
    "ax.legend([\"Train Loss\", \"Test Loss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "With an alpha of 0.1 both training and testing loss converge in between 20 and 40 iterations. Testing loss converges slightly earlier and at a slightly higher loss than training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Experiment 2: Newton vs Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LR = LogisticRegression() \n",
    "optn = NewtonOptimizer(LR)\n",
    "\n",
    "n_loss_train = []\n",
    "n_loss_test = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    train_loss = LR.loss(X_train, y_train)\n",
    "    n_loss_train.append(train_loss.item())\n",
    "\n",
    "    test_loss = LR.loss(X_test, y_test)\n",
    "    n_loss_test.append(test_loss.item())\n",
    "    \n",
    "    optn.step(X_train, y_train, alpha)\n",
    "\n",
    "LR = LogisticRegression() \n",
    "optg = GradientDescentOptimizer(LR)\n",
    "\n",
    "g_loss_train = []\n",
    "g_loss_test = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    train_loss = LR.loss(X_train, y_train)\n",
    "    g_loss_train.append(train_loss.item())\n",
    "\n",
    "    test_loss = LR.loss(X_test, y_test)\n",
    "    g_loss_test.append(test_loss.item())\n",
    "\n",
    "    optg.step(X_train, y_train, alpha, beta=0.9)\n",
    "\n",
    "# Plotting the loss\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))  # Single axes, adjust figsize if needed\n",
    "ax[0].plot(torch.arange(1, iterations + 1), n_loss_train, color=\"black\")\n",
    "ax[0].plot(torch.arange(1, iterations + 1), n_loss_test, color=\"orange\")\n",
    "ax[0].set_xlabel(\"Iterations\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Newton Optimizer Loss\")\n",
    "ax[0].legend([\"Train Loss\", \"Test Loss\"])\n",
    "\n",
    "ax[1].plot(torch.arange(1, iterations + 1), g_loss_train, color=\"black\")\n",
    "ax[1].plot(torch.arange(1, iterations + 1), g_loss_test, color=\"orange\")\n",
    "ax[1].set_xlabel(\"Iterations\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_title(\"Gradient Descent Loss\")\n",
    "ax[1].legend([\"Train Loss\", \"Test Loss\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "In this plot, we see that with the Newton Optimizer the loss convergences faster than the gradient descent optimizer. For Newton optimizer, the loss reaches convergence at ~30-35 iterations whereas for the gradient optimizer the loss converges at ~50-60. They both share an alpha of 0.1 while gradient descent has a beta of 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Experiment 3: Large Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression() \n",
    "opts = NewtonOptimizer(LR)\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.9\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    train_loss = LR.loss(X_train, y_train)\n",
    "    loss_train.append(train_loss.item())\n",
    "    \n",
    "    test_loss = LR.loss(X_test, y_test)\n",
    "    loss_test.append(test_loss.item())\n",
    "    \n",
    "    opts.step(X_train, y_train, alpha)\n",
    "\n",
    "    \n",
    "# Plotting the loss\n",
    "fig, ax = plt.subplots(figsize=(6, 6))  # Single axes, adjust figsize if needed\n",
    "ax.plot(torch.arange(1, iterations + 1), loss_train, color=\"black\")\n",
    "ax.plot(torch.arange(1, iterations + 1), loss_test, color=\"orange\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss vs Iterations\")\n",
    "ax.legend([\"Train Loss\", \"Test Loss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adam import LogisticRegression, AdamOptimizer\n",
    "\n",
    "LR = LogisticRegression() \n",
    "adam = AdamOptimizer(LR)\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.9\n",
    "\n",
    "n = X_train.shape[0]\n",
    "for _ in range(iterations):\n",
    "    \n",
    "    \n",
    "    train_loss = LR.loss(X_train, y_train)\n",
    "    loss_train.append(train_loss.item())\n",
    "    \n",
    "    test_loss = LR.loss(X_test, y_test)\n",
    "    loss_test.append(test_loss.item())\n",
    "    \n",
    "    adam.step(X_train, y_train, alpha)\n",
    "\n",
    "    \n",
    "# Plotting the loss\n",
    "fig, ax = plt.subplots(figsize=(6, 6))  # Single axes, adjust figsize if needed\n",
    "ax.plot(torch.arange(1, iterations + 1), loss_train, color=\"black\")\n",
    "ax.plot(torch.arange(1, iterations + 1), loss_test, color=\"orange\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss vs Iterations\")\n",
    "ax.legend([\"Train Loss\", \"Test Loss\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
