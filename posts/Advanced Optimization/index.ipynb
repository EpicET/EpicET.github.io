{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Advanced Optimization\n",
    "author: Emmanuel Towner\n",
    "date: '2025-05-07'\n",
    "description: \"Blog Post 8\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Advanced Optimization: Newton’s Method and Adam\n",
    "\n",
    "## Abstract\n",
    "\n",
    "[Link to Newton Optimizer code](https://github.com/EpicET/EpicET.github.io/blob/main/posts/Advanced%20Optimization/newton_logistic.py).\n",
    "\n",
    "[Link to Adam Optimizer code](https://github.com/EpicET/EpicET.github.io/blob/main/posts/Advanced%20Optimization/adam.py).\n",
    "\n",
    "In this blog post, I explore advanced optimization techniques—Newton’s Method and the Adam optimizer—in the context of logistic regression applied to heart disease prediction. Through a series of experiments on a Kaggle heart dataset, I investigate the convergence behavior, sensitivity to learning rates, and efficiency of these methods. The three optimizers I work with are Newton's method, Adam, and gradient descent. The goal is to understand how different optimization algorithms influence model performance and training dynamics in a binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from newton_logistic import LogisticRegression, NewtonOptimizer, GradientDescentOptimizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Newton Experiments\n",
    "\n",
    "### Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "For my external dataset, I used a [heart prediction dataset](https://www.kaggle.com/datasets/shantanugarg274/heart-prediction-dataset-quantum) from Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"shantanugarg274/heart-prediction-dataset-quantum\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "data_path = path + \"/Heart Prediction Quantum Dataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Here I found a dataset on heart disease prediction from Kaggle. The data was in 1 csv file with 7 columns representing age, gender, blood pressure, cholesterol, heart rate, quantum pattern feature, and heart disease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df.drop(\"HeartDisease\", axis=1).values\n",
    "y_data = df[\"HeartDisease\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Since I intend to predict heart disease, I removed that column from the main dataset in part in the target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "X_data = torch.tensor(X_data, dtype=torch.float32)\n",
    "y_data = torch.tensor(y_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The data across features widely varied in range and so I used sci-kit learn's StandardScaler to standardize the both datasets and then converted them into tensors.  The model was trained on the training set and the loss computed for both training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "I also used train_test_split to split the 60% data into training and 40% in test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Experiment 1: Alpha Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LR = LogisticRegression() \n",
    "opt = NewtonOptimizer(LR)\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    train_loss = LR.loss(X_train, y_train)\n",
    "    loss_train.append(train_loss.item())\n",
    "    \n",
    "    test_loss = LR.loss(X_test, y_test)\n",
    "    loss_test.append(test_loss.item())\n",
    "    \n",
    "    opt.step(X_train, y_train, alpha)\n",
    "\n",
    "    \n",
    "# Plotting the loss\n",
    "fig, ax = plt.subplots(figsize=(6, 6))  # Single axes, adjust figsize if needed\n",
    "ax.plot(torch.arange(1, iterations + 1), loss_train, color=\"black\")\n",
    "ax.plot(torch.arange(1, iterations + 1), loss_test, color=\"orange\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss vs Iterations\")\n",
    "ax.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "With an alpha of 0.1 both training and testing loss converge in between 20 and 40 iterations. Testing loss converges slightly earlier and at a slightly higher loss than training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Experiment 2: Newton vs Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LR = LogisticRegression() \n",
    "optn = NewtonOptimizer(LR)\n",
    "\n",
    "n_loss_train = []\n",
    "n_loss_test = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    train_loss = LR.loss(X_train, y_train)\n",
    "    n_loss_train.append(train_loss.item())\n",
    "\n",
    "    test_loss = LR.loss(X_test, y_test)\n",
    "    n_loss_test.append(test_loss.item())\n",
    "    \n",
    "    optn.step(X_train, y_train, alpha)\n",
    "\n",
    "LR = LogisticRegression() \n",
    "optg = GradientDescentOptimizer(LR)\n",
    "\n",
    "g_loss_train = []\n",
    "g_loss_test = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    train_loss = LR.loss(X_train, y_train)\n",
    "    g_loss_train.append(train_loss.item())\n",
    "\n",
    "    test_loss = LR.loss(X_test, y_test)\n",
    "    g_loss_test.append(test_loss.item())\n",
    "\n",
    "    optg.step(X_train, y_train, alpha, beta=0.9)\n",
    "\n",
    "# Plotting the loss\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))  # Single axes, adjust figsize if needed\n",
    "ax[0].plot(torch.arange(1, iterations + 1), n_loss_train, color=\"black\")\n",
    "ax[0].plot(torch.arange(1, iterations + 1), n_loss_test, color=\"orange\")\n",
    "ax[0].set_xlabel(\"Iterations\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Newton Optimizer Loss\")\n",
    "ax[0].legend([\"Train Loss\", \"Test Loss\"])\n",
    "\n",
    "ax[1].plot(torch.arange(1, iterations + 1), g_loss_train, color=\"black\")\n",
    "ax[1].plot(torch.arange(1, iterations + 1), g_loss_test, color=\"orange\")\n",
    "ax[1].set_xlabel(\"Iterations\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_title(\"Gradient Descent Loss\")\n",
    "ax[1].legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "In this plot, we see that with the Newton Optimizer the loss convergences faster than the gradient descent optimizer. For Newton optimizer, the loss reaches convergence at ~30-35 iterations whereas for the gradient optimizer the loss converges at ~50-60. They both share an alpha of 0.1 while gradient descent has a beta of 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Experiment 3: Large Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression() \n",
    "opts = NewtonOptimizer(LR)\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "\n",
    "iterations = 100\n",
    "alpha = 1.5\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    train_loss = LR.loss(X_train, y_train)\n",
    "    loss_train.append(train_loss.item())\n",
    "    \n",
    "    test_loss = LR.loss(X_test, y_test)\n",
    "    loss_test.append(test_loss.item())\n",
    "    \n",
    "    print(\"Train Loss: \", train_loss.item())\n",
    "    opts.step(X_train, y_train, alpha)\n",
    "\n",
    "    \n",
    "# Plotting the loss\n",
    "fig, ax = plt.subplots(figsize=(6, 6))  \n",
    "ax.plot(torch.arange(1, iterations + 1), loss_train, color=\"black\")\n",
    "ax.plot(torch.arange(1, iterations + 1), loss_test, color=\"orange\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss vs Iterations\")\n",
    "ax.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = torch.linspace(0.01, 5.0, 50)  # Range of alpha values\n",
    "final_losses = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    try:\n",
    "        # Re-initialize model for each run\n",
    "        LR = LogisticRegression()\n",
    "        # LR.w = torch.zeros(X_train.shape[1])  # or random init\n",
    "        opt = NewtonOptimizer(LR)\n",
    "        \n",
    "        for _ in range(10):  # Few steps to test stability\n",
    "            opt.step(X_train, y_train, alpha)\n",
    "        \n",
    "        loss = LR.loss(X_train, y_train).item()\n",
    "        final_losses.append(loss)\n",
    "    except Exception as e:\n",
    "        # Divergence or singular matrix → record as NaN\n",
    "        final_losses.append(float('nan'))\n",
    "\n",
    "# Convert to tensor for plotting\n",
    "final_losses = torch.tensor(final_losses)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(alphas, final_losses, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Alpha (step size)\")\n",
    "plt.ylabel(\"Final Train Loss\")\n",
    "plt.title(\"Effect of Step Size on Newton's Method Stability\")\n",
    "plt.yscale(\"log\")  # Optional: helps show divergence\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "This model converges for all alpha's from up to 1. When alpha goes above 1, the model returns an error relating to the calculation the  weight update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "### Test Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adam import AdamOptimizer\n",
    "\n",
    "# Adam optimizer test\n",
    "\n",
    "\n",
    "iterations = 100\n",
    "batch_size = 40\n",
    "\n",
    "alphas = [0.001, 0.01, 0.03, 0.1]\n",
    "adam_losses_dict = {}\n",
    "for alpha in alphas:\n",
    "    LRA = LogisticRegression()\n",
    "    adam = AdamOptimizer(LRA)\n",
    "    adam_losses = []\n",
    "    for _ in range(iterations):\n",
    "        train_loss = LRA.loss(X_train, y_train)\n",
    "        adam_losses.append(train_loss.item())\n",
    "        adam.optim(X_train, y_train, batch_size=batch_size, alpha=alpha)\n",
    "    adam_losses_dict[alpha] = adam_losses\n",
    "\n",
    "\n",
    "# SGD (minibatch) test for different step sizes\n",
    "\n",
    "grad_losses_dict = {}\n",
    "\n",
    "for alpha in alphas:\n",
    "    LRG = LogisticRegression()\n",
    "    grad = GradientDescentOptimizer(LRG)\n",
    "    grad_losses = []\n",
    "    for _ in range(iterations):\n",
    "        train_loss = LRG.loss(X_train, y_train)\n",
    "        grad_losses.append(train_loss.item())\n",
    "        grad.step(X_train, y_train, alpha=alpha, beta=0.0, mini_batch=True)\n",
    "    grad_losses_dict[alpha] = grad_losses\n",
    "    \n",
    "# Plotting the loss\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))  \n",
    "for alpha, losses in adam_losses_dict.items():\n",
    "    ax[0].plot(torch.arange(1, iterations + 1), losses, label=f\"Adam (α={alpha})\")\n",
    "ax[0].set_xlabel(\"Iterations\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Adam Optimizer Loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "for alpha, losses in grad_losses_dict.items():\n",
    "    ax[1].plot(torch.arange(1, iterations + 1), losses, label=f\"Grad (α={alpha})\")\n",
    "ax[1].set_xlabel(\"Iterations\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_title(\"Gradient Descent Loss\")\n",
    "ax[1].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "This test compares the performance of the Adam optimizer and mini-batch gradient descent on logistic regression. For each optimizer, the code tests four learning rates (α = 0.001, 0.01, 0.03, 0.1) over 100 iterations, recording the training loss at each step. Adam uses a batch size of 40, while SGD uses mini-batches with no momentum. The results show that Adam consistently shows faster and smoother convergence across all learning rates compared to SGD, which converges more slowly and is more sensitive to the choice of α. This demonstrates Adam’s advantage in stability and speed for this classification task. Both struggle to converge on low alphas such as 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Newton vs Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "target_loss = 0.3\n",
    "alpha = 0.03\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "LRA = LogisticRegression() \n",
    "adam = AdamOptimizer(LRA)\n",
    "\n",
    "adam_loss = float('inf')  # large initial value\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "while adam_loss > target_loss:\n",
    "    # Compute current loss\n",
    "    train_loss = LRA.loss(X_train, y_train)\n",
    "    adam_loss = train_loss.item()\n",
    "    # print(f\"[Adam] Current loss: {adam_loss:.4f}\")\n",
    "    \n",
    "    # Perform one round of updates\n",
    "    adam.optim(X_train, y_train, batch_size=40, alpha=alpha)\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Adam optimizer took {end - start:.4f} seconds\")\n",
    "print(f\"Final Adam loss: {LRA.loss(X_train, y_train).item():.4f}\")\n",
    "\n",
    "# Newton Optimizer\n",
    "LRN = LogisticRegression()\n",
    "newton = NewtonOptimizer(LRN)\n",
    "\n",
    "newton_loss = float('inf')  # large initial value\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "while newton_loss > target_loss:\n",
    "    # Compute current loss\n",
    "    train_loss = LRN.loss(X_train, y_train)\n",
    "    newton_loss = train_loss.item()\n",
    "    # print(f\"[Newton] Current loss: {newton_loss:.4f}\")\n",
    "    # Perform one round of updates\n",
    "    newton.step(X_train, y_train, alpha)\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Newton optimizer took {end - start:.4f} seconds\")\n",
    "print(f\"Final Newton loss: {LRN.loss(X_train, y_train).item():.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "In the above code, the model was trained using Adam and Newton optimizer with the goal of reducing the training loss below a target value of 0.3. Both optimizers iteratively updated the model parameters, with Adam using adaptive learning rates and mini-batches, while Gradient Descent used momentum and mini-batch updates. The results showed that Adam achieved a lower final loss (0.2379) but took longer to converge (0.1134 seconds), whereas Gradient Descent was significantly faster (0.0062 seconds) but only marginally met the loss target (0.2989). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The experiments show that different optimizers can perform better in different situations. Newton’s Method achieved faster convergence than standard gradient descent in terms of iteration count but when alpha became really large it was unable to converge. Gradient descent methods, while slower, offer greater stability across a wider range of learning rates. Adam strikes a balance, providing both great loss rate and rapid convergence. It was faster the mini-batch gradient descent but slower than Newton. In both situations, it had the better loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
