{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Auditing Bias\n",
    "author: Emmanuel Towner\n",
    "date: '2025-03-12'\n",
    "description: \"Blog Post 3\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing Bias\n",
    "\n",
    "## Abstract\n",
    "\n",
    "My goal was to make a predicition of employment status based on various demographic excluding race using a subset of data from the American Community Service focused on Massachusetts residents in 2023. Based on the data of the 58,500 residents, only half were employed. Most of the time men, people without disabilities, and people who born abroad or with no citizensip had higher proportions of employment. The model I used was sklearns Decision Tree Classifier because results are easy to interpret. I tuned complexity by I using GridSearchCV which cross-validated that the best depth out of the numbers I provided was 10 which overall accuracy 0.82. The different group accuracies weren't that much different. Auditing my model showed that white people lead in PPV and FNR while Asians lead in TPR and FPR rates. In these summary I left races 5 and 6 because their data often had many missing values. Based on those values, my model failed approximate error balance and statistical parity but satisfied calibration. The plot that used the fixed PPV values and p values to graph feasible FPR and FNR combinations between Black and White residents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\n",
    "import numpy as np\n",
    "\n",
    "STATE = \"MA\" \n",
    "\n",
    "data_source = ACSDataSource(survey_year='2023', # Get more recent data\n",
    "                            horizon='1-Year', \n",
    "                            survey='person')\n",
    "\n",
    "acs_data = data_source.get_data(states=[STATE], download=True)\n",
    "\n",
    "acs_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No RELP avaiable \n",
    "possible_features=['AGEP', 'SCHL', 'MAR', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n",
    "acs_data[possible_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Feature selection And Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from Professor Phil's code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmploymentProblem = BasicProblem(\n",
    "    features=features_to_use,\n",
    "    target='ESR',\n",
    "    target_transform=lambda x: x == 1,\n",
    "    group='RAC1P',\n",
    "    preprocess=lambda x: x,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")\n",
    "\n",
    "features, label, group = EmploymentProblem.df_to_numpy(acs_data)\n",
    "\n",
    "for obj in [features, label, group]:\n",
    "  print(obj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features, label, group, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X_train, columns = features_to_use)\n",
    "df[\"race\"] = group_train\n",
    "df[\"label\"] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method allows the conversion of race numbers to more helpful categorical labels. Some of them have been shortened because they were too long. \n",
    "1. SPAA - \"American Indian and Alaska Native tribes specified, or American Indian or AlaskaNative, not specified and no other races\". \n",
    "2. NPI - Native Hawaiian and Other Pacific Islander alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_race(df: pd.DataFrame):\n",
    "    df = df.sort_values(by='race')\n",
    "    df['race'] = df['race'].replace({1: \"White\", 2: \"Black\", 3: \"N. American\", 4:\"N. Alaskan\", \n",
    "                        5:\"SPAA\", \n",
    "                        6:'Asian', 7: 'NPI', 8:'Other', 9: 'Multi'})\n",
    "\n",
    "    df['race'] = pd.Categorical(df['race'])\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to the original for calculations and copying\n",
    "relabeled = df.copy() \n",
    "relabeled = convert_race(relabeled)\n",
    "relabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The number of indiviuals in this df are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of indiviuals in this df are\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The proportion of employed individuals are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = df[df['label'] == True][[\"label\"]].size # 29703\n",
    "total = df['label'].size # 58500\n",
    "emp_prop = emp / total\n",
    "print(\"The proportion of employed indiviuals are\", emp_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The population of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabeled.groupby('race')[['label']].aggregate('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, I might consider whether groups with less than 30 labels provide an adequate sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The proportion of employed people in each group are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabeled.groupby('race')[['label']].aggregate('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Intersectional Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14, 4))\n",
    "\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\"]\n",
    "plt1 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"SEX\", palette=colors, ax=ax[0])\n",
    "plt1.set_xlabel(\"Race\")\n",
    "plt1.set_ylabel(\"Employment\")\n",
    "handles, _ = plt1.get_legend_handles_labels()\n",
    "plt1.legend(handles=handles, title='Sex', labels=['Male', 'Female'])\n",
    "\n",
    "plt2 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"DIS\", palette=colors, ax=ax[1])\n",
    "plt2.set_xlabel(\"Race\")\n",
    "plt2.set_ylabel(\"Employment\")\n",
    "handles, _ = plt2.get_legend_handles_labels()\n",
    "plt2.legend(handles=handles, title='Disability', labels=['Has disability', 'No disability'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we see intersectionality between employment based on the sex of the person or whether they have a disability. As we can see in both graphs generally male and able bodied people are more likely to be employed. Multiple factors can contribute to this desparity. One is that women and people with disabilities are discriminated against by employers. Another is also considering how many of them are applying for jobs. For women, even with more of them joining the workforce, they may be more likely to doing childcare at home. Because disabilites is such a broad it's hard which may be capable of working and those that aren't. This probably a big factor why relatively few are employed. Something I noticed is that for Black Americans, Native Americans and SPAA more females are employed than males. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt3 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"CIT\")\n",
    "plt3.set_xlabel(\"Race\")\n",
    "plt3.set_ylabel(\"Employment\")\n",
    "handles, _ = plt3.get_legend_handles_labels()\n",
    "status = ['Born in US', 'Born in US Territories', 'Born Abroad', 'US by naturalization', 'Not a citizen']\n",
    "plt3.legend(handles=handles, title='Citizenship', labels=status, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph, we see that for people born in the U.S or it's territories have lower percentage employment than people born abroad or by naturalization. I suspect this is might be due to the smaller populations and people to who move here are more likely to have specific work purposes.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n",
    "\n",
    "The decision tree classifier was one of the recommedations and scikit learn said it was easy to interpret. In order to find the best depth for I used GridSearchCV which cross validated the best paramaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [1, 3, 5, 10]\n",
    "}\n",
    "\n",
    "model = DecisionTreeClassifier() \n",
    "grid = GridSearchCV(model, param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params\", grid.best_params_)\n",
    "\n",
    "best_dtree = grid.best_estimator_\n",
    "\n",
    "best_dtree.predict(X_train)\n",
    "best_dtree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best depth was 10 and gave us a solid accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_dtree.predict(X_test)\n",
    "score = best_dtree.score(X_test, y_test)\n",
    "\n",
    "print(\"Test score\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Audit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the PPV, the false negative and false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "ppv = tp / (tp + fp)\n",
    "fnr = fn / (fn + tp)\n",
    "fpr = tp / (tp + tn)\n",
    "\n",
    "print(\"Overall PPV:\", ppv)\n",
    "print(\"Overall false negative rate:\", fnr)\n",
    "print(\"Overall false positive rate:\", fpr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this the model is pretty good at predicting who's not employed but seems to overestimate the amount of employed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By-Group Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the accuracy by group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df[\"race\"].unique() # Need race as numbers in order to compare with group_test\n",
    "audit = pd.DataFrame(groups, columns=[\"race\"])\n",
    "\n",
    "accuracies = []\n",
    "for group in groups:\n",
    "   accuracy = (pred == y_test)[group_test == group].mean()\n",
    "   accuracies.append(accuracy)\n",
    "\n",
    "audit[\"accuracy\"] = accuracies\n",
    "audit = convert_race(audit)\n",
    "audit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basides NPI and SPAA, the accuracies are rather similar to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the PPV, the false negative and false positive rate by group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppvs = []\n",
    "tprs = []\n",
    "fnrs = []\n",
    "fprs = []\n",
    "\n",
    "for group in groups:\n",
    "    tp = int(0)\n",
    "    fp = int(0)\n",
    "    tn = int(0)\n",
    "    fn = int(0)\n",
    "    for n, m, grp in zip(y_test, pred, group_test):\n",
    "        if(grp == group):\n",
    "            if m == n:\n",
    "                if n == True:\n",
    "                    tp += 1\n",
    "                if n == False:\n",
    "                    tn += 1\n",
    "            if m != n:\n",
    "                if n == True:\n",
    "                    fn += 1\n",
    "                if n == False:\n",
    "                    fp += 1\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    ppvs.append(ppv)\n",
    "    fnr = fn / (fn + tp)  if (fn + tp) > 0 else 0\n",
    "    fnrs.append(fnr)\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0 \n",
    "    fprs.append(fpr)\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    tprs.append(tpr)\n",
    "    \n",
    "\n",
    "audit[\"ppv\"] = ppvs\n",
    "audit[\"tpr\"] = tprs\n",
    "audit[\"fpr\"] = fprs\n",
    "audit[\"fnr\"] = fnrs\n",
    "audit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Measures\n",
    "\n",
    "Using code adapted from [from Machine Learning Master](https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/) and sklearn documentation, I used a sci-kit learn's calibration curve to diagnose the calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve \n",
    "\n",
    "y_prob = best_dtree.predict_proba(X_test)[:, 1] # Gets the probability of positive values\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n",
    "\n",
    "avgs = []\n",
    "actuals = []\n",
    "for group in groups:\n",
    "    # P(Y = 1 | S = {0,1}, R = r) \n",
    "    avg_pred_prob = y_prob[group_test == group].mean()\n",
    "    avgs.append(avg_pred_prob)\n",
    "    actual_employment_rate = (y_test == 1)[group_test == group].mean()\n",
    "    actuals.append(actual_employment_rate)\n",
    "\n",
    "calibrate= pd.DataFrame(groups, columns=[\"race\"])\n",
    "calibrate[\"employed\"] = actuals\n",
    "calibrate[\"avg_predicted_probs\"] = avgs\n",
    "print(calibrate)\n",
    "\n",
    "cal = plt\n",
    "cal.plot([0, 1], [0, 1], linestyle='--') # Actual values\n",
    "cal.plot(prob_pred, prob_true, marker='.') # Predicted values\n",
    "cal.title(\"Model Calibration\")\n",
    "cal.xlabel(\"Average Probability\")\n",
    "cal.ylabel(\"Fraction of Positive\")\n",
    "cal.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph and table shows that my model is nearly calibrated, meaning that the model usually predicts probabilites that are the same as the real probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate error balance rate: Looking at the table dataframe above we can see that the model does not meet approximate error rate balance for groups. The groups differ in true and false positive rates. The code below double checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row1 in audit.iterrows():\n",
    "    equal = True;\n",
    "    race1 = row1['race']\n",
    "    tpr1 = row1['tpr']\n",
    "    fpr1 = row1['fpr']\n",
    "    for j, row2 in audit.iterrows():\n",
    "        race2 = row2['race']\n",
    "        tpr2 = row2['tpr']\n",
    "        fpr2 = row2['fpr']\n",
    "        if(tpr1 != tpr2 or fpr1 != fpr2):\n",
    "            equal = False\n",
    "            print(f\"{race1} did not have an equal TPR or FPR as {race2}\")\n",
    "            break\n",
    "    if(equal != True):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical parity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "for group in groups:\n",
    "    prob = (pred == True)[group_test == group].mean()\n",
    "    probs.append(prob)\n",
    "\n",
    "parity= pd.DataFrame(groups, columns=[\"race\"])\n",
    "parity[\"prob\"] = probs\n",
    "parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not meet statistical parity which means not all groups have an equal change of achieving favorable odds. Therefore we can assume that the probability of predicting employment is not independent of race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feasible FNR and FPR Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add prevalance to data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit[\"p\"] = (1 + (audit[\"tpr\"] / audit[\"fpr\"]) * ((1 - audit[\"ppv\"])/(audit[\"ppv\"]))) ** -1\n",
    "audit[\"p\"] = audit[\"p\"].fillna(0)\n",
    "audit\n",
    "print(audit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I plot the feasibility of FPR and FNR for Black and White groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Filter to only include Black and White groups\n",
    "filtered = audit[audit[\"race\"].isin([\"Black\", \"White\"])]\n",
    "\n",
    "orange_color = \"#E69F00\"\n",
    "black_color = \"#000000\"\n",
    "\n",
    "# A cleaner table\n",
    "feasible = filtered[[\"race\", \"fpr\", \"fnr\", \"p\"]].copy() \n",
    "lines = []\n",
    "\n",
    "# Make fixed ppv based on the black ppv\n",
    "fixed_ppv = filtered.loc[filtered[\"race\"] == \"Black\", \"ppv\"].values[0]\n",
    "fnr_range = np.linspace(0, 1, 100)\n",
    "\n",
    "# Compute feasible FPR for different FNR values\n",
    "for i, row in feasible.iterrows():\n",
    "    race = row[\"race\"]\n",
    "    p = row[\"p\"]  \n",
    "    \n",
    "    fprs = (p / (1 - p)) * ((1 - fixed_ppv) / fixed_ppv) * (1 - fnr_range)\n",
    "\n",
    "    for fnr, fpr in zip(fnr_range, fprs):\n",
    "        lines.append({\"race\": race, \"fnr\": fnr, \"fpr\": fpr})\n",
    "\n",
    "lines_df = pd.DataFrame(lines)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot observed (fnr, fpr)\n",
    "for i, row in feasible.iterrows():\n",
    "    color = orange_color if row[\"race\"] == \"White\" else black_color\n",
    "    plt.scatter(row[\"fnr\"], row[\"fpr\"], color=color)\n",
    "\n",
    "# Plot feasible (fnr, fpr) line\n",
    "for race, color in zip([\"White\", \"Black\"], [orange_color, black_color]):\n",
    "    line = lines_df[lines_df[\"race\"] == race]\n",
    "    plt.plot(line[\"fnr\"], line[\"fpr\"], color=color)\n",
    "\n",
    "plt.xlabel(\"False Negative Rate\")\n",
    "plt.ylabel(\"False Positive Rate\")\n",
    "plt.title(\"Feasible (FNR, FPR) combinations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this plot, to get equal false positive rates we would need to reduce $\\mathrm{FNR}_w$ by about 0.04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Discussion\n",
    "\n",
    "1. If we assume that these institutions don't already have employment data, then it could be used by finacial and government institutions. For example, a bank may use employment status prediction to determine whether to give a loan to someone. The government could use its prediction to determine where they need to improve in providing employment opportunities. Landlords could use this information to decide if they want to accept a tenant or how much they would to charge them. \n",
    "\n",
    "2. Since my model proved to not be fair, then commercial or government industries would be misguided in their decisions. This algorithm may be used to unreasonably target certain populations with advertisements. Some people may be missclassified which would either disallow them from certian opportunities or stop them from getting the help they need. \n",
    "\n",
    "3. My model failed tests of statistical parity and error rate balance which means that it does display problematic biases.\n",
    "\n",
    "4. I think transparency might be an issue, if the algorithm is not shared to the public as people would unknownigly be judged by a algorithm.  I'm unsure if this bias but smaller populations have very weird data points that probably don't accurate reflect what's going on. I feel like there could be some social bias or judgement as result of this algorithm, especially for people who aren't employed or predicted to not be employed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
