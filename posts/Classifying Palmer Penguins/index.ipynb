{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Classifying Palmer Penguins\n",
    "author: Emmanuel Towner\n",
    "date: '2025-02-12'\n",
    "image: \"penguins.jpg\"\n",
    "description: \"Blog Post 1\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "fig-cap-location: margin\n",
    "code-fold: false\n",
    "---\n",
    "# Classifying Palmer Penguins\n",
    "\n",
    "![Image source: \\@gabednick](https://www.gabemednick.com/post/penguin/featured.png)\n",
    "\n",
    "## Abstract\n",
    "\n",
    "In this blog post, I take the Palmer Penguins dataset and try to determine the best features to be used to determine the species of a penguin based on its measurements. Firstly, I create two figures and a table to analysize the relationships between features. Then I use sci-kit learns feature selection with chi-squared tests to pick 2 numerical features and 1 categorical feature. Then using those features, I train and test a logistic regression model. The model was fairly accurate but to understand the results better, I plot the decision regions and use a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\n",
    "train = pd.read_csv(train_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This is code from Professor Phil's website. It removes the columns that we don't use and NA values, converts the categorical feature columns into \"one-hot encoded\" 0-1 columns and saves the dataframe X_train. Also, \"Species\" is coded with the LabelEncoder and is saved as y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train[\"Species\"])\n",
    "\n",
    "def prepare_data(df):\n",
    "  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n",
    "  df = df[df[\"Sex\"] != \".\"]\n",
    "  df = df.dropna()\n",
    "  y = le.transform(df[\"Species\"])\n",
    "  df = df.drop([\"Species\"], axis = 1)\n",
    "  df = pd.get_dummies(df)\n",
    "  return df, y\n",
    "\n",
    "X_train, y_train = prepare_data(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what the columns look like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures\n",
    "\n",
    "I created two graphs each 2 quantative columns and 1 qualitative columns. Plot 1 shows the relationship with the body mass and flipper length between different penguin species. Plot 2 shows the difference in Culmen Length and depth across different penguin species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unencoded columns for easier graphing.\n",
    "qual = train[[\"Island\", \"Sex\", \"Species\"]].dropna()\n",
    "\n",
    "# Shorten species label for the legend\n",
    "qual[\"Species\"] = qual[\"Species\"].apply(lambda x: \"Chinstrap\" if x == \"Chinstrap penguin (Pygoscelis antarctica)\" \n",
    "                                         else (\"Gentoo\" if x == \"Gentoo penguin (Pygoscelis papua)\" \n",
    "                                               else \"Adelie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 4))\n",
    "   \n",
    "p1 = sns.scatterplot(X_train, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", hue=qual[\"Species\"], ax = ax[0])\n",
    "p2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=qual[\"Species\"], ax = ax[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left side: Plot 1. The Gentoo penguins tend to be the biggest while Chinstrap and Adelie seem to have a great overlap in size. The Adelie seem to have slightly more variety in mass given a flipper length compared to Chinstrap. This is features seem decent for selection because there is some correlation between species and body size but they are not easily distinguishable by species.\n",
    "\n",
    "Right side: Plot 2. Based on the graph, the each specie of penguin seemed to have distinct beaks. The Adelie have greatest culmen depth but shortest length. The Gentoo have longer culmens but not as much depth and the Chinstrap lie in the middle with medium to large culmens. These are good features because there is a clear distinction in culmen sizes between species of penquins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table\n",
    "Now I create a summary table of the penguins measurements based on clutch completetion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = X_train[[\"Clutch Completion_Yes\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n",
    "table.groupby(\"Clutch Completion_Yes\").aggregate(['min', 'median', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1: This table shows the biggest difference between penguins that had a full clutch and those that did not was the weight. Most of the penguins the produced two eggs weighed about 300 grams more. There is most likely not a causation but there a may be a correlation. Since clutch completion does not seem impact this data very much, it may not be a feature worth looking at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection \n",
    "\n",
    "Here I use sci-kits SelectKBest to pick the 3 feautures I'm going to use for my model. I had to seperate feature selection because all 3 selected features would be numerical. SelectKBest selects the k best features based on user specified scoring function. I used chi-squared since my feautres were meant for classification and they were non-negative.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Selecting 2 numerical feature\n",
    "quant = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n",
    "sel1 = SelectKBest(chi2, k=2)\n",
    "sel1.fit_transform(X_train[quant], y_train)\n",
    "f1 = sel1.get_feature_names_out()\n",
    "\n",
    "# Selecting 1 categorical feature\n",
    "qual = [\"Clutch Completion_Yes\", \"Clutch Completion_No\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Sex_FEMALE\", \"Sex_MALE\"]\n",
    "sel2 = SelectKBest(chi2, k=1)\n",
    "sel2.fit_transform(X_train[qual], y_train)\n",
    "f2 = sel2.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is so that I can get all the variations of the categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat(f1, cat):\n",
    "    cols = list(f1)\n",
    "    clutch = [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n",
    "    island = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n",
    "    sex = [\"Sex_FEMALE\", \"Sex_MALE\"]\n",
    "    \n",
    "    if cat in clutch: return cols + clutch\n",
    "    if cat in island: return cols + island\n",
    "    if cat in sex: return cols + sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = get_feat(f1, f2[0])\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So despite the culmen sizes being appearing to be better features, based on the statistical tests the flipper length and body mass were better features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The model is trained on the data with features determined from above. I had to use StandardScalar to avoid a convergence error. I used the Logistic Regression model as it is a good fit for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train[cols], y_train)\n",
    "pipe.score(X_train[cols], y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "X_test, y_test = prepare_data(test)\n",
    "pipe.score(X_test[cols], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Decision Regions \n",
    "\n",
    "Most of this code is adapted from Prof. Phil's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def plot_regions(model, X, y):\n",
    "    \n",
    "    x0 = X[X.columns[0]]\n",
    "    x1 = X[X.columns[1]]\n",
    "    qual_features = X.columns[2:]\n",
    "    \n",
    "    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 3))\n",
    "\n",
    "    # create a grid\n",
    "    grid_x = np.linspace(x0.min(),x0.max(),501)\n",
    "    grid_y = np.linspace(x1.min(),x1.max(),501)\n",
    "    xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "    XX = xx.ravel()\n",
    "    YY = yy.ravel()\n",
    "\n",
    "    for i in range(len(qual_features)):\n",
    "      XY = pd.DataFrame({\n",
    "          X.columns[0] : XX,\n",
    "          X.columns[1] : YY\n",
    "      })\n",
    "\n",
    "      for j in qual_features:\n",
    "        XY[j] = 0\n",
    "\n",
    "      XY[qual_features[i]] = 1\n",
    "\n",
    "      p = model.predict(XY)\n",
    "      p = p.reshape(xx.shape)\n",
    "      \n",
    "      \n",
    "      # use contour plot to visualize the predictions\n",
    "      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n",
    "      \n",
    "      ix = X[qual_features[i]] == 1\n",
    "      # plot the data\n",
    "      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n",
    "      \n",
    "      axarr[i].set(xlabel = X.columns[0], \n",
    "            ylabel  = X.columns[1], \n",
    "            title = qual_features[i])\n",
    "      \n",
    "      patches = []\n",
    "      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n",
    "        patches.append(Patch(color = color, label = spec))\n",
    "\n",
    "      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n",
    "      \n",
    "      plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regions for training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regions(pipe, X_train[cols], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regions for testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regions(pipe, X_test[cols], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these decision plots, we see that our model is pretty successful determing of the difference between Gentoo and Adelie on the Biscoe and Torgersen islands. However, on the Dream island, there is a mixture of Gentoo and Chinstrap where the model the struggles to distinguish between the two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test_pred = pipe.predict(X_test[cols])\n",
    "confusion_matrix(y_test, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, this shows that model struggled the most with Gentoo and Chinstrap. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The model had very similar training and testing accuracy of 0.89. This means the model is fairly solid at predicting penguins based on the flipper lenth, body mass and island. Based on the decision regions, the model struggled to correctly determine the difference between Chinstrap and Gentoo on the Island Dream. It seems that those 2 species are of similar sizes and therefore are hard to differentiate. \n",
    "\n",
    "During this blog post, the two biggest things were I learned about different tools for feature selection and the need to normalize data before putting it in a model. In terms of feature selection, sci-kit learn provides various methods whether variance, statistical tests or recursion. I went with statistical test as it seemed more thorough then determining a random variance yet not as complex as the recursion. For normalizing data, I learned how to deal with a convergence error when trainig a model. Through some investigation I came to understand I recieved that error  because some of the features may not normally distributed. I used sci-kit recommended scaling function to standardize the data and everything was fine after that. Finally, I got more practice with modifing and selecting data from dataframes in general."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
