{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Overfitting and Double Descents\n",
    "author: Emmanuel Towner\n",
    "date: '2025-04-23'\n",
    "description: \"Blog Post 6\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Overfitting and Double Descents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Abstract\n",
    "\n",
    "In this blog post, I explore deep learning technique of overfitting and the resulting double descent. First, I created the a linear regression model and an overparameterized optimizer that could fit the model to the data. For the first visualization, I tested model prediction after having it fitted to the data. Then I used my model to calculate the number of corruptions in a image. To measure model performance, I calculated mean squared error (MSE) and graphed how it changes as the number features increased. In this visualization, I observed double descent on the testing set. I found the optimal number of features was beyond the interpolation threshold at 188 features and a MSE of 280."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Random Features Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sig(x): \n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def square(x): \n",
    "    return x**2\n",
    "\n",
    "class RandomFeatures:\n",
    "    \"\"\"\n",
    "    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n",
    "\n",
    "    phi = RandomFeatures(n_features = 10)\n",
    "    phi.fit(X_train)\n",
    "    X_train_phi = phi.transform(X_train)\n",
    "    X_test_phi = phi.transform(X_test)\n",
    "\n",
    "    model.fit(X_train_phi, y_train)\n",
    "    model.score(X_test_phi, y_test)\n",
    "\n",
    "    It is important to fit the feature map once on the training set and zero times on the test set. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, activation = sig):\n",
    "        self.n_features = n_features\n",
    "        self.u = None\n",
    "        self.b = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n",
    "        self.b = torch.rand((self.n_features), dtype = torch.float64) \n",
    "\n",
    "    def transform(self, X: torch.Tensor):\n",
    "        return self.activation(X @ self.u + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Part 0\n",
    "If p > n then (X^T X)^{-1} is not invertible. The matrix is rank deficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Testing MyLinearRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyLinearRegression import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\n",
    "X = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\n",
    "y = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n",
    "\n",
    "phi = RandomFeatures(n_features= 100)\n",
    "phi.fit(X)\n",
    "X_train_features = phi.transform(X)\n",
    "\n",
    "LR = MyLinearRegression()\n",
    "opt = OverParameterizedLinearRegressionOptimizer(LR)\n",
    "opt.fit(X_train_features, y)\n",
    "pred = LR.predict(X_train_features)\n",
    "line = pred.numpy() \n",
    "\n",
    "plt.scatter(X, y, color='darkgrey', label='Data')\n",
    "plt.plot(X, line, color='lightblue', label=\"Predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Tested Linear Regrssion model on 1D data. The model seemd to produce accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Corrupted Flower Images\n",
    "\n",
    "Cells 5 to 9 were adapted from Prof. Phil's code as background for the corrupted flower images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "dataset = load_sample_images()     \n",
    "X = dataset.images[1]\n",
    "X = zoom(X,.2) #decimate resolution\n",
    "X = X.sum(axis = 2)\n",
    "X = X.max() - X \n",
    "X = X / X.max()\n",
    "flower = torch.tensor(X, dtype = torch.float64)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.imshow(flower)\n",
    "off = ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupted_image(im, mean_patches = 5): \n",
    "    n_pixels = im.size()\n",
    "    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n",
    "    num_added = 0\n",
    "\n",
    "    X = im.clone()\n",
    "\n",
    "    for _ in torch.arange(num_pixels_to_corrupt.item()): \n",
    "        \n",
    "        try: \n",
    "            x = torch.randint(0, n_pixels[0], (2,))\n",
    "\n",
    "            x = torch.randint(0, n_pixels[0], (1,))\n",
    "            y = torch.randint(0, n_pixels[1], (1,))\n",
    "\n",
    "            s = torch.randint(5, 10, (1,))\n",
    "            \n",
    "            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n",
    "\n",
    "            # place patch in base image X\n",
    "            X[x:x+s.item(), y:y+s.item()] = patch\n",
    "            num_added += 1\n",
    "\n",
    "            \n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    return X, num_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = corrupted_image(flower, mean_patches = 50)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.imshow(X.numpy(), vmin = 0, vmax = 1)\n",
    "ax.set(title = f\"Corrupted Image: {y} patches\")\n",
    "off = plt.gca().axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "\n",
    "X = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\n",
    "y = torch.zeros(n_samples, dtype = torch.float64)\n",
    "for i in range(n_samples): \n",
    "    X[i], y[i] = corrupted_image(flower, mean_patches = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X.reshape(n_samples, -1)\n",
    "# X.reshape(n_samples, -1).size()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Performance of Model\n",
    "\n",
    "Here I assess the performance of my model by calculating the mean squared error (MSE) and plotting it against the number of features for both training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 200\n",
    "\n",
    "train_loss_vec = []\n",
    "test_loss_vec = []\n",
    "\n",
    "n = X_train.size()[0] \n",
    "\n",
    "# Loop over different numbers of random features\n",
    "for i in range(n_features):\n",
    "    # Initialize the random feature transformation\n",
    "    phi = RandomFeatures(n_features = i, activation = square)\n",
    "    phi.fit(X_train) # Fit the random features to the training data\n",
    "    \n",
    "    # Transform both training and test data using the fitted feature map\n",
    "    X_train_phi = phi.transform(X_train)\n",
    "    X_test_phi = phi.transform(X_test)\n",
    "    \n",
    "    # Initialize the linear regression model and optimizer\n",
    "    LR = MyLinearRegression()\n",
    "    opt = OverParameterizedLinearRegressionOptimizer(LR)\n",
    "    \n",
    "    # Fit the model to the transformed training data\n",
    "    opt.fit(X_train_phi, y_train)\n",
    "    \n",
    "    # Compute training and testing loss (mse)\n",
    "    train_loss = LR.loss(X_train_phi, y_train).item()\n",
    "    test_loss = LR.loss(X_test_phi, y_test).item()\n",
    "    \n",
    "    # Append the losses to the respective lists\n",
    "    train_loss_vec.append(train_loss)\n",
    "    test_loss_vec.append(test_loss)\n",
    "   \n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))  \n",
    "\n",
    "# Plot training loss\n",
    "ax[0].scatter(range(n_features), train_loss_vec, color='darkgrey')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].axvline(n, color='black', linewidth = 1)\n",
    "ax[0].set_xlabel(\"Number of features\")\n",
    "ax[0].set_ylabel(\"Mean Squared Error (Training)\")\n",
    "\n",
    "# Plot testing loss\n",
    "ax[1].scatter(range(n_features), test_loss_vec, color='darkred')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].axvline(n, color='black', linewidth = 1)\n",
    "ax[1].set_xlabel(\"Number of features\")\n",
    "ax[1].set_ylabel(\"Mean Squared Error (Testing)\")\n",
    "\n",
    "# Find the best number of features with the lowest test loss\n",
    "best_idx = torch.tensor(test_loss_vec).argmin().item() # convert vector to tensor and get index of minimum\n",
    "best_features = best_idx + 1 \n",
    "best_loss = test_loss_vec[best_idx]\n",
    "\n",
    "print(f\"Lowest test loss: {best_loss:.4f} at {best_features} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "For the training data, I saw that as the number of features increased, the MSE decreased. In particular at 100 features, which was was the interpolation threshold, the MSE dropped from about from approximately $10^{-4}$ to $10^{-20}$. \n",
    "For the testing data, I observed that the MSE decreased as the number of features increased, but then increased again after a certain point, indicating overfitting. Following the innterpolation threshold, the error started to decrease again. The best error rate for the testing set was around 280 at 188 features. This was after the interpolation threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "My most important finding was my model does achieve double descent. For training loss, the mse continued to trend down as the number features increased. For testing loss, there was an initial decrease in loss but then the loss rose when it reached near the interpolation threshold due to overfitting. After the interpolation threshold, the loss decreased again reachinga a second minimum\n",
    "Based on my results, 188 features achieved the lowest mse of 280 was achieved. \n",
    "\n",
    "\n",
    "This blog post was my first time making a fit function for a machine learning model. I also improved my overall graphing skills like a graphing lines on scatter plots and for the interpolation methods. Also, logging the scales is new."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
