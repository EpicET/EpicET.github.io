{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Sparse Kernel Machines\n",
    "author: Emmanuel Towner\n",
    "date: '2025-04-29'\n",
    "description: \"Blog Post 7\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Sparse Kernel Machines\n",
    "\n",
    "## Abstract\n",
    "\n",
    "[Link to source code](https://github.com/EpicET/EpicET.github.io/blob/main/posts/blog7/kernel_logistic.py)\n",
    "\n",
    "In this post, we explore sparse kernel logistic regression using radial basis function (RBF) kernels. We demonstrate how sparsity naturally emerges in these models, meaning only a subset of training points — the support vectors — contribute significantly to the final decision function. We examine how key hyperparameters like the regularization strength (𝜆) and the kernel sharpness (𝛾) influence model behavior, including sparsity, decision boundaries, and overfitting. Through several experiments, we visualize decision surfaces, investigate the impact of parameter choices, and show how kernel methods can capture nonlinear patterns effectively. To evaluate generalization, we conclude with an overfitting case study using ROC curves to compare training and test performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Classification and kernel code adapted from Prof. Phil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "\n",
    "def classification_data(n_points=300, noise=0.2, p_dims=2):\n",
    "\n",
    "    y = torch.arange(n_points) >= int(n_points / 2)\n",
    "    y = 1.0 * y\n",
    "    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n",
    "    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n",
    "\n",
    "    X = X - X.mean(dim=0, keepdim=True)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_classification_data(X, y, ax):\n",
    "    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n",
    "    targets = [0, 1]\n",
    "    markers = [\"o\", \",\"]\n",
    "    for i in range(2):\n",
    "        ix = y == targets[i]\n",
    "        ax.scatter(\n",
    "            X[ix, 0],\n",
    "            X[ix, 1],\n",
    "            s=20,\n",
    "            c=y[ix],\n",
    "            facecolors=\"none\",\n",
    "            edgecolors=\"darkgrey\",\n",
    "            cmap=\"BrBG\",\n",
    "            vmin=-1,\n",
    "            vmax=2,\n",
    "            alpha=0.8,\n",
    "            marker=markers[i],\n",
    "        )\n",
    "    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "X, y = classification_data(n_points=100, noise=0.4)\n",
    "plot_classification_data(X, y, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X_1, X_2, gamma):\n",
    "    return torch.exp(-gamma * torch.cdist(X_1, X_2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernel_logistic import KernelLogisticRegression\n",
    "\n",
    "KR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\n",
    "KR.fit(X, y, m_epochs = 500000, lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.0 * (KR.a > 0.001)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.abs(KR.a) > 0.001\n",
    "\n",
    "x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n",
    "x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n",
    "\n",
    "X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n",
    "\n",
    "x1 = X1.ravel()\n",
    "x2 = X2.ravel()\n",
    "\n",
    "X_ = torch.stack((x1, x2), dim=1)\n",
    "\n",
    "preds = KR.prediction(X_, recompute_kernel=True)\n",
    "preds = 1.0 * torch.reshape(preds, X1.size())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.contourf(\n",
    "    X1,\n",
    "    X2,\n",
    "    preds,\n",
    "    origin=\"lower\",\n",
    "    cmap=\"BrBG\",\n",
    "    vmin=2 * preds.min() - preds.max(),\n",
    "    vmax=2 * preds.max() - preds.min(),\n",
    ")\n",
    "plot_classification_data(X, y, ax)\n",
    "plt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Basic Experiments\n",
    "\n",
    "### Experiment 1: Large lambda\n",
    "\n",
    "When 𝜆 is very large, there may be only one point in the training data with a wieght distinguishable from zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "KR = KernelLogisticRegression(rbf_kernel, lam=1000, gamma=1)\n",
    "KR.fit(X, y, m_epochs=500000, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.abs(KR.a) > 0.001\n",
    "\n",
    "x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n",
    "x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n",
    "\n",
    "X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n",
    "\n",
    "x1 = X1.ravel()\n",
    "x2 = X2.ravel()\n",
    "\n",
    "X_ = torch.stack((x1, x2), dim=1)\n",
    "\n",
    "preds = KR.prediction(X_, recompute_kernel=True)\n",
    "preds = 1.0 * torch.reshape(preds, X1.size())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.contourf(\n",
    "    X1,\n",
    "    X2,\n",
    "    preds,\n",
    "    origin=\"lower\",\n",
    "    cmap=\"BrBG\",\n",
    "    vmin=2 * preds.min() - preds.max(),\n",
    "    vmax=2 * preds.max() - preds.min(),\n",
    ")\n",
    "plot_classification_data(X, y, ax)\n",
    "plt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Experiment 2: Changing gamma\n",
    "\n",
    "Change 𝛄 can result in wigglier descision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    KR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=i)\n",
    "    KR.fit(X, y, m_epochs=200000, lr=0.0001)\n",
    "\n",
    "    ix = torch.abs(KR.a) > 0.001\n",
    "\n",
    "    x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n",
    "    x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n",
    "\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n",
    "\n",
    "    x1 = X1.ravel()\n",
    "    x2 = X2.ravel()\n",
    "\n",
    "    X_ = torch.stack((x1, x2), dim=1)\n",
    "\n",
    "    preds = KR.prediction(X_, recompute_kernel=True)\n",
    "    preds = 1.0 * torch.reshape(preds, X1.size())\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.contourf(\n",
    "        X1,\n",
    "        X2,\n",
    "        preds,\n",
    "        origin=\"lower\",\n",
    "        cmap=\"BrBG\",\n",
    "        vmin=2 * preds.min() - preds.max(),\n",
    "        vmax=2 * preds.max() - preds.min(),\n",
    "    )\n",
    "    plot_classification_data(X, y, ax)\n",
    "    plt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Experiment 3: Dealing with nonlinear data\n",
    "\n",
    "When the data has a nonlinear pattern (e.g. as generated by sklearn.datasets.make_moons), kernel methods with appropriate parameters can find this pattern effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "KR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=1)\n",
    "KR.fit(X, y, m_epochs=200000, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.abs(KR.a) > 0.001\n",
    "\n",
    "x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n",
    "x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n",
    "\n",
    "X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n",
    "\n",
    "x1 = X1.ravel()\n",
    "x2 = X2.ravel()\n",
    "\n",
    "X_ = torch.stack((x1, x2), dim=1)\n",
    "\n",
    "preds = KR.prediction(X_, recompute_kernel=True)\n",
    "preds = 1.0 * torch.reshape(preds, X1.size())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.contourf(\n",
    "    X1,\n",
    "    X2,\n",
    "    preds,\n",
    "    origin=\"lower\",\n",
    "    cmap=\"BrBG\",\n",
    "    vmin=2 * preds.min() - preds.max(),\n",
    "    vmax=2 * preds.max() - preds.min(),\n",
    ")\n",
    "plot_classification_data(X, y, ax)\n",
    "plt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate and split data\n",
    "X, y = classification_data(n_points=300, noise=0.3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Fit a model with low regularization (likely to overfit)\n",
    "KR = KernelLogisticRegression(rbf_kernel, lam=1e-5, gamma=10)\n",
    "KR.fit(X_train, y_train, m_epochs=300000, lr=0.0001)\n",
    "\n",
    "# Predict probabilities on train and test\n",
    "with torch.no_grad():\n",
    "    y_train_scores = KR.prediction(X_train)\n",
    "    y_test_scores = KR.prediction(X_test)\n",
    "\n",
    "# Convert to NumPy for sklearn\n",
    "y_train_np = y_train.numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "train_scores_np = y_train_scores.numpy()\n",
    "test_scores_np = y_test_scores.numpy()\n",
    "\n",
    "# Compute ROC curves\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train_np, train_scores_np)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test_np, test_scores_np)\n",
    "\n",
    "# Compute AUCs\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr_train, tpr_train, label=f\"Train ROC (AUC = {auc_train:.2f})\")\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test ROC (AUC = {auc_test:.2f})\", linestyle='--')\n",
    "plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves: Overfitting Example\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "In this experiment, we simulate the problem of overfitting by training a kernel logistic regression model with extremely low regularization (very small lambda). This allows the model to fit the training data very closely, but it generalizes poorly to unseen test data.\n",
    "\n",
    "We evaluate performance using ROC curves and the Area Under the Curve (AUC) metric. A large gap between train and test AUCs is a strong signal of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- Summary of blog post\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
