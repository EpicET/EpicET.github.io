[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I‚Äôm Emmanuel Towner, a computer science major at Middlebury College. This site shows work from my courses and solo projects."
  },
  {
    "objectID": "posts/kernel/index.html",
    "href": "posts/kernel/index.html",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "In this post, I explore sparse kernel logistic regression using radial basis function (RBF) kernels. I demonstrate how sparsity naturally emerges in these models, meaning only a subset of training points contribute significantly to the final decision function. I examine how key hyperparameters like the regularization strength (ùúÜ) and the bandwidth (ùõæ) influence model behavior, including sparsity, decision boundaries, and overfitting. Through several experiments, I visualize decision surface, investigate the impact of parameter choices, and show how kernel methods can capture nonlinear patterns effectively. To evaluate generalization, I conclude with an overfitting case study using ROC curves to compare training and test performance.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nClassification (cell 15) and kernel (cell 16) code adapted from Prof.¬†Phil.\n\n\nCode\nimport torch\nfrom matplotlib import pyplot as plt\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n\ndef classification_data(n_points=300, noise=0.2, p_dims=2):\n\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    X = X - X.mean(dim=0, keepdim=True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(\n            X[ix, 0],\n            X[ix, 1],\n            s=20,\n            c=y[ix],\n            facecolors=\"none\",\n            edgecolors=\"darkgrey\",\n            cmap=\"BrBG\",\n            vmin=-1,\n            vmax=2,\n            alpha=0.8,\n            marker=markers[i],\n        )\n    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points=100, noise=0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma * torch.cdist(X_1, X_2) ** 2)\n\n\n\n\nCode\nfrom kernel_logistic import KernelLogisticRegression\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\n\n\nCode\n(1.0 * (KR.a &gt; 0.001)).mean()\n\n\ntensor(0.5300)\n\n\n\n\nCode\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\nx2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim=1)\n\npreds = KR.prediction(X_, recompute_kernel=True)\npreds = 1.0 * torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(\n    X1,\n    X2,\n    preds,\n    origin=\"lower\",\n    cmap=\"BrBG\",\n    vmin=2 * preds.min() - preds.max(),\n    vmax=2 * preds.max() - preds.min(),\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis experiment shows that when ùúÜ is very large, there may be only one point in the training data with a wieght distinguishable from zero.\n\n\nCode\nKR = KernelLogisticRegression(rbf_kernel, lam=166, gamma=1)\nKR.fit(X, y, m_epochs=20000, lr=0.0001)\n\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\nx2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim=1)\n\npreds = KR.prediction(X_, recompute_kernel=True)\npreds = 1.0 * torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(\n    X1,\n    X2,\n    preds,\n    origin=\"lower\",\n    cmap=\"BrBG\",\n    vmin=2 * preds.min() - preds.max(),\n    vmax=2 * preds.max() - preds.min(),\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAt a lambda of ~166, there was only 1 data point in the center of brown region that had a distinguishable weight.\n\n\n\nThis experiment is to show that changing ùõÑ can result in wigglier descision boundaries.\n\n\nCode\ngammas = [1, 10, 20, 50, 100]  # List of gamma values to try\nfig, axs = plt.subplots(1, 5, figsize=(20, 4))  # One row, 5 subplots\n\nfor i, gamma in enumerate(gammas):\n    KR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=gamma)\n    KR.fit(X, y, m_epochs=10000, lr=0.0001)\n\n    ix = torch.abs(KR.a) &gt; 0.001\n\n    x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n    X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n    x1_r = X1.ravel()\n    x2_r = X2.ravel()\n    X_ = torch.stack((x1_r, x2_r), dim=1)\n\n    preds = KR.prediction(X_, recompute_kernel=True)\n    preds = torch.reshape(preds, X1.size())\n\n    axs[i].set_title(f\"Gamma = {gamma}\")\n    \n    axs[i].contourf(\n        X1,\n        X2,\n        preds,\n        origin=\"lower\",\n        cmap=\"BrBG\",\n        vmin=2 * preds.min() - preds.max(),\n        vmax=2 * preds.max() - preds.min(),\n    )\n    # Actual decision boundary at probability 0.5\n    axs[i].contour(X1, X2, preds, levels=[0.5], colors='black', linewidths=2)\n\n    plot_classification_data(X, y, axs[i])\n    axs[i].scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\", label='Support Vectors')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this experiment, I test my model with a gammas of 1, 10, 20, 50, 100 to see with the decision boundaries looked like. I observed that is the gamma increased the boundaries became more focused on one class and more wiggly.\n\n\n\nThis experiment shows that the kernelized model can still find the pattern within nonlinear data. First, I generate nonlinear data for the model to use.\n\n\nCode\nfrom sklearn.datasets import make_moons\n\nX_nonlinear, y_nonlinear = make_moons(n_samples=400, noise=0.9)\n\n\nX_nonlinear = torch.tensor(X_nonlinear, dtype=torch.float32)\ny_nonlinear = torch.tensor(y_nonlinear, dtype=torch.float32)\n\n\nThen I run the usual graphing code to visualize results.\n\n\nCode\nKR = KernelLogisticRegression(rbf_kernel, lam=1.0, gamma=10)\nKR.fit(X_nonlinear, y_nonlinear, m_epochs=200000, lr=0.0001)\n\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X_nonlinear[:, 0].min() - 0.2, X_nonlinear[:, 0].max() + 0.2, 101)\nx2 = torch.linspace(X_nonlinear[:, 1].min() - 0.2, X_nonlinear[:, 1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim=1)\n\npreds = KR.prediction(X_, recompute_kernel=True)\npreds = 1.0 * torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(\n    X1,\n    X2,\n    preds,\n    origin=\"lower\",\n    cmap=\"BrBG\",\n    vmin=2 * preds.min() - preds.max(),\n    vmax=2 * preds.max() - preds.min(),\n)\n\nax.contour(X1, X2, preds, levels=[0.5], colors='black', linewidths=2)\n\nplot_classification_data(X_nonlinear, y_nonlinear, ax)\nplt.scatter(X_nonlinear[ix, 0], X_nonlinear[ix, 1], facecolors=\"none\", edgecolors=\"black\")\nplt.title(\"Kernel Logistic Regression on Nonlinear Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this experiment, the model is used to classify classify nonlinear data generated by make_moons. Initially, with a low gamma value (Œ≥ = 0.01), the model underfit the data, producing a nearly linear decision boundary that failed to capture the curved structure of the classes. By increasing gamma to 10, the kernel returned a more accurate nonlinear decision boundaries. Both separated concentrations of the blue region from the brown.s\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\n\n# Generate and split data\nX_over, y_over = classification_data(n_points=1000, noise=0.8)\nX_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.4, random_state=42)\n\n# Fit a model with high gamma (likely to overfit)\nKRO = KernelLogisticRegression(rbf_kernel, lam=1e-5, gamma=1000)\nKRO.fit(X_train, y_train, m_epochs=30000, lr=0.0001)\n\n# Predict probabilities on train and test\nwith torch.no_grad():\n    y_train_scores = KRO.prediction(X_train, recompute_kernel=True)\n    y_test_scores = KRO.prediction(X_test, recompute_kernel=True)\n\n# Convert to NumPy for sklearn\ny_train_np = y_train.numpy()\ny_test_np = y_test.numpy()\ntrain_scores_np = y_train_scores.numpy()\ntest_scores_np = y_test_scores.numpy()\n\n# Compute ROC curves\nfpr_train, tpr_train, _ = roc_curve(y_train_np, train_scores_np)\nfpr_test, tpr_test, _ = roc_curve(y_test_np, test_scores_np)\n\n# Compute AUC\nroc_auc_train = auc(fpr_train, tpr_train)\nroc_auc_test = auc(fpr_test, tpr_test)\n\n# Plot\nplt.figure(figsize=(7, 5))\nplt.plot(fpr_train, tpr_train, label=f\"Train ROC (AUC = {roc_auc_train:.2f})\")\nplt.plot(fpr_test, tpr_test, label=f\"Test ROC (AUC = {roc_auc_test:.2f})\", linestyle='--')\nplt.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curves: Overfitting\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this experiment, I simulate the problem of overfitting by training a kernel logistic regression model with extremely high gamma of 1000. This allows the model to fit the training data very closely, but it generalizes poorly to unseen test data.\nThe (AUC) score for training set was near with perfect with 0.99 but the testing set was near 0.72. This large gap between train and test AUCs is a sign that there is overfitting.\n\n\n\nIn this post, I explored the behavior of sparse kernel logistic regression with RBF kernels through several experiments:\n\nSparsity and Regularization: When using a large regularization parameter (ùúÜ = 166), only 1 out of 100 training points had a weight (Œ±) greater than 0.001, demonstrating extreme sparsity. In contrast, with a smaller ùúÜ (e.g., 0.1), a larger fraction of points acted as support vectors, contributing to the decision boundary.\nEffect of Gamma (ùõæ): By varying ùõæ from 1 to 100, I observed that the decision boundary became increasingly complex and less smooth. For example, at ùõæ = 1, the boundary was broad and circular, while at ùõæ = 100, it became highly irregular, closely fitting the training data.\nNonlinear Data: On a challenging, noisy dataset (400 points, noise = 0.9), the kernelized model with ùõæ = 10 was still able to capture the nonlinear patterns.\nOverfitting: Training with a very high ùõæ (1000) and low ùúÜ (1e-5) on a larger dataset (1000 points, noise = 0.8), the model achieved a near-perfect training AUC of 0.99, but the test AUC dropped to 0.72. This large gap quantitatively demonstrates overfitting: the model fits the training data extremely well but fails to generalize.\n\n\n\n\n\nAdapted code from Professor Phil Chodrow at Middlebury College in his class CSCI 0451: Machine Learning."
  },
  {
    "objectID": "posts/kernel/index.html#abstract",
    "href": "posts/kernel/index.html#abstract",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "In this post, I explore sparse kernel logistic regression using radial basis function (RBF) kernels. I demonstrate how sparsity naturally emerges in these models, meaning only a subset of training points contribute significantly to the final decision function. I examine how key hyperparameters like the regularization strength (ùúÜ) and the bandwidth (ùõæ) influence model behavior, including sparsity, decision boundaries, and overfitting. Through several experiments, I visualize decision surface, investigate the impact of parameter choices, and show how kernel methods can capture nonlinear patterns effectively. To evaluate generalization, I conclude with an overfitting case study using ROC curves to compare training and test performance.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nClassification (cell 15) and kernel (cell 16) code adapted from Prof.¬†Phil.\n\n\nCode\nimport torch\nfrom matplotlib import pyplot as plt\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n\ndef classification_data(n_points=300, noise=0.2, p_dims=2):\n\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    X = X - X.mean(dim=0, keepdim=True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(\n            X[ix, 0],\n            X[ix, 1],\n            s=20,\n            c=y[ix],\n            facecolors=\"none\",\n            edgecolors=\"darkgrey\",\n            cmap=\"BrBG\",\n            vmin=-1,\n            vmax=2,\n            alpha=0.8,\n            marker=markers[i],\n        )\n    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points=100, noise=0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma * torch.cdist(X_1, X_2) ** 2)\n\n\n\n\nCode\nfrom kernel_logistic import KernelLogisticRegression\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\n\n\nCode\n(1.0 * (KR.a &gt; 0.001)).mean()\n\n\ntensor(0.5300)\n\n\n\n\nCode\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\nx2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim=1)\n\npreds = KR.prediction(X_, recompute_kernel=True)\npreds = 1.0 * torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(\n    X1,\n    X2,\n    preds,\n    origin=\"lower\",\n    cmap=\"BrBG\",\n    vmin=2 * preds.min() - preds.max(),\n    vmax=2 * preds.max() - preds.min(),\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")\nplt.show()"
  },
  {
    "objectID": "posts/kernel/index.html#basic-experiments",
    "href": "posts/kernel/index.html#basic-experiments",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "This experiment shows that when ùúÜ is very large, there may be only one point in the training data with a wieght distinguishable from zero.\n\n\nCode\nKR = KernelLogisticRegression(rbf_kernel, lam=166, gamma=1)\nKR.fit(X, y, m_epochs=20000, lr=0.0001)\n\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\nx2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim=1)\n\npreds = KR.prediction(X_, recompute_kernel=True)\npreds = 1.0 * torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(\n    X1,\n    X2,\n    preds,\n    origin=\"lower\",\n    cmap=\"BrBG\",\n    vmin=2 * preds.min() - preds.max(),\n    vmax=2 * preds.max() - preds.min(),\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAt a lambda of ~166, there was only 1 data point in the center of brown region that had a distinguishable weight.\n\n\n\nThis experiment is to show that changing ùõÑ can result in wigglier descision boundaries.\n\n\nCode\ngammas = [1, 10, 20, 50, 100]  # List of gamma values to try\nfig, axs = plt.subplots(1, 5, figsize=(20, 4))  # One row, 5 subplots\n\nfor i, gamma in enumerate(gammas):\n    KR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=gamma)\n    KR.fit(X, y, m_epochs=10000, lr=0.0001)\n\n    ix = torch.abs(KR.a) &gt; 0.001\n\n    x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n    X1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n    x1_r = X1.ravel()\n    x2_r = X2.ravel()\n    X_ = torch.stack((x1_r, x2_r), dim=1)\n\n    preds = KR.prediction(X_, recompute_kernel=True)\n    preds = torch.reshape(preds, X1.size())\n\n    axs[i].set_title(f\"Gamma = {gamma}\")\n    \n    axs[i].contourf(\n        X1,\n        X2,\n        preds,\n        origin=\"lower\",\n        cmap=\"BrBG\",\n        vmin=2 * preds.min() - preds.max(),\n        vmax=2 * preds.max() - preds.min(),\n    )\n    # Actual decision boundary at probability 0.5\n    axs[i].contour(X1, X2, preds, levels=[0.5], colors='black', linewidths=2)\n\n    plot_classification_data(X, y, axs[i])\n    axs[i].scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\", label='Support Vectors')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this experiment, I test my model with a gammas of 1, 10, 20, 50, 100 to see with the decision boundaries looked like. I observed that is the gamma increased the boundaries became more focused on one class and more wiggly.\n\n\n\nThis experiment shows that the kernelized model can still find the pattern within nonlinear data. First, I generate nonlinear data for the model to use.\n\n\nCode\nfrom sklearn.datasets import make_moons\n\nX_nonlinear, y_nonlinear = make_moons(n_samples=400, noise=0.9)\n\n\nX_nonlinear = torch.tensor(X_nonlinear, dtype=torch.float32)\ny_nonlinear = torch.tensor(y_nonlinear, dtype=torch.float32)\n\n\nThen I run the usual graphing code to visualize results.\n\n\nCode\nKR = KernelLogisticRegression(rbf_kernel, lam=1.0, gamma=10)\nKR.fit(X_nonlinear, y_nonlinear, m_epochs=200000, lr=0.0001)\n\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X_nonlinear[:, 0].min() - 0.2, X_nonlinear[:, 0].max() + 0.2, 101)\nx2 = torch.linspace(X_nonlinear[:, 1].min() - 0.2, X_nonlinear[:, 1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing=\"ij\")\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim=1)\n\npreds = KR.prediction(X_, recompute_kernel=True)\npreds = 1.0 * torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(\n    X1,\n    X2,\n    preds,\n    origin=\"lower\",\n    cmap=\"BrBG\",\n    vmin=2 * preds.min() - preds.max(),\n    vmax=2 * preds.max() - preds.min(),\n)\n\nax.contour(X1, X2, preds, levels=[0.5], colors='black', linewidths=2)\n\nplot_classification_data(X_nonlinear, y_nonlinear, ax)\nplt.scatter(X_nonlinear[ix, 0], X_nonlinear[ix, 1], facecolors=\"none\", edgecolors=\"black\")\nplt.title(\"Kernel Logistic Regression on Nonlinear Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this experiment, the model is used to classify classify nonlinear data generated by make_moons. Initially, with a low gamma value (Œ≥ = 0.01), the model underfit the data, producing a nearly linear decision boundary that failed to capture the curved structure of the classes. By increasing gamma to 10, the kernel returned a more accurate nonlinear decision boundaries. Both separated concentrations of the blue region from the brown.s"
  },
  {
    "objectID": "posts/kernel/index.html#overfitting",
    "href": "posts/kernel/index.html#overfitting",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "Code\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\n\n# Generate and split data\nX_over, y_over = classification_data(n_points=1000, noise=0.8)\nX_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.4, random_state=42)\n\n# Fit a model with high gamma (likely to overfit)\nKRO = KernelLogisticRegression(rbf_kernel, lam=1e-5, gamma=1000)\nKRO.fit(X_train, y_train, m_epochs=30000, lr=0.0001)\n\n# Predict probabilities on train and test\nwith torch.no_grad():\n    y_train_scores = KRO.prediction(X_train, recompute_kernel=True)\n    y_test_scores = KRO.prediction(X_test, recompute_kernel=True)\n\n# Convert to NumPy for sklearn\ny_train_np = y_train.numpy()\ny_test_np = y_test.numpy()\ntrain_scores_np = y_train_scores.numpy()\ntest_scores_np = y_test_scores.numpy()\n\n# Compute ROC curves\nfpr_train, tpr_train, _ = roc_curve(y_train_np, train_scores_np)\nfpr_test, tpr_test, _ = roc_curve(y_test_np, test_scores_np)\n\n# Compute AUC\nroc_auc_train = auc(fpr_train, tpr_train)\nroc_auc_test = auc(fpr_test, tpr_test)\n\n# Plot\nplt.figure(figsize=(7, 5))\nplt.plot(fpr_train, tpr_train, label=f\"Train ROC (AUC = {roc_auc_train:.2f})\")\nplt.plot(fpr_test, tpr_test, label=f\"Test ROC (AUC = {roc_auc_test:.2f})\", linestyle='--')\nplt.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curves: Overfitting\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this experiment, I simulate the problem of overfitting by training a kernel logistic regression model with extremely high gamma of 1000. This allows the model to fit the training data very closely, but it generalizes poorly to unseen test data.\nThe (AUC) score for training set was near with perfect with 0.99 but the testing set was near 0.72. This large gap between train and test AUCs is a sign that there is overfitting."
  },
  {
    "objectID": "posts/kernel/index.html#discussion",
    "href": "posts/kernel/index.html#discussion",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "In this post, I explored the behavior of sparse kernel logistic regression with RBF kernels through several experiments:\n\nSparsity and Regularization: When using a large regularization parameter (ùúÜ = 166), only 1 out of 100 training points had a weight (Œ±) greater than 0.001, demonstrating extreme sparsity. In contrast, with a smaller ùúÜ (e.g., 0.1), a larger fraction of points acted as support vectors, contributing to the decision boundary.\nEffect of Gamma (ùõæ): By varying ùõæ from 1 to 100, I observed that the decision boundary became increasingly complex and less smooth. For example, at ùõæ = 1, the boundary was broad and circular, while at ùõæ = 100, it became highly irregular, closely fitting the training data.\nNonlinear Data: On a challenging, noisy dataset (400 points, noise = 0.9), the kernelized model with ùõæ = 10 was still able to capture the nonlinear patterns.\nOverfitting: Training with a very high ùõæ (1000) and low ùúÜ (1e-5) on a larger dataset (1000 points, noise = 0.8), the model achieved a near-perfect training AUC of 0.99, but the test AUC dropped to 0.72. This large gap quantitatively demonstrates overfitting: the model fits the training data extremely well but fails to generalize."
  },
  {
    "objectID": "posts/kernel/index.html#acknowledgements",
    "href": "posts/kernel/index.html#acknowledgements",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "Adapted code from Professor Phil Chodrow at Middlebury College in his class CSCI 0451: Machine Learning."
  },
  {
    "objectID": "posts/optimization/index.html",
    "href": "posts/optimization/index.html",
    "title": "Advanced Optimization",
    "section": "",
    "text": "In this blog post, I explore advanced optimization techniques‚ÄîNewton‚Äôs Method and the Adam optimizer‚Äîin the context of logistic regression applied to heart disease prediction. Through a series of experiments on a Kaggle heart dataset, I investigate the convergence behavior, sensitivity to learning rates, and efficiency of these methods. The three optimizers I work with are Newton‚Äôs method, Adam, and gradient descent. The goal is to understand how different optimization algorithms influence model performance and training dynamics in a binary classification tasks.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\nfrom newton_logistic import LogisticRegression, NewtonOptimizer, GradientDescentOptimizer\nimport torch\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\nIn this experiment, I apply my Newton logistic regression model to a real-world dataset: the Kaggle heart disease prediction dataset. I preprocess the data, train the model, and evaluate its performance on training, validation, and test sets.\n\n\nCode\nimport kagglehub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Download dataset from Kaggle\npath = kagglehub.dataset_download(\"shantanugarg274/heart-prediction-dataset-quantum\")\nprint(\"Path to dataset files:\", path)\n\ndata_path = path + \"/Heart Prediction Quantum Dataset.csv\"\ndf = pd.read_csv(data_path)\n# print(df.head())\n\n\nWarning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.12).\nPath to dataset files: /Users/emmanueltowner/.cache/kagglehub/datasets/shantanugarg274/heart-prediction-dataset-quantum/versions/1\n\n\nThe data was in 1 csv file with 7 columns representing age, gender, blood pressure, cholesterol, heart rate, quantum pattern feature, and heart disease.\n\n\nCode\nX_data = df.drop(\"HeartDisease\", axis=1).values\ny_data = df[\"HeartDisease\"].values\n\n\nSince I intend to predict heart disease, I removed that column from the main dataset and used it in the target set.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_data = scaler.fit_transform(X_data)\nX_data = torch.tensor(X_data, dtype=torch.float32)\ny_data = torch.tensor(y_data, dtype=torch.float32)\n\n\nThe data across features widely varied in range and so I used sci-kit learn‚Äôs StandardScaler to standardize the both datasets and then converted them into tensors. The model was trained on the training set and the loss computed for both training and validation.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.4, random_state=42)\n\n\nI also used train_test_split to split the 60% data into training and 40% in test sets.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_train = []\nloss_test = []\n\niterations = 100\nalpha = 0.1\n\n\nfor _ in range(iterations):\n    train_loss = LR.loss(X_train, y_train)\n    loss_train.append(train_loss.item())\n    \n    test_loss = LR.loss(X_test, y_test)\n    loss_test.append(test_loss.item())\n    \n    opt.step(X_train, y_train, alpha)\n\n    \n# Plotting the loss\nfig, ax = plt.subplots(figsize=(6, 6))  # Single axes, adjust figsize if needed\nax.plot(torch.arange(1, iterations + 1), loss_train, color=\"black\")\nax.plot(torch.arange(1, iterations + 1), loss_test, color=\"orange\")\nax.set_xlabel(\"Iterations\")\nax.set_ylabel(\"Loss\")\nax.set_title(\"Loss vs Iterations\")\nax.legend([\"Train Loss\", \"Test Loss\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nWith an alpha of 0.1 both training and testing loss converge in between 20 and 40 iterations. Testing loss converges slightly earlier and at a slightly higher loss than training loss.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nLR = LogisticRegression() \noptn = NewtonOptimizer(LR)\n\nn_loss_train = []\nn_loss_test = []\n\niterations = 100\nalpha = 0.1\n\n\nfor _ in range(iterations):\n    train_loss = LR.loss(X_train, y_train)\n    n_loss_train.append(train_loss.item())\n\n    test_loss = LR.loss(X_test, y_test)\n    n_loss_test.append(test_loss.item())\n    \n    optn.step(X_train, y_train, alpha)\n\nLR = LogisticRegression() \noptg = GradientDescentOptimizer(LR)\n\ng_loss_train = []\ng_loss_test = []\n\niterations = 100\nalpha = 0.1\n\n\nfor _ in range(iterations):\n    train_loss = LR.loss(X_train, y_train)\n    g_loss_train.append(train_loss.item())\n\n    test_loss = LR.loss(X_test, y_test)\n    g_loss_test.append(test_loss.item())\n\n    optg.step(X_train, y_train, alpha, beta=0.9)\n\n# Plotting the loss\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))  # Single axes, adjust figsize if needed\nax[0].plot(torch.arange(1, iterations + 1), n_loss_train, color=\"black\")\nax[0].plot(torch.arange(1, iterations + 1), n_loss_test, color=\"orange\")\nax[0].set_xlabel(\"Iterations\")\nax[0].set_ylabel(\"Loss\")\nax[0].set_title(\"Newton Optimizer Loss\")\nax[0].legend([\"Train Loss\", \"Test Loss\"])\n\nax[1].plot(torch.arange(1, iterations + 1), g_loss_train, color=\"black\")\nax[1].plot(torch.arange(1, iterations + 1), g_loss_test, color=\"orange\")\nax[1].set_xlabel(\"Iterations\")\nax[1].set_ylabel(\"Loss\")\nax[1].set_title(\"Gradient Descent Loss\")\nax[1].legend([\"Train Loss\", \"Test Loss\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this plot, we see that with the Newton Optimizer the loss convergences faster than the gradient descent optimizer. For Newton optimizer, the loss reaches convergence at ~30-35 iterations whereas for the gradient optimizer the loss converges at ~50-60. They both share an alpha of 0.1 while gradient descent has a beta of 0.9.\n\n\n\n\n\nCode\nLR = LogisticRegression() \nopts = NewtonOptimizer(LR)\n\nloss_train = []\nloss_test = []\n\niterations = 100\nalpha = 1.3\n\n\nfor _ in range(iterations):\n    train_loss = LR.loss(X_train, y_train)\n    loss_train.append(train_loss.item())\n    \n    test_loss = LR.loss(X_test, y_test)\n    loss_test.append(test_loss.item())\n    \n    # print(\"Train Loss: \", train_loss.item())\n    opts.step(X_train, y_train, alpha)\n\n    \n# Plotting the loss\nfig, ax = plt.subplots(figsize=(6, 6))  \nax.plot(torch.arange(1, iterations + 1), loss_train, color=\"black\")\nax.plot(torch.arange(1, iterations + 1), loss_test, color=\"orange\")\nax.set_xlabel(\"Iterations\")\nax.set_ylabel(\"Loss\")\nax.set_title(\"Loss vs Iterations\")\nax.legend([\"Train Loss\", \"Test Loss\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nThis model converges for all alpha‚Äôs from up to 1. The original hessian simply returned an error when alpha went above, now with regularization the loss doesn‚Äôt always commpute and returns a broken graph.\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom adam import AdamOptimizer\n\n# Adam optimizer test\n\n\niterations = 100\nbatch_size = 40\n\nalphas = [0.001, 0.01, 0.03, 0.1]\nadam_losses_dict = {}\nfor alpha in alphas:\n    LRA = LogisticRegression()\n    adam = AdamOptimizer(LRA)\n    adam_losses = []\n    for _ in range(iterations):\n        train_loss = LRA.loss(X_train, y_train)\n        adam_losses.append(train_loss.item())\n        adam.optim(X_train, y_train, batch_size=batch_size, alpha=alpha)\n    adam_losses_dict[alpha] = adam_losses\n\n\n# SGD (minibatch) test for different step sizes\n\ngrad_losses_dict = {}\n\nfor alpha in alphas:\n    LRG = LogisticRegression()\n    grad = GradientDescentOptimizer(LRG)\n    grad_losses = []\n    for _ in range(iterations):\n        train_loss = LRG.loss(X_train, y_train)\n        grad_losses.append(train_loss.item())\n        grad.step(X_train, y_train, alpha=alpha, beta=0.0, mini_batch=True)\n    grad_losses_dict[alpha] = grad_losses\n    \n# Plotting the loss\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))  \nfor alpha, losses in adam_losses_dict.items():\n    ax[0].plot(torch.arange(1, iterations + 1), losses, label=f\"Adam (Œ±={alpha})\")\nax[0].set_xlabel(\"Iterations\")\nax[0].set_ylabel(\"Loss\")\nax[0].set_title(\"Adam Optimizer Loss\")\nax[0].legend()\n\nfor alpha, losses in grad_losses_dict.items():\n    ax[1].plot(torch.arange(1, iterations + 1), losses, label=f\"Grad (Œ±={alpha})\")\nax[1].set_xlabel(\"Iterations\")\nax[1].set_ylabel(\"Loss\")\nax[1].set_title(\"Gradient Descent Loss\")\nax[1].legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis test compares the performance of the Adam optimizer and mini-batch gradient descent on logistic regression. For each optimizer, the code tests four learning rates (Œ± = 0.001, 0.01, 0.03, 0.1) over 100 iterations, recording the training loss at each step. Adam uses a batch size of 40, while SGD uses mini-batches with no momentum. The results show that Adam consistently shows faster and smoother convergence across all learning rates compared to SGD, which converges more slowly and is more sensitive to the choice of Œ±. This demonstrates Adam‚Äôs advantage in stability and speed for this classification task. Both struggle to converge on low alphas such as 0.01.\n\n\n\n\n\nCode\n\nimport time\n\ntarget_loss = 0.3\nalpha = 0.03\n\n\n# Adam optimizer\nLRA = LogisticRegression() \nadam = AdamOptimizer(LRA)\n\nadam_loss = float('inf')  # large initial value\n\nstart = time.perf_counter()\n\nwhile adam_loss &gt; target_loss:\n    # Compute current loss\n    train_loss = LRA.loss(X_train, y_train)\n    adam_loss = train_loss.item()\n    # print(f\"[Adam] Current loss: {adam_loss:.4f}\")\n    \n    # Perform one round of updates\n    adam.optim(X_train, y_train, batch_size=40, alpha=alpha)\n\nend = time.perf_counter()\nprint(f\"Adam optimizer took {end - start:.4f} seconds\")\nprint(f\"Final Adam loss: {LRA.loss(X_train, y_train).item():.4f}\")\n\n# Newton Optimizer\nLRN = LogisticRegression()\nnewton = NewtonOptimizer(LRN)\n\nnewton_loss = float('inf')  # large initial value\n\nstart = time.perf_counter()\n\nwhile newton_loss &gt; target_loss:\n    # Compute current loss\n    train_loss = LRN.loss(X_train, y_train)\n    newton_loss = train_loss.item()\n    # print(f\"[Newton] Current loss: {newton_loss:.4f}\")\n    # Perform one round of updates\n    newton.step(X_train, y_train, alpha)\n\nend = time.perf_counter()\nprint(f\"Newton optimizer took {end - start:.4f} seconds\")\nprint(f\"Final Newton loss: {LRN.loss(X_train, y_train).item():.4f}\")\n\n\n\n\n\nAdam optimizer took 0.1112 seconds\nFinal Adam loss: 0.2380\nNewton optimizer took 0.0081 seconds\nFinal Newton loss: 0.2965\n\n\nIn the above code, the model was trained using Adam and Newton optimizer with the goal of reducing the training loss below a target value of 0.3. Both optimizers iteratively updated the model parameters, with Adam using adaptive learning rates and mini-batches, while Gradient Descent used momentum and mini-batch updates. The results showed that Adam achieved a lower final loss (0.2379) but took longer to converge (0.1134 seconds), whereas Gradient Descent was significantly faster (0.0062 seconds) but only marginally met the loss target (0.2989).\n\n\n\n\nThe experiments show that different optimizers can perform better in different situations. Newton‚Äôs Method achieved faster convergence than standard gradient descent in terms of iteration count but when alpha became really large it was unable to converge. Gradient descent methods, while slower, offer greater stability across a wider range of learning rates. Adam strikes a balance, providing both great loss rate and rapid convergence. It was faster the mini-batch gradient descent but slower than Newton. In both situations, it had the better loss. \n\n\n\nAdapted code from Professor Phil Chodrow at Middlebury College in his class CSCI 0451: Machine Learning."
  },
  {
    "objectID": "posts/optimization/index.html#abstract",
    "href": "posts/optimization/index.html#abstract",
    "title": "Advanced Optimization",
    "section": "",
    "text": "In this blog post, I explore advanced optimization techniques‚ÄîNewton‚Äôs Method and the Adam optimizer‚Äîin the context of logistic regression applied to heart disease prediction. Through a series of experiments on a Kaggle heart dataset, I investigate the convergence behavior, sensitivity to learning rates, and efficiency of these methods. The three optimizers I work with are Newton‚Äôs method, Adam, and gradient descent. The goal is to understand how different optimization algorithms influence model performance and training dynamics in a binary classification tasks.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\nfrom newton_logistic import LogisticRegression, NewtonOptimizer, GradientDescentOptimizer\nimport torch\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/optimization/index.html#newton-experiments",
    "href": "posts/optimization/index.html#newton-experiments",
    "title": "Advanced Optimization",
    "section": "",
    "text": "In this experiment, I apply my Newton logistic regression model to a real-world dataset: the Kaggle heart disease prediction dataset. I preprocess the data, train the model, and evaluate its performance on training, validation, and test sets.\n\n\nCode\nimport kagglehub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Download dataset from Kaggle\npath = kagglehub.dataset_download(\"shantanugarg274/heart-prediction-dataset-quantum\")\nprint(\"Path to dataset files:\", path)\n\ndata_path = path + \"/Heart Prediction Quantum Dataset.csv\"\ndf = pd.read_csv(data_path)\n# print(df.head())\n\n\nWarning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.12).\nPath to dataset files: /Users/emmanueltowner/.cache/kagglehub/datasets/shantanugarg274/heart-prediction-dataset-quantum/versions/1\n\n\nThe data was in 1 csv file with 7 columns representing age, gender, blood pressure, cholesterol, heart rate, quantum pattern feature, and heart disease.\n\n\nCode\nX_data = df.drop(\"HeartDisease\", axis=1).values\ny_data = df[\"HeartDisease\"].values\n\n\nSince I intend to predict heart disease, I removed that column from the main dataset and used it in the target set.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_data = scaler.fit_transform(X_data)\nX_data = torch.tensor(X_data, dtype=torch.float32)\ny_data = torch.tensor(y_data, dtype=torch.float32)\n\n\nThe data across features widely varied in range and so I used sci-kit learn‚Äôs StandardScaler to standardize the both datasets and then converted them into tensors. The model was trained on the training set and the loss computed for both training and validation.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.4, random_state=42)\n\n\nI also used train_test_split to split the 60% data into training and 40% in test sets.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_train = []\nloss_test = []\n\niterations = 100\nalpha = 0.1\n\n\nfor _ in range(iterations):\n    train_loss = LR.loss(X_train, y_train)\n    loss_train.append(train_loss.item())\n    \n    test_loss = LR.loss(X_test, y_test)\n    loss_test.append(test_loss.item())\n    \n    opt.step(X_train, y_train, alpha)\n\n    \n# Plotting the loss\nfig, ax = plt.subplots(figsize=(6, 6))  # Single axes, adjust figsize if needed\nax.plot(torch.arange(1, iterations + 1), loss_train, color=\"black\")\nax.plot(torch.arange(1, iterations + 1), loss_test, color=\"orange\")\nax.set_xlabel(\"Iterations\")\nax.set_ylabel(\"Loss\")\nax.set_title(\"Loss vs Iterations\")\nax.legend([\"Train Loss\", \"Test Loss\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nWith an alpha of 0.1 both training and testing loss converge in between 20 and 40 iterations. Testing loss converges slightly earlier and at a slightly higher loss than training loss.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nLR = LogisticRegression() \noptn = NewtonOptimizer(LR)\n\nn_loss_train = []\nn_loss_test = []\n\niterations = 100\nalpha = 0.1\n\n\nfor _ in range(iterations):\n    train_loss = LR.loss(X_train, y_train)\n    n_loss_train.append(train_loss.item())\n\n    test_loss = LR.loss(X_test, y_test)\n    n_loss_test.append(test_loss.item())\n    \n    optn.step(X_train, y_train, alpha)\n\nLR = LogisticRegression() \noptg = GradientDescentOptimizer(LR)\n\ng_loss_train = []\ng_loss_test = []\n\niterations = 100\nalpha = 0.1\n\n\nfor _ in range(iterations):\n    train_loss = LR.loss(X_train, y_train)\n    g_loss_train.append(train_loss.item())\n\n    test_loss = LR.loss(X_test, y_test)\n    g_loss_test.append(test_loss.item())\n\n    optg.step(X_train, y_train, alpha, beta=0.9)\n\n# Plotting the loss\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))  # Single axes, adjust figsize if needed\nax[0].plot(torch.arange(1, iterations + 1), n_loss_train, color=\"black\")\nax[0].plot(torch.arange(1, iterations + 1), n_loss_test, color=\"orange\")\nax[0].set_xlabel(\"Iterations\")\nax[0].set_ylabel(\"Loss\")\nax[0].set_title(\"Newton Optimizer Loss\")\nax[0].legend([\"Train Loss\", \"Test Loss\"])\n\nax[1].plot(torch.arange(1, iterations + 1), g_loss_train, color=\"black\")\nax[1].plot(torch.arange(1, iterations + 1), g_loss_test, color=\"orange\")\nax[1].set_xlabel(\"Iterations\")\nax[1].set_ylabel(\"Loss\")\nax[1].set_title(\"Gradient Descent Loss\")\nax[1].legend([\"Train Loss\", \"Test Loss\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this plot, we see that with the Newton Optimizer the loss convergences faster than the gradient descent optimizer. For Newton optimizer, the loss reaches convergence at ~30-35 iterations whereas for the gradient optimizer the loss converges at ~50-60. They both share an alpha of 0.1 while gradient descent has a beta of 0.9.\n\n\n\n\n\nCode\nLR = LogisticRegression() \nopts = NewtonOptimizer(LR)\n\nloss_train = []\nloss_test = []\n\niterations = 100\nalpha = 1.3\n\n\nfor _ in range(iterations):\n    train_loss = LR.loss(X_train, y_train)\n    loss_train.append(train_loss.item())\n    \n    test_loss = LR.loss(X_test, y_test)\n    loss_test.append(test_loss.item())\n    \n    # print(\"Train Loss: \", train_loss.item())\n    opts.step(X_train, y_train, alpha)\n\n    \n# Plotting the loss\nfig, ax = plt.subplots(figsize=(6, 6))  \nax.plot(torch.arange(1, iterations + 1), loss_train, color=\"black\")\nax.plot(torch.arange(1, iterations + 1), loss_test, color=\"orange\")\nax.set_xlabel(\"Iterations\")\nax.set_ylabel(\"Loss\")\nax.set_title(\"Loss vs Iterations\")\nax.legend([\"Train Loss\", \"Test Loss\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nThis model converges for all alpha‚Äôs from up to 1. The original hessian simply returned an error when alpha went above, now with regularization the loss doesn‚Äôt always commpute and returns a broken graph."
  },
  {
    "objectID": "posts/optimization/index.html#adam-optimizer",
    "href": "posts/optimization/index.html#adam-optimizer",
    "title": "Advanced Optimization",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nfrom adam import AdamOptimizer\n\n# Adam optimizer test\n\n\niterations = 100\nbatch_size = 40\n\nalphas = [0.001, 0.01, 0.03, 0.1]\nadam_losses_dict = {}\nfor alpha in alphas:\n    LRA = LogisticRegression()\n    adam = AdamOptimizer(LRA)\n    adam_losses = []\n    for _ in range(iterations):\n        train_loss = LRA.loss(X_train, y_train)\n        adam_losses.append(train_loss.item())\n        adam.optim(X_train, y_train, batch_size=batch_size, alpha=alpha)\n    adam_losses_dict[alpha] = adam_losses\n\n\n# SGD (minibatch) test for different step sizes\n\ngrad_losses_dict = {}\n\nfor alpha in alphas:\n    LRG = LogisticRegression()\n    grad = GradientDescentOptimizer(LRG)\n    grad_losses = []\n    for _ in range(iterations):\n        train_loss = LRG.loss(X_train, y_train)\n        grad_losses.append(train_loss.item())\n        grad.step(X_train, y_train, alpha=alpha, beta=0.0, mini_batch=True)\n    grad_losses_dict[alpha] = grad_losses\n    \n# Plotting the loss\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))  \nfor alpha, losses in adam_losses_dict.items():\n    ax[0].plot(torch.arange(1, iterations + 1), losses, label=f\"Adam (Œ±={alpha})\")\nax[0].set_xlabel(\"Iterations\")\nax[0].set_ylabel(\"Loss\")\nax[0].set_title(\"Adam Optimizer Loss\")\nax[0].legend()\n\nfor alpha, losses in grad_losses_dict.items():\n    ax[1].plot(torch.arange(1, iterations + 1), losses, label=f\"Grad (Œ±={alpha})\")\nax[1].set_xlabel(\"Iterations\")\nax[1].set_ylabel(\"Loss\")\nax[1].set_title(\"Gradient Descent Loss\")\nax[1].legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis test compares the performance of the Adam optimizer and mini-batch gradient descent on logistic regression. For each optimizer, the code tests four learning rates (Œ± = 0.001, 0.01, 0.03, 0.1) over 100 iterations, recording the training loss at each step. Adam uses a batch size of 40, while SGD uses mini-batches with no momentum. The results show that Adam consistently shows faster and smoother convergence across all learning rates compared to SGD, which converges more slowly and is more sensitive to the choice of Œ±. This demonstrates Adam‚Äôs advantage in stability and speed for this classification task. Both struggle to converge on low alphas such as 0.01.\n\n\n\n\n\nCode\n\nimport time\n\ntarget_loss = 0.3\nalpha = 0.03\n\n\n# Adam optimizer\nLRA = LogisticRegression() \nadam = AdamOptimizer(LRA)\n\nadam_loss = float('inf')  # large initial value\n\nstart = time.perf_counter()\n\nwhile adam_loss &gt; target_loss:\n    # Compute current loss\n    train_loss = LRA.loss(X_train, y_train)\n    adam_loss = train_loss.item()\n    # print(f\"[Adam] Current loss: {adam_loss:.4f}\")\n    \n    # Perform one round of updates\n    adam.optim(X_train, y_train, batch_size=40, alpha=alpha)\n\nend = time.perf_counter()\nprint(f\"Adam optimizer took {end - start:.4f} seconds\")\nprint(f\"Final Adam loss: {LRA.loss(X_train, y_train).item():.4f}\")\n\n# Newton Optimizer\nLRN = LogisticRegression()\nnewton = NewtonOptimizer(LRN)\n\nnewton_loss = float('inf')  # large initial value\n\nstart = time.perf_counter()\n\nwhile newton_loss &gt; target_loss:\n    # Compute current loss\n    train_loss = LRN.loss(X_train, y_train)\n    newton_loss = train_loss.item()\n    # print(f\"[Newton] Current loss: {newton_loss:.4f}\")\n    # Perform one round of updates\n    newton.step(X_train, y_train, alpha)\n\nend = time.perf_counter()\nprint(f\"Newton optimizer took {end - start:.4f} seconds\")\nprint(f\"Final Newton loss: {LRN.loss(X_train, y_train).item():.4f}\")\n\n\n\n\n\nAdam optimizer took 0.1112 seconds\nFinal Adam loss: 0.2380\nNewton optimizer took 0.0081 seconds\nFinal Newton loss: 0.2965\n\n\nIn the above code, the model was trained using Adam and Newton optimizer with the goal of reducing the training loss below a target value of 0.3. Both optimizers iteratively updated the model parameters, with Adam using adaptive learning rates and mini-batches, while Gradient Descent used momentum and mini-batch updates. The results showed that Adam achieved a lower final loss (0.2379) but took longer to converge (0.1134 seconds), whereas Gradient Descent was significantly faster (0.0062 seconds) but only marginally met the loss target (0.2989)."
  },
  {
    "objectID": "posts/optimization/index.html#discussion",
    "href": "posts/optimization/index.html#discussion",
    "title": "Advanced Optimization",
    "section": "",
    "text": "The experiments show that different optimizers can perform better in different situations. Newton‚Äôs Method achieved faster convergence than standard gradient descent in terms of iteration count but when alpha became really large it was unable to converge. Gradient descent methods, while slower, offer greater stability across a wider range of learning rates. Adam strikes a balance, providing both great loss rate and rapid convergence. It was faster the mini-batch gradient descent but slower than Newton. In both situations, it had the better loss."
  },
  {
    "objectID": "posts/optimization/index.html#acknowledgements",
    "href": "posts/optimization/index.html#acknowledgements",
    "title": "Advanced Optimization",
    "section": "",
    "text": "Adapted code from Professor Phil Chodrow at Middlebury College in his class CSCI 0451: Machine Learning."
  },
  {
    "objectID": "posts/descent/index.html",
    "href": "posts/descent/index.html",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "In this blog post, I explore the deep learning phenomenon of overfitting and the resulting double descent. First, I created a linear regression model and an overparameterized optimizer that could fit the model to the data. For the first visualization, I tested model predictions after fitting it to the data. Then I used my model to calculate the number of corruptions in an image. To measure model performance, I calculated mean squared error (MSE) and graphed how it changes as the number of features increased. In this visualization, I observed double descent on the testing set. I found that the optimal number of features was beyond the interpolation threshold at 192 features with an MSE of 301s.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\n\n\nCode\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X: torch.Tensor):\n        return self.activation(X @ self.u + self.b)\n\n\n\n\n\n\n\n\n\nCode\nfrom MyLinearRegression import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nphi = RandomFeatures(n_features= 100)\nphi.fit(X)\nX_train_features = phi.transform(X)\n\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\nopt.fit(X_train_features, y)\npred = LR.predict(X_train_features)\nline = pred.numpy() \n\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.plot(X, line, color='lightblue', label=\"Predictions\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nI tested the linear regression model on 1D data. The model seemed to produce accurate predictions.\n\n\n\nCells 5 to 9 were adapted from Prof.¬†Phil‚Äôs code as background for the corrupted flower images.\n\n\nCode\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X, .2) # decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\n\n\nCode\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n\n\n\nHere I assess the performance of my model by calculating the mean squared error (MSE) and plotting it against the number of features for both training and testing sets.\n\n\nCode\nn_features = 200\n\ntrain_loss_vec = []\ntest_loss_vec = []\n\nn = X_train.size()[0] \n\n# Loop over different numbers of random features\nfor i in range(n_features):\n    # Initialize the random feature transformation\n    phi = RandomFeatures(n_features = i, activation = square)\n    phi.fit(X_train) # Fit the random features to the training data\n    \n    # Transform both training and test data using the fitted feature map\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n    \n    # Initialize the linear regression model and optimizer\n    LR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(LR)\n    \n    # Fit the model to the transformed training data\n    opt.fit(X_train_phi, y_train)\n    \n    # Compute training and testing loss (MSE)\n    train_loss = LR.loss(X_train_phi, y_train).item()\n    test_loss = LR.loss(X_test_phi, y_test).item()\n    \n    # Append the losses to the respective lists\n    train_loss_vec.append(train_loss)\n    test_loss_vec.append(test_loss)\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))  \n\n# Plot training loss\nax[0].scatter(range(n_features), train_loss_vec, color='darkgrey')\nax[0].set_yscale('log')\nax[0].axvline(n, color='black', linewidth = 1)\nax[0].set_xlabel(\"Number of features\")\nax[0].set_ylabel(\"Mean Squared Error (Training)\")\n\n# Plot testing loss\nax[1].scatter(range(n_features), test_loss_vec, color='darkred')\nax[1].set_yscale('log')\nax[1].axvline(n, color='black', linewidth = 1)\nax[1].set_xlabel(\"Number of features\")\nax[1].set_ylabel(\"Mean Squared Error (Testing)\")\n\nplt.tight_layout()\nplt.show()\n\n# Find the best number of features with the lowest test loss\nbest_idx = torch.tensor(test_loss_vec).argmin().item() # convert vector to tensor and get index of minimum\nbest_features = best_idx + 1 \nbest_loss = test_loss_vec[best_idx]\n\nprint(f\"Lowest test loss: {best_loss:.4f} at {best_features} features.\")\n\n\n\n\n\n\n\n\n\nLowest test loss: 300.5068 at 192 features.\n\n\nFor the training data, I observed that as the number of features increased, the MSE decreased. In particular, at 100 features, which was the interpolation threshold, the MSE dropped dramatically from approximately \\(10^{-4}\\) to \\(10^{-22}\\).\nFor the testing data, I observed that the MSE decreased as the number of features increased, but then increased again after a certain point, indicating overfitting. Following the interpolation threshold, the error started to decrease again. The best error rate for the testing set was around 301 at 192 features, which was after the interpolation threshold.\n\n\n\n\nMy most important finding was that my model does achieve double descent. For training loss, the MSE continued to trend downward as the number of features increased. For testing loss, there was an initial decrease in loss, but then the loss rose when it reached near the interpolation threshold due to overfitting. After the interpolation threshold, the loss decreased again, reaching a second minimum.\nBased on my results, 192 features achieved the lowest MSE of 301."
  },
  {
    "objectID": "posts/descent/index.html#abstract",
    "href": "posts/descent/index.html#abstract",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "In this blog post, I explore the deep learning phenomenon of overfitting and the resulting double descent. First, I created a linear regression model and an overparameterized optimizer that could fit the model to the data. For the first visualization, I tested model predictions after fitting it to the data. Then I used my model to calculate the number of corruptions in an image. To measure model performance, I calculated mean squared error (MSE) and graphed how it changes as the number of features increased. In this visualization, I observed double descent on the testing set. I found that the optimal number of features was beyond the interpolation threshold at 192 features with an MSE of 301s.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\n\n\nCode\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X: torch.Tensor):\n        return self.activation(X @ self.u + self.b)"
  },
  {
    "objectID": "posts/descent/index.html#testing-mylinearregression-model",
    "href": "posts/descent/index.html#testing-mylinearregression-model",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "Code\nfrom MyLinearRegression import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nphi = RandomFeatures(n_features= 100)\nphi.fit(X)\nX_train_features = phi.transform(X)\n\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\nopt.fit(X_train_features, y)\npred = LR.predict(X_train_features)\nline = pred.numpy() \n\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.plot(X, line, color='lightblue', label=\"Predictions\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nI tested the linear regression model on 1D data. The model seemed to produce accurate predictions."
  },
  {
    "objectID": "posts/descent/index.html#corrupted-flower-images",
    "href": "posts/descent/index.html#corrupted-flower-images",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "Cells 5 to 9 were adapted from Prof.¬†Phil‚Äôs code as background for the corrupted flower images.\n\n\nCode\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X, .2) # decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\n\n\nCode\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n\n\n\nHere I assess the performance of my model by calculating the mean squared error (MSE) and plotting it against the number of features for both training and testing sets.\n\n\nCode\nn_features = 200\n\ntrain_loss_vec = []\ntest_loss_vec = []\n\nn = X_train.size()[0] \n\n# Loop over different numbers of random features\nfor i in range(n_features):\n    # Initialize the random feature transformation\n    phi = RandomFeatures(n_features = i, activation = square)\n    phi.fit(X_train) # Fit the random features to the training data\n    \n    # Transform both training and test data using the fitted feature map\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n    \n    # Initialize the linear regression model and optimizer\n    LR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(LR)\n    \n    # Fit the model to the transformed training data\n    opt.fit(X_train_phi, y_train)\n    \n    # Compute training and testing loss (MSE)\n    train_loss = LR.loss(X_train_phi, y_train).item()\n    test_loss = LR.loss(X_test_phi, y_test).item()\n    \n    # Append the losses to the respective lists\n    train_loss_vec.append(train_loss)\n    test_loss_vec.append(test_loss)\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))  \n\n# Plot training loss\nax[0].scatter(range(n_features), train_loss_vec, color='darkgrey')\nax[0].set_yscale('log')\nax[0].axvline(n, color='black', linewidth = 1)\nax[0].set_xlabel(\"Number of features\")\nax[0].set_ylabel(\"Mean Squared Error (Training)\")\n\n# Plot testing loss\nax[1].scatter(range(n_features), test_loss_vec, color='darkred')\nax[1].set_yscale('log')\nax[1].axvline(n, color='black', linewidth = 1)\nax[1].set_xlabel(\"Number of features\")\nax[1].set_ylabel(\"Mean Squared Error (Testing)\")\n\nplt.tight_layout()\nplt.show()\n\n# Find the best number of features with the lowest test loss\nbest_idx = torch.tensor(test_loss_vec).argmin().item() # convert vector to tensor and get index of minimum\nbest_features = best_idx + 1 \nbest_loss = test_loss_vec[best_idx]\n\nprint(f\"Lowest test loss: {best_loss:.4f} at {best_features} features.\")\n\n\n\n\n\n\n\n\n\nLowest test loss: 300.5068 at 192 features.\n\n\nFor the training data, I observed that as the number of features increased, the MSE decreased. In particular, at 100 features, which was the interpolation threshold, the MSE dropped dramatically from approximately \\(10^{-4}\\) to \\(10^{-22}\\).\nFor the testing data, I observed that the MSE decreased as the number of features increased, but then increased again after a certain point, indicating overfitting. Following the interpolation threshold, the error started to decrease again. The best error rate for the testing set was around 301 at 192 features, which was after the interpolation threshold."
  },
  {
    "objectID": "posts/descent/index.html#discussion",
    "href": "posts/descent/index.html#discussion",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "My most important finding was that my model does achieve double descent. For training loss, the MSE continued to trend downward as the number of features increased. For testing loss, there was an initial decrease in loss, but then the loss rose when it reached near the interpolation threshold due to overfitting. After the interpolation threshold, the loss decreased again, reaching a second minimum.\nBased on my results, 192 features achieved the lowest MSE of 301."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emmanuel‚Äôs GitHub Blog",
    "section": "",
    "text": "Advanced Optimization\n\n\n\n\n\nIn this blog post, I explore three advanced optimization techniques‚ÄîNewton‚Äôs Method, the Adam optimizer, and Gradient Descent Optimizer‚Äîin the context of logistic regression applied to heart disease prediction. The goal is to understand how different optimization algorithms influence model performance and training dynamics in a binary classification tasks.\n\n\n\n\n\nMay 7, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Kernel Machines\n\n\n\n\n\nIn this post, I implement a sparse kernel logistic regression using radial basis function (RBF) kernels. Additionally, explore how regularization strength (ùúÜ) and the bandwidth (ùõæ) effect influence it‚Äôs sparsity and overfitting.\n\n\n\n\n\nApr 29, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting and Double Descents\n\n\n\n\n\nIn this blog post, I explore the deep learning phenomenon of overfitting and the resulting double descent using a linear regression model and an overparameterized optimizer. I evaluated the model by its mean squared error (MSE) based on number of features.\n\n\n\n\n\nApr 23, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nIn this blog post, I analyze the Palmer Penguins dataset to identify the best features for determining penguin species based on their measurements.\n\n\n\n\n\nFeb 12, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/palmer/index.html",
    "href": "posts/palmer/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @gabednick\n\n\n\n\nIn this blog post, I analyze the Palmer Penguins dataset to identify the best features for determining penguin species based on their measurements. First, I create two figures and a table to explore the relationships between different features. Next, I utilize Scikit-learn‚Äôs feature selection methods, employing chi-squared tests to select two numerical features and one categorical feature. With these features, I train and test a logistic regression model. The model demonstrates reasonable accuracy; however, to gain a better understanding of the results, I visualize the decision regions and present a confusion matrix.st features to be used to determine the species of a penguin based on its measurements. Firstly, I create two figures and a table to analysize the relationships between features. Then I use sci-kit learns feature selection with chi-squared tests to pick 2 numerical features and 1 categorical feature. Then using those features, I train and test a logistic regression model. The model was fairly accurate but to understand the results better, I plot the decision regions and use a confusion matrix.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nThis code is from Professor Phil‚Äôs website. It removes unused columns and NA values, converts categorical feature columns into ‚Äúone-hot encoded‚Äù 0-1 columns, and saves the resulting DataFrame as X_train. Additionally, the ‚ÄúSpecies‚Äù column is encoded using LabelEncoder and stored as y_train.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nWe can check what the columns look like now.\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\nI created two graphs each two quantative columns and one qualitative columns. Plot 1 shows the relationship with the body mass and flipper length between different penguin species. Plot 2 shows the difference in Culmen Length and depth across different penguin species.\n\n# Get the unencoded columns for easier graphing.\nqual = train[[\"Island\", \"Sex\", \"Species\"]].dropna()\n\n# Shorten species label for the legend\nqual[\"Species\"] = qual[\"Species\"].apply(lambda x: \"Chinstrap\" if x == \"Chinstrap penguin (Pygoscelis antarctica)\" \n                                         else (\"Gentoo\" if x == \"Gentoo penguin (Pygoscelis papua)\" \n                                               else \"Adelie\"))\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-v0_8-whitegrid')\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 4))\n   \np1 = sns.scatterplot(X_train, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", hue=qual[\"Species\"], ax = ax[0])\np2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=qual[\"Species\"], ax = ax[1])\n\n\n\n\n\n\n\n\nPlot 1 (Left): This graph shows the relationship between body mass and flipper length among different penguin species. Gentoo penguins are the largest, while Chinstrap and Adelie penguins overlap considerably in size. Adelie penguins show slightly more variation in mass for a given flipper length compared to Chinstraps.\nPlot 2 (Right): This graph illustrates the differences in culmen length and depth among the species. Adelie penguins have the deepest but shortest culmen, Gentoo penguins have longer but less deep culmens, and Chinstraps are in between. These differences in beak size are significant for distinguishing penguin species.\n\n\n\nNow I create a summary table of the penguins measurements based on clutch completetion.\n\ntable = X_train[[\"Clutch Completion_Yes\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\ntable.groupby(\"Clutch Completion_Yes\").aggregate(['min', 'median', 'max'])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\nClutch Completion_Yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse\n35.9\n43.35\n58.0\n13.7\n17.85\n19.9\n172.0\n195.0\n225.0\n2700.0\n3737.5\n5700.0\n\n\nTrue\n34.0\n45.10\n55.9\n13.1\n17.20\n21.5\n176.0\n198.0\n230.0\n2850.0\n4100.0\n6300.0\n\n\n\n\n\n\n\nTable 1: This table shows that the most significant difference between penguins that had a full clutch and those that did not is their weight. Most of the penguins that produced two eggs weighed approximately 300 grams more. While there may be a correlation between clutch completion and weight, it is unlikely that there is a direct causation. Since clutch completion does not appear to impact this data significantly, it may not be a feature worth further investigation.\n\n\n\n\nHere I used the SelectKBest function from the sci-kit-learn library to choose the three features that I will include in my model. I separated feature selection because all three selected features are numerical. SelectKBest identifies the k best features based on a user-specified scoring function. I chose the chi-squared scoring function, as my features are intended for classification and are non-negative\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Selecting 2 numerical feature\nquant = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nsel1 = SelectKBest(chi2, k=2)\nsel1.fit_transform(X_train[quant], y_train)\nf1 = sel1.get_feature_names_out()\n\n# Selecting 1 categorical feature\nqual = [\"Clutch Completion_Yes\", \"Clutch Completion_No\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Sex_FEMALE\", \"Sex_MALE\"]\nsel2 = SelectKBest(chi2, k=1)\nsel2.fit_transform(X_train[qual], y_train)\nf2 = sel2.get_feature_names_out()\n\nThis function is so that I can get all the variations of the categorical feature.\n\ndef get_feat(f1, cat):\n    cols = list(f1)\n    clutch = [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n    island = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n    sex = [\"Sex_FEMALE\", \"Sex_MALE\"]\n    \n    if cat in clutch: return cols + clutch\n    if cat in island: return cols + island\n    if cat in sex: return cols + sex\n\n\ncols = get_feat(f1, f2[0])\ncols\n\n['Flipper Length (mm)',\n 'Body Mass (g)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nAlthough the culmen sizes initially appeared to be better features, the statistical tests indicated that flipper length and body mass were, in fact, the more significant features.\n\n\n\nThe model is trained on the data with features determined from above. I had to use StandardScalar to avoid a convergence error. I used the Logistic Regression model as it is a good fit for classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\npipe = make_pipeline(StandardScaler(), LogisticRegression())\npipe.fit(X_train[cols], y_train)\npipe.score(X_train[cols], y_train)\n\n0.8984375\n\n\n\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\npipe.score(X_test[cols], y_test)\n\n0.8970588235294118\n\n\n\n\n\n\n\nMost of this code is adapted from Prof.¬†Phil‚Äôs website.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nRegions for training set:\n\nplot_regions(pipe, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nRegions for testing set:\n\nplot_regions(pipe, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nLooking at the decision plots, we can see that our model is quite successful in distinguishing between Gentoo and Adelie penguins on the Biscoe and Torgersen islands. However, on Dream Island, where there is a mixture of Gentoo and Chinstrap penguins, the model struggles to differentiate between the two species.\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = pipe.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[26,  5,  0],\n       [ 2,  9,  0],\n       [ 0,  0, 26]])\n\n\nOnce again, this shows that model struggled the most with Gentoo and Chinstrap.\n\n\n\n\nThe model achieved a training accuracy of 0.89, which is the same as its testing accuracy. This indicates that the model is quite effective at predicting penguin species based on flipper length, body mass, and island. However, the decision regions suggest that the model had difficulty distinguishing between Chinstrap and Gentoo penguins on Dream Island. It appears that these two species have similar sizes, making them challenging to differentiate.\n\n\n\n\nAdapted code from Professor Phil Chodrow at Middlebury College in his class CSCI 0451: Machine Learning."
  },
  {
    "objectID": "posts/palmer/index.html#abstract",
    "href": "posts/palmer/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this blog post, I analyze the Palmer Penguins dataset to identify the best features for determining penguin species based on their measurements. First, I create two figures and a table to explore the relationships between different features. Next, I utilize Scikit-learn‚Äôs feature selection methods, employing chi-squared tests to select two numerical features and one categorical feature. With these features, I train and test a logistic regression model. The model demonstrates reasonable accuracy; however, to gain a better understanding of the results, I visualize the decision regions and present a confusion matrix.st features to be used to determine the species of a penguin based on its measurements. Firstly, I create two figures and a table to analysize the relationships between features. Then I use sci-kit learns feature selection with chi-squared tests to pick 2 numerical features and 1 categorical feature. Then using those features, I train and test a logistic regression model. The model was fairly accurate but to understand the results better, I plot the decision regions and use a confusion matrix.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/palmer/index.html#data-preparation",
    "href": "posts/palmer/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This code is from Professor Phil‚Äôs website. It removes unused columns and NA values, converts categorical feature columns into ‚Äúone-hot encoded‚Äù 0-1 columns, and saves the resulting DataFrame as X_train. Additionally, the ‚ÄúSpecies‚Äù column is encoded using LabelEncoder and stored as y_train.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nWe can check what the columns look like now.\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/palmer/index.html#data-visualization",
    "href": "posts/palmer/index.html#data-visualization",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "I created two graphs each two quantative columns and one qualitative columns. Plot 1 shows the relationship with the body mass and flipper length between different penguin species. Plot 2 shows the difference in Culmen Length and depth across different penguin species.\n\n# Get the unencoded columns for easier graphing.\nqual = train[[\"Island\", \"Sex\", \"Species\"]].dropna()\n\n# Shorten species label for the legend\nqual[\"Species\"] = qual[\"Species\"].apply(lambda x: \"Chinstrap\" if x == \"Chinstrap penguin (Pygoscelis antarctica)\" \n                                         else (\"Gentoo\" if x == \"Gentoo penguin (Pygoscelis papua)\" \n                                               else \"Adelie\"))\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-v0_8-whitegrid')\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 4))\n   \np1 = sns.scatterplot(X_train, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", hue=qual[\"Species\"], ax = ax[0])\np2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=qual[\"Species\"], ax = ax[1])\n\n\n\n\n\n\n\n\nPlot 1 (Left): This graph shows the relationship between body mass and flipper length among different penguin species. Gentoo penguins are the largest, while Chinstrap and Adelie penguins overlap considerably in size. Adelie penguins show slightly more variation in mass for a given flipper length compared to Chinstraps.\nPlot 2 (Right): This graph illustrates the differences in culmen length and depth among the species. Adelie penguins have the deepest but shortest culmen, Gentoo penguins have longer but less deep culmens, and Chinstraps are in between. These differences in beak size are significant for distinguishing penguin species.\n\n\n\nNow I create a summary table of the penguins measurements based on clutch completetion.\n\ntable = X_train[[\"Clutch Completion_Yes\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\ntable.groupby(\"Clutch Completion_Yes\").aggregate(['min', 'median', 'max'])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\nClutch Completion_Yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse\n35.9\n43.35\n58.0\n13.7\n17.85\n19.9\n172.0\n195.0\n225.0\n2700.0\n3737.5\n5700.0\n\n\nTrue\n34.0\n45.10\n55.9\n13.1\n17.20\n21.5\n176.0\n198.0\n230.0\n2850.0\n4100.0\n6300.0\n\n\n\n\n\n\n\nTable 1: This table shows that the most significant difference between penguins that had a full clutch and those that did not is their weight. Most of the penguins that produced two eggs weighed approximately 300 grams more. While there may be a correlation between clutch completion and weight, it is unlikely that there is a direct causation. Since clutch completion does not appear to impact this data significantly, it may not be a feature worth further investigation."
  },
  {
    "objectID": "posts/palmer/index.html#feature-selection",
    "href": "posts/palmer/index.html#feature-selection",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Here I used the SelectKBest function from the sci-kit-learn library to choose the three features that I will include in my model. I separated feature selection because all three selected features are numerical. SelectKBest identifies the k best features based on a user-specified scoring function. I chose the chi-squared scoring function, as my features are intended for classification and are non-negative\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Selecting 2 numerical feature\nquant = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nsel1 = SelectKBest(chi2, k=2)\nsel1.fit_transform(X_train[quant], y_train)\nf1 = sel1.get_feature_names_out()\n\n# Selecting 1 categorical feature\nqual = [\"Clutch Completion_Yes\", \"Clutch Completion_No\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Sex_FEMALE\", \"Sex_MALE\"]\nsel2 = SelectKBest(chi2, k=1)\nsel2.fit_transform(X_train[qual], y_train)\nf2 = sel2.get_feature_names_out()\n\nThis function is so that I can get all the variations of the categorical feature.\n\ndef get_feat(f1, cat):\n    cols = list(f1)\n    clutch = [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n    island = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n    sex = [\"Sex_FEMALE\", \"Sex_MALE\"]\n    \n    if cat in clutch: return cols + clutch\n    if cat in island: return cols + island\n    if cat in sex: return cols + sex\n\n\ncols = get_feat(f1, f2[0])\ncols\n\n['Flipper Length (mm)',\n 'Body Mass (g)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nAlthough the culmen sizes initially appeared to be better features, the statistical tests indicated that flipper length and body mass were, in fact, the more significant features."
  },
  {
    "objectID": "posts/palmer/index.html#training",
    "href": "posts/palmer/index.html#training",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The model is trained on the data with features determined from above. I had to use StandardScalar to avoid a convergence error. I used the Logistic Regression model as it is a good fit for classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\npipe = make_pipeline(StandardScaler(), LogisticRegression())\npipe.fit(X_train[cols], y_train)\npipe.score(X_train[cols], y_train)\n\n0.8984375"
  },
  {
    "objectID": "posts/palmer/index.html#testing",
    "href": "posts/palmer/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "test_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\npipe.score(X_test[cols], y_test)\n\n0.8970588235294118"
  },
  {
    "objectID": "posts/palmer/index.html#results",
    "href": "posts/palmer/index.html#results",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Most of this code is adapted from Prof.¬†Phil‚Äôs website.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nRegions for training set:\n\nplot_regions(pipe, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nRegions for testing set:\n\nplot_regions(pipe, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nLooking at the decision plots, we can see that our model is quite successful in distinguishing between Gentoo and Adelie penguins on the Biscoe and Torgersen islands. However, on Dream Island, where there is a mixture of Gentoo and Chinstrap penguins, the model struggles to differentiate between the two species.\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = pipe.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[26,  5,  0],\n       [ 2,  9,  0],\n       [ 0,  0, 26]])\n\n\nOnce again, this shows that model struggled the most with Gentoo and Chinstrap."
  },
  {
    "objectID": "posts/palmer/index.html#discussion",
    "href": "posts/palmer/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The model achieved a training accuracy of 0.89, which is the same as its testing accuracy. This indicates that the model is quite effective at predicting penguin species based on flipper length, body mass, and island. However, the decision regions suggest that the model had difficulty distinguishing between Chinstrap and Gentoo penguins on Dream Island. It appears that these two species have similar sizes, making them challenging to differentiate."
  },
  {
    "objectID": "posts/palmer/index.html#acknowledgements",
    "href": "posts/palmer/index.html#acknowledgements",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Adapted code from Professor Phil Chodrow at Middlebury College in his class CSCI 0451: Machine Learning."
  }
]