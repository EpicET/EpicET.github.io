[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Emmanuel’s blog"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @gabednick\n\n\n\n\nIn this blog post, I take the Palmer Penguins dataset and try to determine the best features to be used to determine the species of a penguin based on its measurements. Firstly, I create two figures and a table to analysize the relationships between features. Then I use sci-kit learns feature selection with chi-squared tests to pick 2 numerical features and 1 categorical feature. Then using those features, I train and test a logistic regression model. The model was fairly accurate but to understand the results better, I plot the decision regions and use a confusion matrix.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nThis is code from Professor Phil’s website. It removes the columns that we don’t use and NA values, converts the categorical feature columns into “one-hot encoded” 0-1 columns and saves the dataframe X_train. Also, “Species” is coded with the LabelEncoder and is saved as y_train.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nWe can check what the columns look like now.\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\nI created two graphs each 2 quantative columns and 1 qualitative columns. Plot 1 shows the relationship with the body mass and flipper length between different penguin species. Plot 2 shows the difference in Culmen Length and depth across different penguin species.\n\n# Get the unencoded columns for easier graphing.\nqual = train[[\"Island\", \"Sex\", \"Species\"]].dropna()\n\n# Shorten species label for the legend\nqual[\"Species\"] = qual[\"Species\"].apply(lambda x: \"Chinstrap\" if x == \"Chinstrap penguin (Pygoscelis antarctica)\" \n                                         else (\"Gentoo\" if x == \"Gentoo penguin (Pygoscelis papua)\" \n                                               else \"Adelie\"))\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-v0_8-whitegrid')\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 4))\n   \np1 = sns.scatterplot(X_train, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", hue=qual[\"Species\"], ax = ax[0])\np2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=qual[\"Species\"], ax = ax[1])\n\n\n\n\n\n\n\n\nLeft side: Plot 1. The Gentoo penguins tend to be the biggest while Chinstrap and Adelie seem to have a great overlap in size. The Adelie seem to have slightly more variety in mass given a flipper length compared to Chinstrap. This is features seem decent for selection because there is some correlation between species and body size but they are not easily distinguishable by species.\nRight side: Plot 2. Based on the graph, the each specie of penguin seemed to have distinct beaks. The Adelie have greatest culmen depth but shortest length. The Gentoo have longer culmens but not as much depth and the Chinstrap lie in the middle with medium to large culmens. These are good features because there is a clear distinction in culmen sizes between species of penquins.\n\n\n\nNow I create a summary table of the penguins measurements based on clutch completetion.\n\ntable = X_train[[\"Clutch Completion_Yes\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\ntable.groupby(\"Clutch Completion_Yes\").aggregate(['min', 'median', 'max'])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\nClutch Completion_Yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse\n35.9\n43.35\n58.0\n13.7\n17.85\n19.9\n172.0\n195.0\n225.0\n2700.0\n3737.5\n5700.0\n\n\nTrue\n34.0\n45.10\n55.9\n13.1\n17.20\n21.5\n176.0\n198.0\n230.0\n2850.0\n4100.0\n6300.0\n\n\n\n\n\n\n\nTable 1: This table shows the biggest difference between penguins that had a full clutch and those that did not was the weight. Most of the penguins the produced two eggs weighed about 300 grams more. There is most likely not a causation but there a may be a correlation. Since clutch completion does not seem impact this data very much, it may not be a feature worth looking at.\n\n\n\n\nHere I use sci-kits SelectKBest to pick the 3 feautures I’m going to use for my model. I had to seperate feature selection because all 3 selected features would be numerical. SelectKBest selects the k best features based on user specified scoring function. I used chi-squared since my feautres were meant for classification and they were non-negative.\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Selecting 2 numerical feature\nquant = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nsel1 = SelectKBest(chi2, k=2)\nsel1.fit_transform(X_train[quant], y_train)\nf1 = sel1.get_feature_names_out()\n\n# Selecting 1 categorical feature\nqual = [\"Clutch Completion_Yes\", \"Clutch Completion_No\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Sex_FEMALE\", \"Sex_MALE\"]\nsel2 = SelectKBest(chi2, k=1)\nsel2.fit_transform(X_train[qual], y_train)\nf2 = sel2.get_feature_names_out()\n\nThis function is so that I can get all the variations of the categorical feature.\n\ndef get_feat(f1, cat):\n    cols = list(f1)\n    clutch = [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n    island = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n    sex = [\"Sex_FEMALE\", \"Sex_MALE\"]\n    \n    if cat in clutch: return cols + clutch\n    if cat in island: return cols + island\n    if cat in sex: return cols + sex\n\n\ncols = get_feat(f1, f2[0])\ncols\n\n['Flipper Length (mm)',\n 'Body Mass (g)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nSo despite the culmen sizes being appearing to be better features, based on the statistical tests the flipper length and body mass were better features.\n\n\n\nThe model is trained on the data with features determined from above. I had to use StandardScalar to avoid a convergence error. I used the Logistic Regression model as it is a good fit for classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\npipe = make_pipeline(StandardScaler(), LogisticRegression())\npipe.fit(X_train[cols], y_train)\npipe.score(X_train[cols], y_train)\n\n0.8984375\n\n\n\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\npipe.score(X_test[cols], y_test)\n\n0.8970588235294118\n\n\n\n\n\n\n\nMost of this code is adapted from Prof. Phil’s website.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nRegions for training set:\n\nplot_regions(pipe, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nRegions for testing set:\n\nplot_regions(pipe, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nLooking at these decision plots, we see that our model is pretty successful determing of the difference between Gentoo and Adelie on the Biscoe and Torgersen islands. However, on the Dream island, there is a mixture of Gentoo and Chinstrap where the model the struggles to distinguish between the two.\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = pipe.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[26,  5,  0],\n       [ 2,  9,  0],\n       [ 0,  0, 26]])\n\n\nOnce again, this shows that model struggled the most with Gentoo and Chinstrap.\n\n\n\n\nThe model had very similar training and testing accuracy of 0.89. This means the model is fairly solid at predicting penguins based on the flipper lenth, body mass and island. Based on the decision regions, the model struggled to correctly determine the difference between Chinstrap and Gentoo on the Island Dream. It seems that those 2 species are of similar sizes and therefore are hard to differentiate.\nDuring this blog post, the two biggest things were I learned about different tools for feature selection and the need to normalize data before putting it in a model. In terms of feature selection, sci-kit learn provides various methods whether variance, statistical tests or recursion. I went with statistical test as it seemed more thorough then determining a random variance yet not as complex as the recursion. For normalizing data, I learned how to deal with a convergence error when trainig a model. Through some investigation I came to understand I recieved that error because some of the features may not normally distributed. I used sci-kit recommended scaling function to standardize the data and everything was fine after that. Finally, I got more practice with modifing and selecting data from dataframes in general."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emmanuel’s CSCI 0451 Blog",
    "section": "",
    "text": "Auditing Bias\n\n\n\n\n\nBlog Post 3\n\n\n\n\n\nMar 12, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nBlog Post 2\n\n\n\n\n\nFeb 19, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\nFeb 12, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog1/index.html#abstract",
    "href": "posts/blog1/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this blog post, I take the Palmer Penguins dataset and try to determine the best features to be used to determine the species of a penguin based on its measurements. Firstly, I create two figures and a table to analysize the relationships between features. Then I use sci-kit learns feature selection with chi-squared tests to pick 2 numerical features and 1 categorical feature. Then using those features, I train and test a logistic regression model. The model was fairly accurate but to understand the results better, I plot the decision regions and use a confusion matrix.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/blog1/index.html#data-preparation",
    "href": "posts/blog1/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This is code from Professor Phil’s website. It removes the columns that we don’t use and NA values, converts the categorical feature columns into “one-hot encoded” 0-1 columns and saves the dataframe X_train. Also, “Species” is coded with the LabelEncoder and is saved as y_train.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nWe can check what the columns look like now.\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/blog1/index.html#data-visualization",
    "href": "posts/blog1/index.html#data-visualization",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "I created two graphs each 2 quantative columns and 1 qualitative columns. Plot 1 shows the relationship with the body mass and flipper length between different penguin species. Plot 2 shows the difference in Culmen Length and depth across different penguin species.\n\n# Get the unencoded columns for easier graphing.\nqual = train[[\"Island\", \"Sex\", \"Species\"]].dropna()\n\n# Shorten species label for the legend\nqual[\"Species\"] = qual[\"Species\"].apply(lambda x: \"Chinstrap\" if x == \"Chinstrap penguin (Pygoscelis antarctica)\" \n                                         else (\"Gentoo\" if x == \"Gentoo penguin (Pygoscelis papua)\" \n                                               else \"Adelie\"))\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-v0_8-whitegrid')\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 4))\n   \np1 = sns.scatterplot(X_train, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", hue=qual[\"Species\"], ax = ax[0])\np2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=qual[\"Species\"], ax = ax[1])\n\n\n\n\n\n\n\n\nLeft side: Plot 1. The Gentoo penguins tend to be the biggest while Chinstrap and Adelie seem to have a great overlap in size. The Adelie seem to have slightly more variety in mass given a flipper length compared to Chinstrap. This is features seem decent for selection because there is some correlation between species and body size but they are not easily distinguishable by species.\nRight side: Plot 2. Based on the graph, the each specie of penguin seemed to have distinct beaks. The Adelie have greatest culmen depth but shortest length. The Gentoo have longer culmens but not as much depth and the Chinstrap lie in the middle with medium to large culmens. These are good features because there is a clear distinction in culmen sizes between species of penquins.\n\n\n\nNow I create a summary table of the penguins measurements based on clutch completetion.\n\ntable = X_train[[\"Clutch Completion_Yes\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\ntable.groupby(\"Clutch Completion_Yes\").aggregate(['min', 'median', 'max'])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\nClutch Completion_Yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse\n35.9\n43.35\n58.0\n13.7\n17.85\n19.9\n172.0\n195.0\n225.0\n2700.0\n3737.5\n5700.0\n\n\nTrue\n34.0\n45.10\n55.9\n13.1\n17.20\n21.5\n176.0\n198.0\n230.0\n2850.0\n4100.0\n6300.0\n\n\n\n\n\n\n\nTable 1: This table shows the biggest difference between penguins that had a full clutch and those that did not was the weight. Most of the penguins the produced two eggs weighed about 300 grams more. There is most likely not a causation but there a may be a correlation. Since clutch completion does not seem impact this data very much, it may not be a feature worth looking at."
  },
  {
    "objectID": "posts/blog1/index.html#feature-selection",
    "href": "posts/blog1/index.html#feature-selection",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Here I use sci-kits SelectKBest to pick the 3 feautures I’m going to use for my model. I had to seperate feature selection because all 3 selected features would be numerical. SelectKBest selects the k best features based on user specified scoring function. I used chi-squared since my feautres were meant for classification and they were non-negative.\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Selecting 2 numerical feature\nquant = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nsel1 = SelectKBest(chi2, k=2)\nsel1.fit_transform(X_train[quant], y_train)\nf1 = sel1.get_feature_names_out()\n\n# Selecting 1 categorical feature\nqual = [\"Clutch Completion_Yes\", \"Clutch Completion_No\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Sex_FEMALE\", \"Sex_MALE\"]\nsel2 = SelectKBest(chi2, k=1)\nsel2.fit_transform(X_train[qual], y_train)\nf2 = sel2.get_feature_names_out()\n\nThis function is so that I can get all the variations of the categorical feature.\n\ndef get_feat(f1, cat):\n    cols = list(f1)\n    clutch = [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n    island = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n    sex = [\"Sex_FEMALE\", \"Sex_MALE\"]\n    \n    if cat in clutch: return cols + clutch\n    if cat in island: return cols + island\n    if cat in sex: return cols + sex\n\n\ncols = get_feat(f1, f2[0])\ncols\n\n['Flipper Length (mm)',\n 'Body Mass (g)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nSo despite the culmen sizes being appearing to be better features, based on the statistical tests the flipper length and body mass were better features."
  },
  {
    "objectID": "posts/blog1/index.html#training",
    "href": "posts/blog1/index.html#training",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The model is trained on the data with features determined from above. I had to use StandardScalar to avoid a convergence error. I used the Logistic Regression model as it is a good fit for classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\npipe = make_pipeline(StandardScaler(), LogisticRegression())\npipe.fit(X_train[cols], y_train)\npipe.score(X_train[cols], y_train)\n\n0.8984375"
  },
  {
    "objectID": "posts/blog1/index.html#testing",
    "href": "posts/blog1/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "test_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\npipe.score(X_test[cols], y_test)\n\n0.8970588235294118"
  },
  {
    "objectID": "posts/blog1/index.html#results",
    "href": "posts/blog1/index.html#results",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Most of this code is adapted from Prof. Phil’s website.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nRegions for training set:\n\nplot_regions(pipe, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nRegions for testing set:\n\nplot_regions(pipe, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nLooking at these decision plots, we see that our model is pretty successful determing of the difference between Gentoo and Adelie on the Biscoe and Torgersen islands. However, on the Dream island, there is a mixture of Gentoo and Chinstrap where the model the struggles to distinguish between the two.\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = pipe.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[26,  5,  0],\n       [ 2,  9,  0],\n       [ 0,  0, 26]])\n\n\nOnce again, this shows that model struggled the most with Gentoo and Chinstrap."
  },
  {
    "objectID": "posts/blog1/index.html#discussion",
    "href": "posts/blog1/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The model had very similar training and testing accuracy of 0.89. This means the model is fairly solid at predicting penguins based on the flipper lenth, body mass and island. Based on the decision regions, the model struggled to correctly determine the difference between Chinstrap and Gentoo on the Island Dream. It seems that those 2 species are of similar sizes and therefore are hard to differentiate.\nDuring this blog post, the two biggest things were I learned about different tools for feature selection and the need to normalize data before putting it in a model. In terms of feature selection, sci-kit learn provides various methods whether variance, statistical tests or recursion. I went with statistical test as it seemed more thorough then determining a random variance yet not as complex as the recursion. For normalizing data, I learned how to deal with a convergence error when trainig a model. Through some investigation I came to understand I recieved that error because some of the features may not normally distributed. I used sci-kit recommended scaling function to standardize the data and everything was fine after that. Finally, I got more practice with modifing and selecting data from dataframes in general."
  },
  {
    "objectID": "posts/blog3/index.html",
    "href": "posts/blog3/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "My goal was to make a predicition of employment status based on various demographic excluding race using a subset of data from the American Community Service focused on Massachusetts residents in 2023. Based on the data of the 58500 residents, only half were employed. Most of the time men, people without disabilities, and people who born abroad or with no citizensip had higher proportions of employment. The model I used was sklearns Decision Tree Classifier because results are easy to interpret. I tuned complexity by I using GridSearchCV which cross-validated that the best depth out of the numbers I provided was 10 which overall accuracy 0.82. The different group accuracies weren’t that much different. Auditing my model showed that white people lead in PPV and FNR while Asians lead in TPR and FPR rates. In these summary I left races 5 and 6 because their data often had many missing values. Based on those values, my model failed approximate error balance and statistical parity. The plot that used the fixed PPV values and p values to graph feasible FPR and FNR combinations between Black and White residents.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MA\" \n\ndata_source = ACSDataSource(survey_year='2023', # Get more recent data\n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nSTATE\nADJINC\nPWGTP\nAGEP\nCIT\nCITWP\nCOW\nDDRS\nDEAR\nDEYE\nDOUT\nDPHY\nDRAT\nDRATX\nDREM\nENG\nFER\nGCL\nGCM\nGCR\nHIMRKS\nHINS1\nHINS2\nHINS3\nHINS4\nHINS5\nHINS6\nHINS7\nINTP\nJWMNP\nJWRIP\nJWTRNS\nLANX\nMAR\n...\nPWGTP41\nPWGTP42\nPWGTP43\nPWGTP44\nPWGTP45\nPWGTP46\nPWGTP47\nPWGTP48\nPWGTP49\nPWGTP50\nPWGTP51\nPWGTP52\nPWGTP53\nPWGTP54\nPWGTP55\nPWGTP56\nPWGTP57\nPWGTP58\nPWGTP59\nPWGTP60\nPWGTP61\nPWGTP62\nPWGTP63\nPWGTP64\nPWGTP65\nPWGTP66\nPWGTP67\nPWGTP68\nPWGTP69\nPWGTP70\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2023GQ0000077\n1\n1\n503\n1\n25\n1019518\n11\n89\n1\nNaN\nNaN\n1.0\n1\n1\n1.0\n1.0\nNaN\nNaN\n1.0\nNaN\nNaN\n2.0\nNaN\nNaN\n0\n2\n2\n1\n2\n2\n2\n2\n0.0\nNaN\nNaN\nNaN\n2.0\n2\n...\n13\n13\n12\n13\n12\n13\n13\n13\n13\n13\n13\n13\n13\n12\n12\n11\n13\n14\n13\n12\n13\n11\n12\n11\n13\n12\n15\n12\n11\n13\n12\n13\n13\n13\n13\n13\n13\n12\n13\n13\n\n\n1\nP\n2023GQ0000098\n1\n1\n613\n1\n25\n1019518\n11\n20\n1\nNaN\n1.0\n2.0\n2\n2\n2.0\n2.0\nNaN\nNaN\n2.0\n1.0\nNaN\nNaN\nNaN\nNaN\n0\n1\n2\n2\n2\n2\n2\n2\n0.0\nNaN\nNaN\nNaN\n1.0\n5\n...\n21\n13\n2\n2\n10\n21\n1\n19\n10\n11\n11\n13\n12\n9\n3\n19\n11\n11\n21\n11\n12\n21\n11\n13\n19\n11\n11\n11\n18\n1\n3\n4\n2\n20\n13\n9\n2\n20\n13\n2\n\n\n2\nP\n2023GQ0000109\n1\n1\n613\n1\n25\n1019518\n80\n68\n1\nNaN\nNaN\n2.0\n1\n2\n1.0\n1.0\nNaN\nNaN\n2.0\nNaN\nNaN\n2.0\nNaN\nNaN\n0\n2\n2\n1\n1\n2\n2\n2\n0.0\nNaN\nNaN\nNaN\n2.0\n5\n...\n94\n90\n84\n78\n95\n95\n84\n95\n94\n87\n82\n80\n84\n91\n78\n90\n85\n94\n95\n27\n16\n22\n81\n25\n66\n16\n81\n69\n70\n79\n28\n34\n78\n73\n34\n68\n82\n15\n17\n79\n\n\n3\nP\n2023GQ0000114\n1\n1\n801\n1\n25\n1019518\n69\n21\n1\nNaN\n2.0\n2.0\n2\n2\n2.0\n2.0\nNaN\nNaN\n2.0\nNaN\n2.0\nNaN\nNaN\nNaN\n0\n1\n2\n2\n2\n2\n2\n2\n0.0\n20.0\n1.0\n1.0\n2.0\n5\n...\n132\n64\n62\n12\n135\n12\n12\n68\n77\n129\n132\n9\n60\n63\n57\n9\n69\n67\n62\n58\n69\n130\n12\n75\n71\n73\n63\n132\n123\n78\n60\n74\n161\n11\n127\n57\n11\n12\n11\n12\n\n\n4\nP\n2023GQ0000135\n1\n1\n1201\n1\n25\n1019518\n27\n84\n1\nNaN\nNaN\n2.0\n2\n1\n2.0\n2.0\nNaN\nNaN\n1.0\nNaN\nNaN\n2.0\nNaN\nNaN\n0\n2\n2\n1\n2\n2\n2\n2\n0.0\nNaN\nNaN\nNaN\n2.0\n1\n...\n27\n29\n27\n25\n29\n30\n29\n29\n29\n29\n26\n27\n27\n27\n27\n27\n27\n29\n28\n27\n25\n27\n26\n27\n27\n25\n28\n28\n24\n27\n27\n28\n27\n29\n27\n29\n27\n27\n28\n27\n\n\n\n\n5 rows × 287 columns\n\n\n\n\n# No RELP avaiable \npossible_features=['AGEP', 'SCHL', 'MAR', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n89\n16.0\n2\n1\nNaN\n1\n1.0\n4.0\n3\n1\n1\n1\n1.0\n2\n1\n6.0\n\n\n1\n20\n16.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n2\n68\n18.0\n5\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n2.0\n1\n1\n6.0\n\n\n3\n21\n19.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n1.0\n\n\n4\n84\n16.0\n1\n1\nNaN\n1\n3.0\n4.0\n3\n1\n2\n1\n1.0\n2\n1\n6.0\n\n\n\n\n\n\n\n\n\n\n\n\nAdapted from Professor Phil’s code.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(73126, 14)\n(73126,)\n(73126,)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"race\"] = group_train\ndf[\"label\"] = y_train\n\nThis method allows the conversion of race numbers to more helpful categorical labels. Some of them have been shortened because they were too long. 1. SPAA - “American Indian and Alaska Native tribes specified, or American Indian or AlaskaNative, not specified and no other races”. 2. NPI - Native Hawaiian and Other Pacific Islander alone\n\ndef convert_race(df: pd.DataFrame):\n    df = df.sort_values(by='race')\n    df['race'] = df['race'].replace({1: \"White\", 2: \"Black\", 3: \"N. American\", 4:\"N. Alaskan\", \n                        5:\"SPAA\", \n                        6:'Asian', 7: 'NPI', 8:'Other', 9: 'Multi'})\n\n    df['race'] = pd.Categorical(df['race'])\n    return df\n    \n\n\n\n\n\n# Save to the original for calculations and copying\nrelabeled = df.copy() \nrelabeled = convert_race(relabeled)\nrelabeled\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nrace\nlabel\n\n\n\n\n0\n69.0\n19.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nFalse\n\n\n35997\n12.0\n9.0\n5.0\n2.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nFalse\n\n\n35999\n70.0\n24.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\nWhite\nTrue\n\n\n36000\n74.0\n22.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\nWhite\nFalse\n\n\n36001\n68.0\n23.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n51260\n6.0\n3.0\n5.0\n2.0\n2.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n51257\n7.0\n4.0\n5.0\n2.0\n7.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n5335\n14.0\n11.0\n5.0\n2.0\n3.0\n1.0\n1.0\n0.0\n3.0\n1.0\n2.0\n2.0\n2.0\n1.0\nMulti\nFalse\n\n\n34217\n18.0\n14.0\n5.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n40473\n18.0\n18.0\n5.0\n2.0\n0.0\n1.0\n3.0\n4.0\n4.0\n1.0\n2.0\n2.0\n2.0\n1.0\nMulti\nTrue\n\n\n\n\n58500 rows × 16 columns\n\n\n\n\nThe number of indiviuals in this df are\n\n\nprint(\"The number of indiviuals in this df are\", df.shape[0])\n\nThe number of indiviuals in this df are 58500\n\n\n\nThe proportion of employed individuals are\n\n\nemp = df[df['label'] == True][[\"label\"]].size # 29703\ntotal = df['label'].size # 58500\nemp_prop = emp / total\nprint(\"The proportion of employed indiviuals are\", emp_prop)\n\nThe proportion of employed indiviuals are 0.5077435897435898\n\n\n\nThe population of each group\n\n\nrelabeled.groupby('race')[['label']].aggregate('sum')\n\n/var/folders/t8/bssz0nhj0h94fjkhfd648d280000gn/T/ipykernel_70250/1987727674.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  relabeled.groupby('race')[['label']].aggregate('sum')\n\n\n\n\n\n\n\n\n\nlabel\n\n\nrace\n\n\n\n\n\nAsian\n2399\n\n\nBlack\n1529\n\n\nMulti\n2478\n\n\nN. American\n52\n\n\nNPI\n15\n\n\nOther\n1284\n\n\nSPAA\n26\n\n\nWhite\n21920\n\n\n\n\n\n\n\nBased on this, I might consider whether groups with less than 30 labels are statistically significant\n\nThe proportion of employed people in each group are:\n\n\nrelabeled.groupby('race')[['label']].aggregate('mean')\n\n/var/folders/t8/bssz0nhj0h94fjkhfd648d280000gn/T/ipykernel_70250/802482079.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  relabeled.groupby('race')[['label']].aggregate('mean')\n\n\n\n\n\n\n\n\n\nlabel\n\n\nrace\n\n\n\n\n\nAsian\n0.540315\n\n\nBlack\n0.473668\n\n\nMulti\n0.456270\n\n\nN. American\n0.530612\n\n\nNPI\n0.625000\n\n\nOther\n0.486364\n\n\nSPAA\n0.520000\n\n\nWhite\n0.514687\n\n\n\n\n\n\n\n\nIntersectional Trends\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize = (14, 4))\n\ncolors = [\"#1f77b4\", \"#ff7f0e\"]\nplt1 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"SEX\", palette=colors, ax=ax[0])\nplt1.set_xlabel(\"Race\")\nplt1.set_ylabel(\"Employment\")\nhandles, _ = plt1.get_legend_handles_labels()\nplt1.legend(handles=handles, title='Sex', labels=['Male', 'Female'])\n\nplt2 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"DIS\", palette=colors, ax=ax[1])\nplt2.set_xlabel(\"Race\")\nplt2.set_ylabel(\"Employment\")\nhandles, _ = plt2.get_legend_handles_labels()\nplt2.legend(handles=handles, title='Disability', labels=['Has disability', 'No disability'])\nplt.show()\n\n\n\n\n\n\n\n\nIn this we see intersectionality between employment based on the sex of the person or whether they have a disability. As we can see in both graphs generally male and able bodied people are more likely to be employed. Multiple factors can contribute to this desparity. One is that women and people with disabilities are discriminated against by employers. Another is also considering how many of them are applying for jobs. For women, even with more of them joining the workforce, they may be more likely to doing childcare at home. Because disabilites is such a broad it’s hard which may be capable of working and those that aren’t. This probably a big factor why relatively few are employed. Something I noticed is that for Black Americans, Native Americans and SPAA more females are employed than males.\n\nplt3 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"CIT\")\nplt3.set_xlabel(\"Race\")\nplt3.set_ylabel(\"Employment\")\nhandles, _ = plt3.get_legend_handles_labels()\nstatus = ['Born in US', 'Born in US Territories', 'Born Abroad', 'US by naturalization', 'Not a citizen']\nplt3.legend(handles=handles, title='Citizenship', labels=status, loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we see that for people born in the U.S or it’s territories have lower percentage employment than people born abroad or by naturalization. I suspect this is might be due to the smaller populations and people to who move here are more likely to have specific work purposes.\n\n\n\nThe decision tree classifier was one of the recommedations and scikit learn said it was easy to interpret. In order to find the best depth for I used GridSearchCV which cross validated the best paramaters.\n\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n\nparam_grid = {\n    'max_depth': [1, 3, 5, 10]\n}\n\nmodel = DecisionTreeClassifier() \ngrid = GridSearchCV(model, param_grid, scoring=\"accuracy\")\ngrid.fit(X_train, y_train)\n\nprint(\"Best params\", grid.best_params_)\n\nbest_dtree = grid.best_estimator_\n\nbest_dtree.predict(X_train)\nbest_dtree.score(X_train, y_train)\n\nBest params {'max_depth': 10}\n\n\n0.8346495726495726\n\n\nThe best depth was 10 and gave us a solid accuracy.\n\n\n\n\npred = best_dtree.predict(X_test)\n\n\n\n\n\n\n\nThe overall accuracy:\n\n(pred == y_test).mean()\n\nnp.float64(0.8222343771366061)\n\n\nHere we calculate the PPV, the false negative and false positive rate\n\ntn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n\nppv = tp / (tp + fp)\nfnr = fn / (fn + tp)\nfpr = tp / (tp + tn)\n\nprint(\"Overall PPV:\", ppv)\nprint(\"Overall false negative rate:\", fnr)\nprint(\"Overall false positive rate:\", fpr)\n\nOverall PPV: 0.7942554240631163\nOverall false negative rate: 0.1262544073772715\nOverall false positive rate: 0.5357558622983536\n\n\nBased on this the model is pretty good at predicting who’s not employed but seems to overestimate the amount of employed.\n\n\n\nThis is the accuracy by group:\n\ngroups = df[\"race\"].unique() # Need race as numbers in order to compare with group_test\naudit = pd.DataFrame(groups, columns=[\"race\"])\n\naccuracies = []\nfor group in groups:\n   accuracy = (pred == y_test)[group_test == group].mean()\n   accuracies.append(accuracy)\n\naudit[\"accuracy\"] = accuracies\naudit = convert_race(audit)\naudit\n\n\n\n\n\n\n\n\nrace\naccuracy\n\n\n\n\n0\nWhite\n0.824527\n\n\n3\nBlack\n0.799747\n\n\n7\nN. American\n0.843750\n\n\n6\nSPAA\n1.000000\n\n\n4\nAsian\n0.820998\n\n\n5\nNPI\n0.500000\n\n\n1\nOther\n0.807092\n\n\n2\nMulti\n0.825856\n\n\n\n\n\n\n\nBasides NPI and SPAA, the accuracies are rather similar to each other.\nThis is the PPV, the false negative and false positive rate by group:\n\nppvs = []\ntprs = []\nfnrs = []\nfprs = []\n\nfor group in groups:\n    tp = int(0)\n    fp = int(0)\n    tn = int(0)\n    fn = int(0)\n    for n, m, grp in zip(y_test, pred, group_test):\n        if(grp == group):\n            if m == n:\n                if n == True:\n                    tp += 1\n                if n == False:\n                    tn += 1\n            if m != n:\n                if n == True:\n                    fn += 1\n                if n == False:\n                    fp += 1\n    ppv = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n    ppvs.append(ppv)\n    fnr = fn / (fn + tp)  if (fn + tp) &gt; 0 else 0\n    fnrs.append(fnr)\n    fpr = fp / (fp + tn) if (fp + tn) &gt; 0 else 0 \n    fprs.append(fpr)\n    tpr = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n    tprs.append(tpr)\n    \n\naudit[\"ppv\"] = ppvs\naudit[\"tpr\"] = tprs\naudit[\"fpr\"] = fprs\naudit[\"fnr\"] = fnrs\naudit\n\n1 1.0\n8 0.8691070438158625\n9 0.8895522388059701\n2 0.8733108108108109\n6 0.8770491803278688\n7 0.9016641452344932\n5 0.0\n3 0\n\n\n\n\n\n\n\n\n\nrace\naccuracy\nppv\ntpr\nfpr\nfnr\np\n\n\n\n\n0\nWhite\n0.824527\n0.802766\n0.869107\n0.221774\n0.130893\n0.509466\n\n\n3\nBlack\n0.799747\n0.750630\n0.889552\n0.267568\n0.110448\n0.475177\n\n\n7\nN. American\n0.843750\n0.770492\n0.873311\n0.213001\n0.126689\n0.450190\n\n\n6\nSPAA\n1.000000\n0.739631\n0.877049\n0.267139\n0.122951\n0.463878\n\n\n4\nAsian\n0.820998\n0.806495\n0.901664\n0.285429\n0.098336\n0.568847\n\n\n5\nNPI\n0.500000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n1\nOther\n0.807092\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\nMulti\n0.825856\n0.666667\n1.000000\n0.227273\n0.000000\n0.312500\n\n\n\n\n\n\n\n\n\n\nApproximately calibrated:\n\ncal = pd.DataFrame(pred)\ncal[\"prediction\"] = pred\ncal.groupby(\"prediction\").count()\n\n\n\n\n\n\n\n\n0\n\n\nprediction\n\n\n\n\n\nFalse\n6514\n\n\nTrue\n8112\n\n\n\n\n\n\n\nThis table shows that values either True or False, therefore this model is calibrated.\nApproximate error balance rate: Looking at the table dataframe above we can see that the model does not meet approximate error rate balance for groups. The groups differ in true and false positive rates. The code below double checks.\n\nfor i, row1 in audit.iterrows():\n    equal = True;\n    race1 = row1['race']\n    tpr1 = row1['tpr']\n    fpr1 = row1['fpr']\n    for j, row2 in audit.iterrows():\n        race2 = row2['race']\n        tpr2 = row2['tpr']\n        fpr2 = row2['fpr']\n        if(tpr1 != tpr2 or fpr1 != fpr2):\n            equal = False\n            print(f\"{race1} did not have an equal TPR or FPR as {race2}\")\n            break\n    if(equal != True):\n        break\n        \n\nWhite did not have an equal TPR or FPR as Black\n\n\nStatistical parity:\n\nprobs = []\nfor group in groups:\n    prob = (pred == True)[group_test == group].mean()\n    probs.append(prob)\n\nparity= pd.DataFrame(groups, columns=[\"race\"])\nparity[\"prob\"] = probs\nparity\n\n\n\n\n\n\n\n\nrace\nprob\n\n\n\n\n0\n1\n0.551568\n\n\n1\n8\n0.563121\n\n\n2\n9\n0.510266\n\n\n3\n2\n0.550063\n\n\n4\n6\n0.635972\n\n\n5\n7\n0.000000\n\n\n6\n5\n0.000000\n\n\n7\n3\n0.468750\n\n\n\n\n\n\n\nThe model does not meet statistical parity which means not all groups have an equal change of achieving favorable odds. Therefore we can assume that the probability of predicting employment is not independent of race.\nAdd prevalance to data table\n\naudit[\"p\"] = (1 + (audit[\"tpr\"] / audit[\"fpr\"]) * ((1 - audit[\"ppv\"])/(audit[\"ppv\"]))) ** -1\naudit[\"p\"] = audit[\"p\"].fillna(0)\naudit\nprint(audit)\n\n          race  accuracy       ppv       tpr       fpr       fnr         p\n0        White  0.824527  0.802766  0.869107  0.221774  0.130893  0.509466\n3        Black  0.799747  0.750630  0.889552  0.267568  0.110448  0.475177\n7  N. American  0.843750  0.770492  0.873311  0.213001  0.126689  0.450190\n6         SPAA  1.000000  0.739631  0.877049  0.267139  0.122951  0.463878\n4        Asian  0.820998  0.806495  0.901664  0.285429  0.098336  0.568847\n5          NPI  0.500000  0.000000  0.000000  0.000000  1.000000  0.000000\n1        Other  0.807092  0.000000  0.000000  0.000000  0.000000  0.000000\n2        Multi  0.825856  0.666667  1.000000  0.227273  0.000000  0.312500\n\n\nHere I plot the feasibility of FPR and FNR for Black and White groups.\n\nimport seaborn as sns\n\n# Filter to only include Black and White groups\nfiltered = audit[audit[\"race\"].isin([\"Black\", \"White\"])]\n\norange_color = \"#E69F00\"\nblack_color = \"#000000\"\n\n# A cleaner table\nfeasible = filtered[[\"race\", \"fpr\", \"fnr\", \"p\"]].copy() \nlines = []\n\n# Make fixed ppv based on the white ppv\nfixed_ppv = filtered.loc[filtered[\"race\"] == \"White\", \"ppv\"].values[0]\nfnr_range = np.linspace(0, 1, 100)\n\n# Compute feasible FPR for different FNR values\nfor i, row in feasible.iterrows():\n    race = row[\"race\"]\n    p = row[\"p\"]  \n    \n    fprs = (p / (1 - p)) * ((1 - fixed_ppv) / fixed_ppv) * (1 - fnr_range)\n\n    for fnr, fpr in zip(fnr_range, fprs):\n        lines.append({\"race\": race, \"fnr\": fnr, \"fpr\": fpr})\n\nlines_df = pd.DataFrame(lines)\n\nplt.figure(figsize=(7, 5))\nsns.set_style(\"whitegrid\")\n\n# Plot observed (fnr, fpr)\nfor i, row in feasible.iterrows():\n    color = orange_color if row[\"race\"] == \"White\" else black_color\n    plt.scatter(row[\"fnr\"], row[\"fpr\"], color=color)\n\n# Plot feasible (fnr, fpr) line\nfor race, color in zip([\"White\", \"Black\"], [orange_color, black_color]):\n    line = lines_df[lines_df[\"race\"] == race]\n    plt.plot(line[\"fnr\"], line[\"fpr\"], color=color)\n\nplt.xlabel(\"False Negative Rate\")\nplt.ylabel(\"False Positive Rate\")\nplt.title(\"Feasible (FNR, FPR) combinations\")\nplt.show()\n\n\n\n\n\n\n\n\nTo get equal false positive rates, we would need to make a large increase to the FNR.\n\n\n\n\n\nA bank may use employment status to determine to give a lone to someone.\nSince my model proved not to be fair, then using for commercial or government reasons would perpetuate inequalities.\nBased on the results for statistical parity and error rate balance the results my model does display problematic biases. Some groups are more likely to be misclassified than others.\nI think transparency might be an issue, if the algorithm is not shared to the public. I’m unsure if this bias but smaller populations have very weird data points that probably accurate reflect what’s going on."
  },
  {
    "objectID": "posts/blog3/index.html#abstract",
    "href": "posts/blog3/index.html#abstract",
    "title": "Auditing Bias",
    "section": "",
    "text": "My goal was to make a predicition of employment status based on various demographic excluding race using a subset of data from the American Community Service focused on Massachusetts residents in 2023. Based on the data of the 58500 residents, only half were employed. Most of the time men, people without disabilities, and people who born abroad or with no citizensip had higher proportions of employment. The model I used was sklearns Decision Tree Classifier because results are easy to interpret. I tuned complexity by I using GridSearchCV which cross-validated that the best depth out of the numbers I provided was 10 which overall accuracy 0.82. The different group accuracies weren’t that much different. Auditing my model showed that white people lead in PPV and FNR while Asians lead in TPR and FPR rates. In these summary I left races 5 and 6 because their data often had many missing values. Based on those values, my model failed approximate error balance and statistical parity. The plot that used the fixed PPV values and p values to graph feasible FPR and FNR combinations between Black and White residents.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MA\" \n\ndata_source = ACSDataSource(survey_year='2023', # Get more recent data\n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nSTATE\nADJINC\nPWGTP\nAGEP\nCIT\nCITWP\nCOW\nDDRS\nDEAR\nDEYE\nDOUT\nDPHY\nDRAT\nDRATX\nDREM\nENG\nFER\nGCL\nGCM\nGCR\nHIMRKS\nHINS1\nHINS2\nHINS3\nHINS4\nHINS5\nHINS6\nHINS7\nINTP\nJWMNP\nJWRIP\nJWTRNS\nLANX\nMAR\n...\nPWGTP41\nPWGTP42\nPWGTP43\nPWGTP44\nPWGTP45\nPWGTP46\nPWGTP47\nPWGTP48\nPWGTP49\nPWGTP50\nPWGTP51\nPWGTP52\nPWGTP53\nPWGTP54\nPWGTP55\nPWGTP56\nPWGTP57\nPWGTP58\nPWGTP59\nPWGTP60\nPWGTP61\nPWGTP62\nPWGTP63\nPWGTP64\nPWGTP65\nPWGTP66\nPWGTP67\nPWGTP68\nPWGTP69\nPWGTP70\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2023GQ0000077\n1\n1\n503\n1\n25\n1019518\n11\n89\n1\nNaN\nNaN\n1.0\n1\n1\n1.0\n1.0\nNaN\nNaN\n1.0\nNaN\nNaN\n2.0\nNaN\nNaN\n0\n2\n2\n1\n2\n2\n2\n2\n0.0\nNaN\nNaN\nNaN\n2.0\n2\n...\n13\n13\n12\n13\n12\n13\n13\n13\n13\n13\n13\n13\n13\n12\n12\n11\n13\n14\n13\n12\n13\n11\n12\n11\n13\n12\n15\n12\n11\n13\n12\n13\n13\n13\n13\n13\n13\n12\n13\n13\n\n\n1\nP\n2023GQ0000098\n1\n1\n613\n1\n25\n1019518\n11\n20\n1\nNaN\n1.0\n2.0\n2\n2\n2.0\n2.0\nNaN\nNaN\n2.0\n1.0\nNaN\nNaN\nNaN\nNaN\n0\n1\n2\n2\n2\n2\n2\n2\n0.0\nNaN\nNaN\nNaN\n1.0\n5\n...\n21\n13\n2\n2\n10\n21\n1\n19\n10\n11\n11\n13\n12\n9\n3\n19\n11\n11\n21\n11\n12\n21\n11\n13\n19\n11\n11\n11\n18\n1\n3\n4\n2\n20\n13\n9\n2\n20\n13\n2\n\n\n2\nP\n2023GQ0000109\n1\n1\n613\n1\n25\n1019518\n80\n68\n1\nNaN\nNaN\n2.0\n1\n2\n1.0\n1.0\nNaN\nNaN\n2.0\nNaN\nNaN\n2.0\nNaN\nNaN\n0\n2\n2\n1\n1\n2\n2\n2\n0.0\nNaN\nNaN\nNaN\n2.0\n5\n...\n94\n90\n84\n78\n95\n95\n84\n95\n94\n87\n82\n80\n84\n91\n78\n90\n85\n94\n95\n27\n16\n22\n81\n25\n66\n16\n81\n69\n70\n79\n28\n34\n78\n73\n34\n68\n82\n15\n17\n79\n\n\n3\nP\n2023GQ0000114\n1\n1\n801\n1\n25\n1019518\n69\n21\n1\nNaN\n2.0\n2.0\n2\n2\n2.0\n2.0\nNaN\nNaN\n2.0\nNaN\n2.0\nNaN\nNaN\nNaN\n0\n1\n2\n2\n2\n2\n2\n2\n0.0\n20.0\n1.0\n1.0\n2.0\n5\n...\n132\n64\n62\n12\n135\n12\n12\n68\n77\n129\n132\n9\n60\n63\n57\n9\n69\n67\n62\n58\n69\n130\n12\n75\n71\n73\n63\n132\n123\n78\n60\n74\n161\n11\n127\n57\n11\n12\n11\n12\n\n\n4\nP\n2023GQ0000135\n1\n1\n1201\n1\n25\n1019518\n27\n84\n1\nNaN\nNaN\n2.0\n2\n1\n2.0\n2.0\nNaN\nNaN\n1.0\nNaN\nNaN\n2.0\nNaN\nNaN\n0\n2\n2\n1\n2\n2\n2\n2\n0.0\nNaN\nNaN\nNaN\n2.0\n1\n...\n27\n29\n27\n25\n29\n30\n29\n29\n29\n29\n26\n27\n27\n27\n27\n27\n27\n29\n28\n27\n25\n27\n26\n27\n27\n25\n28\n28\n24\n27\n27\n28\n27\n29\n27\n29\n27\n27\n28\n27\n\n\n\n\n5 rows × 287 columns\n\n\n\n\n# No RELP avaiable \npossible_features=['AGEP', 'SCHL', 'MAR', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n89\n16.0\n2\n1\nNaN\n1\n1.0\n4.0\n3\n1\n1\n1\n1.0\n2\n1\n6.0\n\n\n1\n20\n16.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n2\n68\n18.0\n5\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n2.0\n1\n1\n6.0\n\n\n3\n21\n19.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n1.0\n\n\n4\n84\n16.0\n1\n1\nNaN\n1\n3.0\n4.0\n3\n1\n2\n1\n1.0\n2\n1\n6.0"
  },
  {
    "objectID": "posts/blog3/index.html#model",
    "href": "posts/blog3/index.html#model",
    "title": "Auditing Bias",
    "section": "",
    "text": "Adapted from Professor Phil’s code.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(73126, 14)\n(73126,)\n(73126,)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"race\"] = group_train\ndf[\"label\"] = y_train\n\nThis method allows the conversion of race numbers to more helpful categorical labels. Some of them have been shortened because they were too long. 1. SPAA - “American Indian and Alaska Native tribes specified, or American Indian or AlaskaNative, not specified and no other races”. 2. NPI - Native Hawaiian and Other Pacific Islander alone\n\ndef convert_race(df: pd.DataFrame):\n    df = df.sort_values(by='race')\n    df['race'] = df['race'].replace({1: \"White\", 2: \"Black\", 3: \"N. American\", 4:\"N. Alaskan\", \n                        5:\"SPAA\", \n                        6:'Asian', 7: 'NPI', 8:'Other', 9: 'Multi'})\n\n    df['race'] = pd.Categorical(df['race'])\n    return df\n    \n\n\n\n\n\n# Save to the original for calculations and copying\nrelabeled = df.copy() \nrelabeled = convert_race(relabeled)\nrelabeled\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nrace\nlabel\n\n\n\n\n0\n69.0\n19.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nFalse\n\n\n35997\n12.0\n9.0\n5.0\n2.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nFalse\n\n\n35999\n70.0\n24.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\nWhite\nTrue\n\n\n36000\n74.0\n22.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\nWhite\nFalse\n\n\n36001\n68.0\n23.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n51260\n6.0\n3.0\n5.0\n2.0\n2.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n51257\n7.0\n4.0\n5.0\n2.0\n7.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n5335\n14.0\n11.0\n5.0\n2.0\n3.0\n1.0\n1.0\n0.0\n3.0\n1.0\n2.0\n2.0\n2.0\n1.0\nMulti\nFalse\n\n\n34217\n18.0\n14.0\n5.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n40473\n18.0\n18.0\n5.0\n2.0\n0.0\n1.0\n3.0\n4.0\n4.0\n1.0\n2.0\n2.0\n2.0\n1.0\nMulti\nTrue\n\n\n\n\n58500 rows × 16 columns\n\n\n\n\nThe number of indiviuals in this df are\n\n\nprint(\"The number of indiviuals in this df are\", df.shape[0])\n\nThe number of indiviuals in this df are 58500\n\n\n\nThe proportion of employed individuals are\n\n\nemp = df[df['label'] == True][[\"label\"]].size # 29703\ntotal = df['label'].size # 58500\nemp_prop = emp / total\nprint(\"The proportion of employed indiviuals are\", emp_prop)\n\nThe proportion of employed indiviuals are 0.5077435897435898\n\n\n\nThe population of each group\n\n\nrelabeled.groupby('race')[['label']].aggregate('sum')\n\n/var/folders/t8/bssz0nhj0h94fjkhfd648d280000gn/T/ipykernel_70250/1987727674.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  relabeled.groupby('race')[['label']].aggregate('sum')\n\n\n\n\n\n\n\n\n\nlabel\n\n\nrace\n\n\n\n\n\nAsian\n2399\n\n\nBlack\n1529\n\n\nMulti\n2478\n\n\nN. American\n52\n\n\nNPI\n15\n\n\nOther\n1284\n\n\nSPAA\n26\n\n\nWhite\n21920\n\n\n\n\n\n\n\nBased on this, I might consider whether groups with less than 30 labels are statistically significant\n\nThe proportion of employed people in each group are:\n\n\nrelabeled.groupby('race')[['label']].aggregate('mean')\n\n/var/folders/t8/bssz0nhj0h94fjkhfd648d280000gn/T/ipykernel_70250/802482079.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  relabeled.groupby('race')[['label']].aggregate('mean')\n\n\n\n\n\n\n\n\n\nlabel\n\n\nrace\n\n\n\n\n\nAsian\n0.540315\n\n\nBlack\n0.473668\n\n\nMulti\n0.456270\n\n\nN. American\n0.530612\n\n\nNPI\n0.625000\n\n\nOther\n0.486364\n\n\nSPAA\n0.520000\n\n\nWhite\n0.514687\n\n\n\n\n\n\n\n\nIntersectional Trends\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize = (14, 4))\n\ncolors = [\"#1f77b4\", \"#ff7f0e\"]\nplt1 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"SEX\", palette=colors, ax=ax[0])\nplt1.set_xlabel(\"Race\")\nplt1.set_ylabel(\"Employment\")\nhandles, _ = plt1.get_legend_handles_labels()\nplt1.legend(handles=handles, title='Sex', labels=['Male', 'Female'])\n\nplt2 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"DIS\", palette=colors, ax=ax[1])\nplt2.set_xlabel(\"Race\")\nplt2.set_ylabel(\"Employment\")\nhandles, _ = plt2.get_legend_handles_labels()\nplt2.legend(handles=handles, title='Disability', labels=['Has disability', 'No disability'])\nplt.show()\n\n\n\n\n\n\n\n\nIn this we see intersectionality between employment based on the sex of the person or whether they have a disability. As we can see in both graphs generally male and able bodied people are more likely to be employed. Multiple factors can contribute to this desparity. One is that women and people with disabilities are discriminated against by employers. Another is also considering how many of them are applying for jobs. For women, even with more of them joining the workforce, they may be more likely to doing childcare at home. Because disabilites is such a broad it’s hard which may be capable of working and those that aren’t. This probably a big factor why relatively few are employed. Something I noticed is that for Black Americans, Native Americans and SPAA more females are employed than males.\n\nplt3 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"CIT\")\nplt3.set_xlabel(\"Race\")\nplt3.set_ylabel(\"Employment\")\nhandles, _ = plt3.get_legend_handles_labels()\nstatus = ['Born in US', 'Born in US Territories', 'Born Abroad', 'US by naturalization', 'Not a citizen']\nplt3.legend(handles=handles, title='Citizenship', labels=status, loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we see that for people born in the U.S or it’s territories have lower percentage employment than people born abroad or by naturalization. I suspect this is might be due to the smaller populations and people to who move here are more likely to have specific work purposes.\n\n\n\nThe decision tree classifier was one of the recommedations and scikit learn said it was easy to interpret. In order to find the best depth for I used GridSearchCV which cross validated the best paramaters.\n\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n\nparam_grid = {\n    'max_depth': [1, 3, 5, 10]\n}\n\nmodel = DecisionTreeClassifier() \ngrid = GridSearchCV(model, param_grid, scoring=\"accuracy\")\ngrid.fit(X_train, y_train)\n\nprint(\"Best params\", grid.best_params_)\n\nbest_dtree = grid.best_estimator_\n\nbest_dtree.predict(X_train)\nbest_dtree.score(X_train, y_train)\n\nBest params {'max_depth': 10}\n\n\n0.8346495726495726\n\n\nThe best depth was 10 and gave us a solid accuracy.\n\n\n\n\npred = best_dtree.predict(X_test)"
  },
  {
    "objectID": "posts/blog3/index.html#bias-audit",
    "href": "posts/blog3/index.html#bias-audit",
    "title": "Auditing Bias",
    "section": "",
    "text": "The overall accuracy:\n\n(pred == y_test).mean()\n\nnp.float64(0.8222343771366061)\n\n\nHere we calculate the PPV, the false negative and false positive rate\n\ntn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n\nppv = tp / (tp + fp)\nfnr = fn / (fn + tp)\nfpr = tp / (tp + tn)\n\nprint(\"Overall PPV:\", ppv)\nprint(\"Overall false negative rate:\", fnr)\nprint(\"Overall false positive rate:\", fpr)\n\nOverall PPV: 0.7942554240631163\nOverall false negative rate: 0.1262544073772715\nOverall false positive rate: 0.5357558622983536\n\n\nBased on this the model is pretty good at predicting who’s not employed but seems to overestimate the amount of employed.\n\n\n\nThis is the accuracy by group:\n\ngroups = df[\"race\"].unique() # Need race as numbers in order to compare with group_test\naudit = pd.DataFrame(groups, columns=[\"race\"])\n\naccuracies = []\nfor group in groups:\n   accuracy = (pred == y_test)[group_test == group].mean()\n   accuracies.append(accuracy)\n\naudit[\"accuracy\"] = accuracies\naudit = convert_race(audit)\naudit\n\n\n\n\n\n\n\n\nrace\naccuracy\n\n\n\n\n0\nWhite\n0.824527\n\n\n3\nBlack\n0.799747\n\n\n7\nN. American\n0.843750\n\n\n6\nSPAA\n1.000000\n\n\n4\nAsian\n0.820998\n\n\n5\nNPI\n0.500000\n\n\n1\nOther\n0.807092\n\n\n2\nMulti\n0.825856\n\n\n\n\n\n\n\nBasides NPI and SPAA, the accuracies are rather similar to each other.\nThis is the PPV, the false negative and false positive rate by group:\n\nppvs = []\ntprs = []\nfnrs = []\nfprs = []\n\nfor group in groups:\n    tp = int(0)\n    fp = int(0)\n    tn = int(0)\n    fn = int(0)\n    for n, m, grp in zip(y_test, pred, group_test):\n        if(grp == group):\n            if m == n:\n                if n == True:\n                    tp += 1\n                if n == False:\n                    tn += 1\n            if m != n:\n                if n == True:\n                    fn += 1\n                if n == False:\n                    fp += 1\n    ppv = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n    ppvs.append(ppv)\n    fnr = fn / (fn + tp)  if (fn + tp) &gt; 0 else 0\n    fnrs.append(fnr)\n    fpr = fp / (fp + tn) if (fp + tn) &gt; 0 else 0 \n    fprs.append(fpr)\n    tpr = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n    tprs.append(tpr)\n    \n\naudit[\"ppv\"] = ppvs\naudit[\"tpr\"] = tprs\naudit[\"fpr\"] = fprs\naudit[\"fnr\"] = fnrs\naudit\n\n1 1.0\n8 0.8691070438158625\n9 0.8895522388059701\n2 0.8733108108108109\n6 0.8770491803278688\n7 0.9016641452344932\n5 0.0\n3 0\n\n\n\n\n\n\n\n\n\nrace\naccuracy\nppv\ntpr\nfpr\nfnr\np\n\n\n\n\n0\nWhite\n0.824527\n0.802766\n0.869107\n0.221774\n0.130893\n0.509466\n\n\n3\nBlack\n0.799747\n0.750630\n0.889552\n0.267568\n0.110448\n0.475177\n\n\n7\nN. American\n0.843750\n0.770492\n0.873311\n0.213001\n0.126689\n0.450190\n\n\n6\nSPAA\n1.000000\n0.739631\n0.877049\n0.267139\n0.122951\n0.463878\n\n\n4\nAsian\n0.820998\n0.806495\n0.901664\n0.285429\n0.098336\n0.568847\n\n\n5\nNPI\n0.500000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n1\nOther\n0.807092\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\nMulti\n0.825856\n0.666667\n1.000000\n0.227273\n0.000000\n0.312500\n\n\n\n\n\n\n\n\n\n\nApproximately calibrated:\n\ncal = pd.DataFrame(pred)\ncal[\"prediction\"] = pred\ncal.groupby(\"prediction\").count()\n\n\n\n\n\n\n\n\n0\n\n\nprediction\n\n\n\n\n\nFalse\n6514\n\n\nTrue\n8112\n\n\n\n\n\n\n\nThis table shows that values either True or False, therefore this model is calibrated.\nApproximate error balance rate: Looking at the table dataframe above we can see that the model does not meet approximate error rate balance for groups. The groups differ in true and false positive rates. The code below double checks.\n\nfor i, row1 in audit.iterrows():\n    equal = True;\n    race1 = row1['race']\n    tpr1 = row1['tpr']\n    fpr1 = row1['fpr']\n    for j, row2 in audit.iterrows():\n        race2 = row2['race']\n        tpr2 = row2['tpr']\n        fpr2 = row2['fpr']\n        if(tpr1 != tpr2 or fpr1 != fpr2):\n            equal = False\n            print(f\"{race1} did not have an equal TPR or FPR as {race2}\")\n            break\n    if(equal != True):\n        break\n        \n\nWhite did not have an equal TPR or FPR as Black\n\n\nStatistical parity:\n\nprobs = []\nfor group in groups:\n    prob = (pred == True)[group_test == group].mean()\n    probs.append(prob)\n\nparity= pd.DataFrame(groups, columns=[\"race\"])\nparity[\"prob\"] = probs\nparity\n\n\n\n\n\n\n\n\nrace\nprob\n\n\n\n\n0\n1\n0.551568\n\n\n1\n8\n0.563121\n\n\n2\n9\n0.510266\n\n\n3\n2\n0.550063\n\n\n4\n6\n0.635972\n\n\n5\n7\n0.000000\n\n\n6\n5\n0.000000\n\n\n7\n3\n0.468750\n\n\n\n\n\n\n\nThe model does not meet statistical parity which means not all groups have an equal change of achieving favorable odds. Therefore we can assume that the probability of predicting employment is not independent of race.\nAdd prevalance to data table\n\naudit[\"p\"] = (1 + (audit[\"tpr\"] / audit[\"fpr\"]) * ((1 - audit[\"ppv\"])/(audit[\"ppv\"]))) ** -1\naudit[\"p\"] = audit[\"p\"].fillna(0)\naudit\nprint(audit)\n\n          race  accuracy       ppv       tpr       fpr       fnr         p\n0        White  0.824527  0.802766  0.869107  0.221774  0.130893  0.509466\n3        Black  0.799747  0.750630  0.889552  0.267568  0.110448  0.475177\n7  N. American  0.843750  0.770492  0.873311  0.213001  0.126689  0.450190\n6         SPAA  1.000000  0.739631  0.877049  0.267139  0.122951  0.463878\n4        Asian  0.820998  0.806495  0.901664  0.285429  0.098336  0.568847\n5          NPI  0.500000  0.000000  0.000000  0.000000  1.000000  0.000000\n1        Other  0.807092  0.000000  0.000000  0.000000  0.000000  0.000000\n2        Multi  0.825856  0.666667  1.000000  0.227273  0.000000  0.312500\n\n\nHere I plot the feasibility of FPR and FNR for Black and White groups.\n\nimport seaborn as sns\n\n# Filter to only include Black and White groups\nfiltered = audit[audit[\"race\"].isin([\"Black\", \"White\"])]\n\norange_color = \"#E69F00\"\nblack_color = \"#000000\"\n\n# A cleaner table\nfeasible = filtered[[\"race\", \"fpr\", \"fnr\", \"p\"]].copy() \nlines = []\n\n# Make fixed ppv based on the white ppv\nfixed_ppv = filtered.loc[filtered[\"race\"] == \"White\", \"ppv\"].values[0]\nfnr_range = np.linspace(0, 1, 100)\n\n# Compute feasible FPR for different FNR values\nfor i, row in feasible.iterrows():\n    race = row[\"race\"]\n    p = row[\"p\"]  \n    \n    fprs = (p / (1 - p)) * ((1 - fixed_ppv) / fixed_ppv) * (1 - fnr_range)\n\n    for fnr, fpr in zip(fnr_range, fprs):\n        lines.append({\"race\": race, \"fnr\": fnr, \"fpr\": fpr})\n\nlines_df = pd.DataFrame(lines)\n\nplt.figure(figsize=(7, 5))\nsns.set_style(\"whitegrid\")\n\n# Plot observed (fnr, fpr)\nfor i, row in feasible.iterrows():\n    color = orange_color if row[\"race\"] == \"White\" else black_color\n    plt.scatter(row[\"fnr\"], row[\"fpr\"], color=color)\n\n# Plot feasible (fnr, fpr) line\nfor race, color in zip([\"White\", \"Black\"], [orange_color, black_color]):\n    line = lines_df[lines_df[\"race\"] == race]\n    plt.plot(line[\"fnr\"], line[\"fpr\"], color=color)\n\nplt.xlabel(\"False Negative Rate\")\nplt.ylabel(\"False Positive Rate\")\nplt.title(\"Feasible (FNR, FPR) combinations\")\nplt.show()\n\n\n\n\n\n\n\n\nTo get equal false positive rates, we would need to make a large increase to the FNR."
  },
  {
    "objectID": "posts/blog3/index.html#concluding-discussion",
    "href": "posts/blog3/index.html#concluding-discussion",
    "title": "Auditing Bias",
    "section": "",
    "text": "A bank may use employment status to determine to give a lone to someone.\nSince my model proved not to be fair, then using for commercial or government reasons would perpetuate inequalities.\nBased on the results for statistical parity and error rate balance the results my model does display problematic biases. Some groups are more likely to be misclassified than others.\nI think transparency might be an issue, if the algorithm is not shared to the public. I’m unsure if this bias but smaller populations have very weird data points that probably accurate reflect what’s going on."
  },
  {
    "objectID": "posts/blog2/index.html",
    "href": "posts/blog2/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "import pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train = df_train[df_train[\"person_income\"] &lt; 1e6] # Remove the really high outlier\ndf_train = df_train[df_train[\"person_age\"] &lt;= 100] # Remove the really high outlier\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\np1 = sns.scatterplot(df_train, x=\"person_age\", y=\"person_emp_length\", hue= \"loan_intent\")\n\n\n\n\n\n\n\n\n\n\np2 = sns.histplot(df_train, x=\"person_income\", y=\"loan_percent_income\", bins=50)\n\n\n\n\n\n\n\n\n\np4 = sns.barplot(df_train, x=\"loan_status\", y=\"person_income\")\n\n\n\n\n\n\n\n\n\np5 = sns.scatterplot(df_train, x=\"person_age\", y=\"loan_amnt\", hue=\"loan_intent\")\n\n\n\n\n\n\n\n\n\np6 = sns.boxplot(df_train, x=\"loan_intent\", y=\"loan_amnt\", hue=\"loan_intent\", legend=False)\n\n\n\n\n\n\n\n\n\np7 = sns.scatterplot(df_train, y=\"cb_person_default_on_file\", x=\"person_income\")\n\n\n\n\n\n\n\n\n\nsns.FacetGrid(df_train, col= \"loan_intent\")"
  },
  {
    "objectID": "posts/blog2/index.html#data-visualization",
    "href": "posts/blog2/index.html#data-visualization",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\np1 = sns.scatterplot(df_train, x=\"person_age\", y=\"person_emp_length\", hue= \"loan_intent\")\n\n\n\n\n\n\n\n\n\n\np2 = sns.histplot(df_train, x=\"person_income\", y=\"loan_percent_income\", bins=50)\n\n\n\n\n\n\n\n\n\np4 = sns.barplot(df_train, x=\"loan_status\", y=\"person_income\")\n\n\n\n\n\n\n\n\n\np5 = sns.scatterplot(df_train, x=\"person_age\", y=\"loan_amnt\", hue=\"loan_intent\")\n\n\n\n\n\n\n\n\n\np6 = sns.boxplot(df_train, x=\"loan_intent\", y=\"loan_amnt\", hue=\"loan_intent\", legend=False)\n\n\n\n\n\n\n\n\n\np7 = sns.scatterplot(df_train, y=\"cb_person_default_on_file\", x=\"person_income\")\n\n\n\n\n\n\n\n\n\nsns.FacetGrid(df_train, col= \"loan_intent\")"
  }
]