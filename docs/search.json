[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Emmanuel’s blog"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @gabednick\n\n\n\n\nIn this blog post, I take the Palmer Penguins dataset and try to determine the best features to be used to determine the species of a penguin based on its measurements. Firstly, I create two figures and a table to analysize the relationships between features. Then I use sci-kit learns feature selection with chi-squared tests to pick 2 numerical features and 1 categorical feature. Then using those features, I train and test a logistic regression model. The model was fairly accurate but to understand the results better, I plot the decision regions and use a confusion matrix.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nThis is code from Professor Phil’s website. It removes the columns that we don’t use and NA values, converts the categorical feature columns into “one-hot encoded” 0-1 columns and saves the dataframe X_train. Also, “Species” is coded with the LabelEncoder and is saved as y_train.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nWe can check what the columns look like now.\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\nI created two graphs each 2 quantative columns and 1 qualitative columns. Plot 1 shows the relationship with the body mass and flipper length between different penguin species. Plot 2 shows the difference in Culmen Length and depth across different penguin species.\n\n# Get the unencoded columns for easier graphing.\nqual = train[[\"Island\", \"Sex\", \"Species\"]].dropna()\n\n# Shorten species label for the legend\nqual[\"Species\"] = qual[\"Species\"].apply(lambda x: \"Chinstrap\" if x == \"Chinstrap penguin (Pygoscelis antarctica)\" \n                                         else (\"Gentoo\" if x == \"Gentoo penguin (Pygoscelis papua)\" \n                                               else \"Adelie\"))\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-v0_8-whitegrid')\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 4))\n   \np1 = sns.scatterplot(X_train, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", hue=qual[\"Species\"], ax = ax[0])\np2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=qual[\"Species\"], ax = ax[1])\n\n\n\n\n\n\n\n\nLeft side: Plot 1. The Gentoo penguins tend to be the biggest while Chinstrap and Adelie seem to have a great overlap in size. The Adelie seem to have slightly more variety in mass given a flipper length compared to Chinstrap. This is features seem decent for selection because there is some correlation between species and body size but they are not easily distinguishable by species.\nRight side: Plot 2. Based on the graph, the each specie of penguin seemed to have distinct beaks. The Adelie have greatest culmen depth but shortest length. The Gentoo have longer culmens but not as much depth and the Chinstrap lie in the middle with medium to large culmens. These are good features because there is a clear distinction in culmen sizes between species of penquins.\n\n\n\nNow I create a summary table of the penguins measurements based on clutch completetion.\n\ntable = X_train[[\"Clutch Completion_Yes\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\ntable.groupby(\"Clutch Completion_Yes\").aggregate(['min', 'median', 'max'])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\nClutch Completion_Yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse\n35.9\n43.35\n58.0\n13.7\n17.85\n19.9\n172.0\n195.0\n225.0\n2700.0\n3737.5\n5700.0\n\n\nTrue\n34.0\n45.10\n55.9\n13.1\n17.20\n21.5\n176.0\n198.0\n230.0\n2850.0\n4100.0\n6300.0\n\n\n\n\n\n\n\nTable 1: This table shows the biggest difference between penguins that had a full clutch and those that did not was the weight. Most of the penguins the produced two eggs weighed about 300 grams more. There is most likely not a causation but there a may be a correlation. Since clutch completion does not seem impact this data very much, it may not be a feature worth looking at.\n\n\n\n\nHere I use sci-kits SelectKBest to pick the 3 feautures I’m going to use for my model. I had to seperate feature selection because all 3 selected features would be numerical. SelectKBest selects the k best features based on user specified scoring function. I used chi-squared since my feautres were meant for classification and they were non-negative.\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Selecting 2 numerical feature\nquant = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nsel1 = SelectKBest(chi2, k=2)\nsel1.fit_transform(X_train[quant], y_train)\nf1 = sel1.get_feature_names_out()\n\n# Selecting 1 categorical feature\nqual = [\"Clutch Completion_Yes\", \"Clutch Completion_No\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Sex_FEMALE\", \"Sex_MALE\"]\nsel2 = SelectKBest(chi2, k=1)\nsel2.fit_transform(X_train[qual], y_train)\nf2 = sel2.get_feature_names_out()\n\nThis function is so that I can get all the variations of the categorical feature.\n\ndef get_feat(f1, cat):\n    cols = list(f1)\n    clutch = [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n    island = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n    sex = [\"Sex_FEMALE\", \"Sex_MALE\"]\n    \n    if cat in clutch: return cols + clutch\n    if cat in island: return cols + island\n    if cat in sex: return cols + sex\n\n\ncols = get_feat(f1, f2[0])\ncols\n\n['Flipper Length (mm)',\n 'Body Mass (g)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nSo despite the culmen sizes being appearing to be better features, based on the statistical tests the flipper length and body mass were better features.\n\n\n\nThe model is trained on the data with features determined from above. I had to use StandardScalar to avoid a convergence error. I used the Logistic Regression model as it is a good fit for classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\npipe = make_pipeline(StandardScaler(), LogisticRegression())\npipe.fit(X_train[cols], y_train)\npipe.score(X_train[cols], y_train)\n\n0.8984375\n\n\n\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\npipe.score(X_test[cols], y_test)\n\n0.8970588235294118\n\n\n\n\n\n\n\nMost of this code is adapted from Prof. Phil’s website.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nRegions for training set:\n\nplot_regions(pipe, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nRegions for testing set:\n\nplot_regions(pipe, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nLooking at these decision plots, we see that our model is pretty successful determing of the difference between Gentoo and Adelie on the Biscoe and Torgersen islands. However, on the Dream island, there is a mixture of Gentoo and Chinstrap where the model the struggles to distinguish between the two.\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = pipe.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[26,  5,  0],\n       [ 2,  9,  0],\n       [ 0,  0, 26]])\n\n\nOnce again, this shows that model struggled the most with Gentoo and Chinstrap.\n\n\n\n\nThe model had very similar training and testing accuracy of 0.89. This means the model is fairly solid at predicting penguins based on the flipper lenth, body mass and island. Based on the decision regions, the model struggled to correctly determine the difference between Chinstrap and Gentoo on the Island Dream. It seems that those 2 species are of similar sizes and therefore are hard to differentiate.\nDuring this blog post, the two biggest things were I learned about different tools for feature selection and the need to normalize data before putting it in a model. In terms of feature selection, sci-kit learn provides various methods whether variance, statistical tests or recursion. I went with statistical test as it seemed more thorough then determining a random variance yet not as complex as the recursion. For normalizing data, I learned how to deal with a convergence error when trainig a model. Through some investigation I came to understand I recieved that error because some of the features may not normally distributed. I used sci-kit recommended scaling function to standardize the data and everything was fine after that. Finally, I got more practice with modifing and selecting data from dataframes in general."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emmanuel’s CSCI 0451 Blog",
    "section": "",
    "text": "Overfitting and Double Descents\n\n\n\n\n\nBlog Post 6\n\n\n\n\n\nApr 23, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nBlog Post 5\n\n\n\n\n\nApr 9, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nBlog Post 4\n\n\n\n\n\nMar 31, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nBlog Post 3\n\n\n\n\n\nMar 12, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nBlog Post 2\n\n\n\n\n\nFeb 19, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\nFeb 12, 2025\n\n\nEmmanuel Towner\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog1/index.html#abstract",
    "href": "posts/blog1/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this blog post, I take the Palmer Penguins dataset and try to determine the best features to be used to determine the species of a penguin based on its measurements. Firstly, I create two figures and a table to analysize the relationships between features. Then I use sci-kit learns feature selection with chi-squared tests to pick 2 numerical features and 1 categorical feature. Then using those features, I train and test a logistic regression model. The model was fairly accurate but to understand the results better, I plot the decision regions and use a confusion matrix.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/blog1/index.html#data-preparation",
    "href": "posts/blog1/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This is code from Professor Phil’s website. It removes the columns that we don’t use and NA values, converts the categorical feature columns into “one-hot encoded” 0-1 columns and saves the dataframe X_train. Also, “Species” is coded with the LabelEncoder and is saved as y_train.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nWe can check what the columns look like now.\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/blog1/index.html#data-visualization",
    "href": "posts/blog1/index.html#data-visualization",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "I created two graphs each 2 quantative columns and 1 qualitative columns. Plot 1 shows the relationship with the body mass and flipper length between different penguin species. Plot 2 shows the difference in Culmen Length and depth across different penguin species.\n\n# Get the unencoded columns for easier graphing.\nqual = train[[\"Island\", \"Sex\", \"Species\"]].dropna()\n\n# Shorten species label for the legend\nqual[\"Species\"] = qual[\"Species\"].apply(lambda x: \"Chinstrap\" if x == \"Chinstrap penguin (Pygoscelis antarctica)\" \n                                         else (\"Gentoo\" if x == \"Gentoo penguin (Pygoscelis papua)\" \n                                               else \"Adelie\"))\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-v0_8-whitegrid')\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 4))\n   \np1 = sns.scatterplot(X_train, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", hue=qual[\"Species\"], ax = ax[0])\np2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=qual[\"Species\"], ax = ax[1])\n\n\n\n\n\n\n\n\nLeft side: Plot 1. The Gentoo penguins tend to be the biggest while Chinstrap and Adelie seem to have a great overlap in size. The Adelie seem to have slightly more variety in mass given a flipper length compared to Chinstrap. This is features seem decent for selection because there is some correlation between species and body size but they are not easily distinguishable by species.\nRight side: Plot 2. Based on the graph, the each specie of penguin seemed to have distinct beaks. The Adelie have greatest culmen depth but shortest length. The Gentoo have longer culmens but not as much depth and the Chinstrap lie in the middle with medium to large culmens. These are good features because there is a clear distinction in culmen sizes between species of penquins.\n\n\n\nNow I create a summary table of the penguins measurements based on clutch completetion.\n\ntable = X_train[[\"Clutch Completion_Yes\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\ntable.groupby(\"Clutch Completion_Yes\").aggregate(['min', 'median', 'max'])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\nClutch Completion_Yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse\n35.9\n43.35\n58.0\n13.7\n17.85\n19.9\n172.0\n195.0\n225.0\n2700.0\n3737.5\n5700.0\n\n\nTrue\n34.0\n45.10\n55.9\n13.1\n17.20\n21.5\n176.0\n198.0\n230.0\n2850.0\n4100.0\n6300.0\n\n\n\n\n\n\n\nTable 1: This table shows the biggest difference between penguins that had a full clutch and those that did not was the weight. Most of the penguins the produced two eggs weighed about 300 grams more. There is most likely not a causation but there a may be a correlation. Since clutch completion does not seem impact this data very much, it may not be a feature worth looking at."
  },
  {
    "objectID": "posts/blog1/index.html#feature-selection",
    "href": "posts/blog1/index.html#feature-selection",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Here I use sci-kits SelectKBest to pick the 3 feautures I’m going to use for my model. I had to seperate feature selection because all 3 selected features would be numerical. SelectKBest selects the k best features based on user specified scoring function. I used chi-squared since my feautres were meant for classification and they were non-negative.\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Selecting 2 numerical feature\nquant = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nsel1 = SelectKBest(chi2, k=2)\nsel1.fit_transform(X_train[quant], y_train)\nf1 = sel1.get_feature_names_out()\n\n# Selecting 1 categorical feature\nqual = [\"Clutch Completion_Yes\", \"Clutch Completion_No\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Sex_FEMALE\", \"Sex_MALE\"]\nsel2 = SelectKBest(chi2, k=1)\nsel2.fit_transform(X_train[qual], y_train)\nf2 = sel2.get_feature_names_out()\n\nThis function is so that I can get all the variations of the categorical feature.\n\ndef get_feat(f1, cat):\n    cols = list(f1)\n    clutch = [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n    island = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n    sex = [\"Sex_FEMALE\", \"Sex_MALE\"]\n    \n    if cat in clutch: return cols + clutch\n    if cat in island: return cols + island\n    if cat in sex: return cols + sex\n\n\ncols = get_feat(f1, f2[0])\ncols\n\n['Flipper Length (mm)',\n 'Body Mass (g)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nSo despite the culmen sizes being appearing to be better features, based on the statistical tests the flipper length and body mass were better features."
  },
  {
    "objectID": "posts/blog1/index.html#training",
    "href": "posts/blog1/index.html#training",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The model is trained on the data with features determined from above. I had to use StandardScalar to avoid a convergence error. I used the Logistic Regression model as it is a good fit for classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\npipe = make_pipeline(StandardScaler(), LogisticRegression())\npipe.fit(X_train[cols], y_train)\npipe.score(X_train[cols], y_train)\n\n0.8984375"
  },
  {
    "objectID": "posts/blog1/index.html#testing",
    "href": "posts/blog1/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "test_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\npipe.score(X_test[cols], y_test)\n\n0.8970588235294118"
  },
  {
    "objectID": "posts/blog1/index.html#results",
    "href": "posts/blog1/index.html#results",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Most of this code is adapted from Prof. Phil’s website.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nRegions for training set:\n\nplot_regions(pipe, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nRegions for testing set:\n\nplot_regions(pipe, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nLooking at these decision plots, we see that our model is pretty successful determing of the difference between Gentoo and Adelie on the Biscoe and Torgersen islands. However, on the Dream island, there is a mixture of Gentoo and Chinstrap where the model the struggles to distinguish between the two.\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = pipe.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[26,  5,  0],\n       [ 2,  9,  0],\n       [ 0,  0, 26]])\n\n\nOnce again, this shows that model struggled the most with Gentoo and Chinstrap."
  },
  {
    "objectID": "posts/blog1/index.html#discussion",
    "href": "posts/blog1/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The model had very similar training and testing accuracy of 0.89. This means the model is fairly solid at predicting penguins based on the flipper lenth, body mass and island. Based on the decision regions, the model struggled to correctly determine the difference between Chinstrap and Gentoo on the Island Dream. It seems that those 2 species are of similar sizes and therefore are hard to differentiate.\nDuring this blog post, the two biggest things were I learned about different tools for feature selection and the need to normalize data before putting it in a model. In terms of feature selection, sci-kit learn provides various methods whether variance, statistical tests or recursion. I went with statistical test as it seemed more thorough then determining a random variance yet not as complex as the recursion. For normalizing data, I learned how to deal with a convergence error when trainig a model. Through some investigation I came to understand I recieved that error because some of the features may not normally distributed. I used sci-kit recommended scaling function to standardize the data and everything was fine after that. Finally, I got more practice with modifing and selecting data from dataframes in general."
  },
  {
    "objectID": "posts/blog3/index.html",
    "href": "posts/blog3/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "My goal was to make a predicition of employment status based on various demographic excluding race using a subset of data from the American Community Service focused on Massachusetts residents in 2023. Based on the data of the 58,500 residents, only half were employed. Most of the time men, people without disabilities, and people who born abroad or with no citizensip had higher proportions of employment. The model I used was sklearns Decision Tree Classifier because results are easy to interpret. I tuned complexity by I using GridSearchCV which cross-validated that the best depth out of the numbers I provided was 10 which overall accuracy 0.82. The different group accuracies weren’t that much different. Auditing my model showed that white people lead in PPV and FNR while Asians lead in TPR and FPR rates. In these summary I left races 5 and 6 because their data often had many missing values. Based on those values, my model failed approximate error balance and statistical parity but satisfied calibration. The plot that used the fixed PPV values and p values to graph feasible FPR and FNR combinations between Black and White residents.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MA\" \n\ndata_source = ACSDataSource(survey_year='2023', # Get more recent data\n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nSTATE\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2023GQ0000077\n1\n1\n503\n1\n25\n1019518\n11\n89\n...\n12\n13\n13\n13\n13\n13\n13\n12\n13\n13\n\n\n1\nP\n2023GQ0000098\n1\n1\n613\n1\n25\n1019518\n11\n20\n...\n3\n4\n2\n20\n13\n9\n2\n20\n13\n2\n\n\n2\nP\n2023GQ0000109\n1\n1\n613\n1\n25\n1019518\n80\n68\n...\n28\n34\n78\n73\n34\n68\n82\n15\n17\n79\n\n\n3\nP\n2023GQ0000114\n1\n1\n801\n1\n25\n1019518\n69\n21\n...\n60\n74\n161\n11\n127\n57\n11\n12\n11\n12\n\n\n4\nP\n2023GQ0000135\n1\n1\n1201\n1\n25\n1019518\n27\n84\n...\n27\n28\n27\n29\n27\n29\n27\n27\n28\n27\n\n\n\n\n5 rows × 287 columns\n\n\n\n\n# No RELP avaiable \npossible_features=['AGEP', 'SCHL', 'MAR', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n89\n16.0\n2\n1\nNaN\n1\n1.0\n4.0\n3\n1\n1\n1\n1.0\n2\n1\n6.0\n\n\n1\n20\n16.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n2\n68\n18.0\n5\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n2.0\n1\n1\n6.0\n\n\n3\n21\n19.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n1.0\n\n\n4\n84\n16.0\n1\n1\nNaN\n1\n3.0\n4.0\n3\n1\n2\n1\n1.0\n2\n1\n6.0\n\n\n\n\n\n\n\n\n\n\n\n\nAdapted from Professor Phil’s code.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(73126, 14)\n(73126,)\n(73126,)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"race\"] = group_train\ndf[\"label\"] = y_train\n\nThis method allows the conversion of race numbers to more helpful categorical labels. Some of them have been shortened because they were too long. 1. SPAA - “American Indian and Alaska Native tribes specified, or American Indian or AlaskaNative, not specified and no other races”. 2. NPI - Native Hawaiian and Other Pacific Islander alone\n\ndef convert_race(df: pd.DataFrame):\n    df = df.sort_values(by='race')\n    df['race'] = df['race'].replace({1: \"White\", 2: \"Black\", 3: \"N. American\", 4:\"N. Alaskan\", \n                        5:\"SPAA\", \n                        6:'Asian', 7: 'NPI', 8:'Other', 9: 'Multi'})\n\n    df['race'] = pd.Categorical(df['race'])\n    return df\n    \n\n\n\n\n\n# Save to the original for calculations and copying\nrelabeled = df.copy() \nrelabeled = convert_race(relabeled)\nrelabeled\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nrace\nlabel\n\n\n\n\n0\n69.0\n19.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nFalse\n\n\n35997\n12.0\n9.0\n5.0\n2.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nFalse\n\n\n35999\n70.0\n24.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\nWhite\nTrue\n\n\n36000\n74.0\n22.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\nWhite\nFalse\n\n\n36001\n68.0\n23.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n51260\n6.0\n3.0\n5.0\n2.0\n2.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n51257\n7.0\n4.0\n5.0\n2.0\n7.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n5335\n14.0\n11.0\n5.0\n2.0\n3.0\n1.0\n1.0\n0.0\n3.0\n1.0\n2.0\n2.0\n2.0\n1.0\nMulti\nFalse\n\n\n34217\n18.0\n14.0\n5.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n40473\n18.0\n18.0\n5.0\n2.0\n0.0\n1.0\n3.0\n4.0\n4.0\n1.0\n2.0\n2.0\n2.0\n1.0\nMulti\nTrue\n\n\n\n\n58500 rows × 16 columns\n\n\n\n\nThe number of indiviuals in this df are\n\n\nprint(\"The number of indiviuals in this df are\", df.shape[0])\n\nThe number of indiviuals in this df are 58500\n\n\n\nThe proportion of employed individuals are\n\n\nemp = df[df['label'] == True][[\"label\"]].size # 29703\ntotal = df['label'].size # 58500\nemp_prop = emp / total\nprint(\"The proportion of employed indiviuals are\", emp_prop)\n\nThe proportion of employed indiviuals are 0.5077435897435898\n\n\n\nThe population of each group\n\n\nrelabeled.groupby('race')[['label']].aggregate('sum')\n\n/var/folders/t8/bssz0nhj0h94fjkhfd648d280000gn/T/ipykernel_33206/1987727674.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  relabeled.groupby('race')[['label']].aggregate('sum')\n\n\n\n\n\n\n\n\n\nlabel\n\n\nrace\n\n\n\n\n\nAsian\n2399\n\n\nBlack\n1529\n\n\nMulti\n2478\n\n\nN. American\n52\n\n\nNPI\n15\n\n\nOther\n1284\n\n\nSPAA\n26\n\n\nWhite\n21920\n\n\n\n\n\n\n\nBased on this, I might consider whether groups with less than 30 labels provide an adequate sample size.\n\nThe proportion of employed people in each group are:\n\n\nrelabeled.groupby('race')[['label']].aggregate('mean')\n\n/var/folders/t8/bssz0nhj0h94fjkhfd648d280000gn/T/ipykernel_33206/802482079.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  relabeled.groupby('race')[['label']].aggregate('mean')\n\n\n\n\n\n\n\n\n\nlabel\n\n\nrace\n\n\n\n\n\nAsian\n0.540315\n\n\nBlack\n0.473668\n\n\nMulti\n0.456270\n\n\nN. American\n0.530612\n\n\nNPI\n0.625000\n\n\nOther\n0.486364\n\n\nSPAA\n0.520000\n\n\nWhite\n0.514687\n\n\n\n\n\n\n\n\nIntersectional Trends\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize = (14, 4))\n\ncolors = [\"#1f77b4\", \"#ff7f0e\"]\nplt1 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"SEX\", palette=colors, ax=ax[0])\nplt1.set_xlabel(\"Race\")\nplt1.set_ylabel(\"Employment\")\nhandles, _ = plt1.get_legend_handles_labels()\nplt1.legend(handles=handles, title='Sex', labels=['Male', 'Female'])\n\nplt2 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"DIS\", palette=colors, ax=ax[1])\nplt2.set_xlabel(\"Race\")\nplt2.set_ylabel(\"Employment\")\nhandles, _ = plt2.get_legend_handles_labels()\nplt2.legend(handles=handles, title='Disability', labels=['Has disability', 'No disability'])\nplt.show()\n\n\n\n\n\n\n\n\nIn this we see intersectionality between employment based on the sex of the person or whether they have a disability. As we can see in both graphs generally male and able bodied people are more likely to be employed. Multiple factors can contribute to this desparity. One is that women and people with disabilities are discriminated against by employers. Another is also considering how many of them are applying for jobs. For women, even with more of them joining the workforce, they may be more likely to doing childcare at home. Because disabilites is such a broad it’s hard which may be capable of working and those that aren’t. This probably a big factor why relatively few are employed. Something I noticed is that for Black Americans, Native Americans and SPAA more females are employed than males.\n\nplt3 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"CIT\")\nplt3.set_xlabel(\"Race\")\nplt3.set_ylabel(\"Employment\")\nhandles, _ = plt3.get_legend_handles_labels()\nstatus = ['Born in US', 'Born in US Territories', 'Born Abroad', 'US by naturalization', 'Not a citizen']\nplt3.legend(handles=handles, title='Citizenship', labels=status, loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we see that for people born in the U.S or it’s territories have lower percentage employment than people born abroad or by naturalization. I suspect this is might be due to the smaller populations and people to who move here are more likely to have specific work purposes.\n\n\n\nThe decision tree classifier was one of the recommedations and scikit learn said it was easy to interpret. In order to find the best depth for I used GridSearchCV which cross validated the best paramaters.\n\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n\nparam_grid = {\n    'max_depth': [1, 3, 5, 10]\n}\n\nmodel = DecisionTreeClassifier() \ngrid = GridSearchCV(model, param_grid, scoring=\"accuracy\")\ngrid.fit(X_train, y_train)\n\nprint(\"Best params\", grid.best_params_)\n\nbest_dtree = grid.best_estimator_\n\nbest_dtree.predict(X_train)\nbest_dtree.score(X_train, y_train)\n\nBest params {'max_depth': 10}\n\n\n0.8346666666666667\n\n\nThe best depth was 10 and gave us a solid accuracy.\n\n\n\n\npred = best_dtree.predict(X_test)\nscore = best_dtree.score(X_test, y_test)\n\nprint(\"Test score\", score)\n\nTest score 0.821892520169561\n\n\n\n\n\n\n\n\nThe overall accuracy:\n\n(pred == y_test).mean()\n\nnp.float64(0.821892520169561)\n\n\nHere we calculate the PPV, the false negative and false positive rate\n\ntn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n\nppv = tp / (tp + fp)\nfnr = fn / (fn + tp)\nfpr = tp / (tp + tn)\n\nprint(\"Overall PPV:\", ppv)\nprint(\"Overall false negative rate:\", fnr)\nprint(\"Overall false positive rate:\", fpr)\n\nOverall PPV: 0.7938385705483673\nOverall false negative rate: 0.12639001898562516\nOverall false positive rate: 0.5358955161800183\n\n\nBased on this the model is pretty good at predicting who’s not employed but seems to overestimate the amount of employed.\n\n\n\nThis is the accuracy by group:\n\ngroups = df[\"race\"].unique() # Need race as numbers in order to compare with group_test\naudit = pd.DataFrame(groups, columns=[\"race\"])\n\naccuracies = []\nfor group in groups:\n   accuracy = (pred == y_test)[group_test == group].mean()\n   accuracies.append(accuracy)\n\naudit[\"accuracy\"] = accuracies\naudit = convert_race(audit)\naudit\n\n\n\n\n\n\n\n\nrace\naccuracy\n\n\n\n\n0\nWhite\n0.824150\n\n\n3\nBlack\n0.799747\n\n\n7\nN. American\n0.812500\n\n\n6\nSPAA\n1.000000\n\n\n4\nAsian\n0.821859\n\n\n5\nNPI\n0.500000\n\n\n1\nOther\n0.807092\n\n\n2\nMulti\n0.825095\n\n\n\n\n\n\n\nBasides NPI and SPAA, the accuracies are rather similar to each other.\nThis is the PPV, the false negative and false positive rate by group:\n\nppvs = []\ntprs = []\nfnrs = []\nfprs = []\n\nfor group in groups:\n    tp = int(0)\n    fp = int(0)\n    tn = int(0)\n    fn = int(0)\n    for n, m, grp in zip(y_test, pred, group_test):\n        if(grp == group):\n            if m == n:\n                if n == True:\n                    tp += 1\n                if n == False:\n                    tn += 1\n            if m != n:\n                if n == True:\n                    fn += 1\n                if n == False:\n                    fp += 1\n    ppv = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n    ppvs.append(ppv)\n    fnr = fn / (fn + tp)  if (fn + tp) &gt; 0 else 0\n    fnrs.append(fnr)\n    fpr = fp / (fp + tn) if (fp + tn) &gt; 0 else 0 \n    fprs.append(fpr)\n    tpr = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n    tprs.append(tpr)\n    \n\naudit[\"ppv\"] = ppvs\naudit[\"tpr\"] = tprs\naudit[\"fpr\"] = fprs\naudit[\"fnr\"] = fnrs\naudit\n\n\n\n\n\n\n\n\nrace\naccuracy\nppv\ntpr\nfpr\nfnr\n\n\n\n\n0\nWhite\n0.824150\n0.802322\n0.868922\n0.222350\n0.131078\n\n\n3\nBlack\n0.799747\n0.750630\n0.889552\n0.267568\n0.110448\n\n\n7\nN. American\n0.812500\n0.769345\n0.873311\n0.214385\n0.126689\n\n\n6\nSPAA\n1.000000\n0.739631\n0.877049\n0.267139\n0.122951\n\n\n4\nAsian\n0.821859\n0.807588\n0.901664\n0.283433\n0.098336\n\n\n5\nNPI\n0.500000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n1\nOther\n0.807092\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\nMulti\n0.825095\n0.625000\n1.000000\n0.272727\n0.000000\n\n\n\n\n\n\n\n\n\n\nUsing code adapted from from Machine Learning Master and sklearn documentation, I used a sci-kit learn’s calibration curve to diagnose the calibration:\n\nfrom sklearn.calibration import calibration_curve \n\ny_prob = best_dtree.predict_proba(X_test)[:, 1] # Gets the probability of positive values\nprob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n\navgs = []\nactuals = []\nfor group in groups:\n    # P(Y = 1 | S = {0,1}, R = r) \n    avg_pred_prob = y_prob[group_test == group].mean()\n    avgs.append(avg_pred_prob)\n    actual_employment_rate = (y_test == 1)[group_test == group].mean()\n    actuals.append(actual_employment_rate)\n\ncalibrate= pd.DataFrame(groups, columns=[\"race\"])\ncalibrate[\"employed\"] = actuals\ncalibrate[\"avg_predicted_probs\"] = avgs\nprint(calibrate)\n\ncal = plt\ncal.plot([0, 1], [0, 1], linestyle='--') # Actual values\ncal.plot(prob_pred, prob_true, marker='.') # Predicted values\ncal.title(\"Model Calibration\")\ncal.xlabel(\"Average Probability\")\ncal.ylabel(\"Fraction of Positive\")\ncal.show()\n\n   race  employed  avg_predicted_probs\n0     1  0.509466             0.510189\n1     8  0.475177             0.486277\n2     9  0.450190             0.449267\n3     2  0.463878             0.482898\n4     6  0.568847             0.571642\n5     7  0.500000             0.336508\n6     5  0.000000             0.094724\n7     3  0.312500             0.424606\n\n\n\n\n\n\n\n\n\nThe above graph and table shows that my model is nearly calibrated, meaning that the model usually predicts probabilites that are the same as the real probabilities.\nApproximate error balance rate: Looking at the table dataframe above we can see that the model does not meet approximate error rate balance for groups. The groups differ in true and false positive rates. The code below double checks.\n\nfor i, row1 in audit.iterrows():\n    equal = True;\n    race1 = row1['race']\n    tpr1 = row1['tpr']\n    fpr1 = row1['fpr']\n    for j, row2 in audit.iterrows():\n        race2 = row2['race']\n        tpr2 = row2['tpr']\n        fpr2 = row2['fpr']\n        if(tpr1 != tpr2 or fpr1 != fpr2):\n            equal = False\n            print(f\"{race1} did not have an equal TPR or FPR as {race2}\")\n            break\n    if(equal != True):\n        break\n\nWhite did not have an equal TPR or FPR as Black\n\n\nStatistical parity:\n\nprobs = []\nfor group in groups:\n    prob = (pred == True)[group_test == group].mean()\n    probs.append(prob)\n\nparity= pd.DataFrame(groups, columns=[\"race\"])\nparity[\"prob\"] = probs\nparity\n\n\n\n\n\n\n\n\nrace\nprob\n\n\n\n\n0\n1\n0.551757\n\n\n1\n8\n0.563121\n\n\n2\n9\n0.511027\n\n\n3\n2\n0.550063\n\n\n4\n6\n0.635112\n\n\n5\n7\n0.000000\n\n\n6\n5\n0.000000\n\n\n7\n3\n0.500000\n\n\n\n\n\n\n\nThe model does not meet statistical parity which means not all groups have an equal change of achieving favorable odds. Therefore we can assume that the probability of predicting employment is not independent of race.\n\n\n\nAdd prevalance to data table\n\naudit[\"p\"] = (1 + (audit[\"tpr\"] / audit[\"fpr\"]) * ((1 - audit[\"ppv\"])/(audit[\"ppv\"]))) ** -1\naudit[\"p\"] = audit[\"p\"].fillna(0)\naudit\nprint(audit)\n\n          race  accuracy       ppv       tpr       fpr       fnr         p\n0        White  0.824338  0.802492  0.869107  0.222158  0.130893  0.509466\n3        Black  0.798479  0.752525  0.889552  0.264865  0.110448  0.475177\n7  N. American  0.843750  0.769001  0.871622  0.214385  0.128378  0.450190\n6         SPAA  1.000000  0.739030  0.874317  0.267139  0.125683  0.463878\n4        Asian  0.821859  0.807588  0.901664  0.283433  0.098336  0.568847\n5          NPI  0.500000  0.000000  0.000000  0.000000  1.000000  0.000000\n1        Other  0.808511  0.000000  0.000000  0.000000  0.000000  0.000000\n2        Multi  0.824335  0.666667  1.000000  0.227273  0.000000  0.312500\n\n\nHere I plot the feasibility of FPR and FNR for Black and White groups.\n\nimport seaborn as sns\n\n# Filter to only include Black and White groups\nfiltered = audit[audit[\"race\"].isin([\"Black\", \"White\"])]\n\norange_color = \"#E69F00\"\nblack_color = \"#000000\"\n\n# A cleaner table\nfeasible = filtered[[\"race\", \"fpr\", \"fnr\", \"p\"]].copy() \nlines = []\n\n# Make fixed ppv based on the black ppv\nfixed_ppv = filtered.loc[filtered[\"race\"] == \"Black\", \"ppv\"].values[0]\nfnr_range = np.linspace(0, 1, 100)\n\n# Compute feasible FPR for different FNR values\nfor i, row in feasible.iterrows():\n    race = row[\"race\"]\n    p = row[\"p\"]  \n    \n    fprs = (p / (1 - p)) * ((1 - fixed_ppv) / fixed_ppv) * (1 - fnr_range)\n\n    for fnr, fpr in zip(fnr_range, fprs):\n        lines.append({\"race\": race, \"fnr\": fnr, \"fpr\": fpr})\n\nlines_df = pd.DataFrame(lines)\n\nplt.figure(figsize=(7, 5))\nsns.set_style(\"whitegrid\")\n\n# Plot observed (fnr, fpr)\nfor i, row in feasible.iterrows():\n    color = orange_color if row[\"race\"] == \"White\" else black_color\n    plt.scatter(row[\"fnr\"], row[\"fpr\"], color=color)\n\n# Plot feasible (fnr, fpr) line\nfor race, color in zip([\"White\", \"Black\"], [orange_color, black_color]):\n    line = lines_df[lines_df[\"race\"] == race]\n    plt.plot(line[\"fnr\"], line[\"fpr\"], color=color)\n\nplt.xlabel(\"False Negative Rate\")\nplt.ylabel(\"False Positive Rate\")\nplt.title(\"Feasible (FNR, FPR) combinations\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on this plot, to get equal false positive rates we would need to reduce \\(\\mathrm{FNR}_w\\) by about 0.04.\n\n\n\n\n\nIf we assume that these institutions don’t already have employment data, then it could be used by finacial and government institutions. For example, a bank may use employment status prediction to determine whether to give a loan to someone. The government could use its prediction to determine where they need to improve in providing employment opportunities. Landlords could use this information to decide if they want to accept a tenant or how much they would to charge them.\nSince my model proved to not be fair, then commercial or government industries would be misguided in their decisions. This algorithm may be used to unreasonably target certain populations with advertisements. Some people may be missclassified which would either disallow them from certian opportunities or stop them from getting the help they need.\nMy model failed tests of statistical parity and error rate balance which means that it does display problematic biases.\nI think transparency might be an issue, if the algorithm is not shared to the public as people would unknownigly be judged by a algorithm. I’m unsure if this bias but smaller populations have very weird data points that probably don’t accurate reflect what’s going on. I feel like there could be some social bias or judgement as result of this algorithm, especially for people who aren’t employed or predicted to not be employed."
  },
  {
    "objectID": "posts/blog3/index.html#abstract",
    "href": "posts/blog3/index.html#abstract",
    "title": "Auditing Bias",
    "section": "",
    "text": "My goal was to make a predicition of employment status based on various demographic excluding race using a subset of data from the American Community Service focused on Massachusetts residents in 2023. Based on the data of the 58,500 residents, only half were employed. Most of the time men, people without disabilities, and people who born abroad or with no citizensip had higher proportions of employment. The model I used was sklearns Decision Tree Classifier because results are easy to interpret. I tuned complexity by I using GridSearchCV which cross-validated that the best depth out of the numbers I provided was 10 which overall accuracy 0.82. The different group accuracies weren’t that much different. Auditing my model showed that white people lead in PPV and FNR while Asians lead in TPR and FPR rates. In these summary I left races 5 and 6 because their data often had many missing values. Based on those values, my model failed approximate error balance and statistical parity but satisfied calibration. The plot that used the fixed PPV values and p values to graph feasible FPR and FNR combinations between Black and White residents.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MA\" \n\ndata_source = ACSDataSource(survey_year='2023', # Get more recent data\n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nSTATE\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2023GQ0000077\n1\n1\n503\n1\n25\n1019518\n11\n89\n...\n12\n13\n13\n13\n13\n13\n13\n12\n13\n13\n\n\n1\nP\n2023GQ0000098\n1\n1\n613\n1\n25\n1019518\n11\n20\n...\n3\n4\n2\n20\n13\n9\n2\n20\n13\n2\n\n\n2\nP\n2023GQ0000109\n1\n1\n613\n1\n25\n1019518\n80\n68\n...\n28\n34\n78\n73\n34\n68\n82\n15\n17\n79\n\n\n3\nP\n2023GQ0000114\n1\n1\n801\n1\n25\n1019518\n69\n21\n...\n60\n74\n161\n11\n127\n57\n11\n12\n11\n12\n\n\n4\nP\n2023GQ0000135\n1\n1\n1201\n1\n25\n1019518\n27\n84\n...\n27\n28\n27\n29\n27\n29\n27\n27\n28\n27\n\n\n\n\n5 rows × 287 columns\n\n\n\n\n# No RELP avaiable \npossible_features=['AGEP', 'SCHL', 'MAR', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n89\n16.0\n2\n1\nNaN\n1\n1.0\n4.0\n3\n1\n1\n1\n1.0\n2\n1\n6.0\n\n\n1\n20\n16.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n2\n68\n18.0\n5\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n2.0\n1\n1\n6.0\n\n\n3\n21\n19.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n1.0\n\n\n4\n84\n16.0\n1\n1\nNaN\n1\n3.0\n4.0\n3\n1\n2\n1\n1.0\n2\n1\n6.0"
  },
  {
    "objectID": "posts/blog3/index.html#model",
    "href": "posts/blog3/index.html#model",
    "title": "Auditing Bias",
    "section": "",
    "text": "Adapted from Professor Phil’s code.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(73126, 14)\n(73126,)\n(73126,)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"race\"] = group_train\ndf[\"label\"] = y_train\n\nThis method allows the conversion of race numbers to more helpful categorical labels. Some of them have been shortened because they were too long. 1. SPAA - “American Indian and Alaska Native tribes specified, or American Indian or AlaskaNative, not specified and no other races”. 2. NPI - Native Hawaiian and Other Pacific Islander alone\n\ndef convert_race(df: pd.DataFrame):\n    df = df.sort_values(by='race')\n    df['race'] = df['race'].replace({1: \"White\", 2: \"Black\", 3: \"N. American\", 4:\"N. Alaskan\", \n                        5:\"SPAA\", \n                        6:'Asian', 7: 'NPI', 8:'Other', 9: 'Multi'})\n\n    df['race'] = pd.Categorical(df['race'])\n    return df\n    \n\n\n\n\n\n# Save to the original for calculations and copying\nrelabeled = df.copy() \nrelabeled = convert_race(relabeled)\nrelabeled\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nrace\nlabel\n\n\n\n\n0\n69.0\n19.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nFalse\n\n\n35997\n12.0\n9.0\n5.0\n2.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nFalse\n\n\n35999\n70.0\n24.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\nWhite\nTrue\n\n\n36000\n74.0\n22.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\nWhite\nFalse\n\n\n36001\n68.0\n23.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nWhite\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n51260\n6.0\n3.0\n5.0\n2.0\n2.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n51257\n7.0\n4.0\n5.0\n2.0\n7.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n5335\n14.0\n11.0\n5.0\n2.0\n3.0\n1.0\n1.0\n0.0\n3.0\n1.0\n2.0\n2.0\n2.0\n1.0\nMulti\nFalse\n\n\n34217\n18.0\n14.0\n5.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\nMulti\nFalse\n\n\n40473\n18.0\n18.0\n5.0\n2.0\n0.0\n1.0\n3.0\n4.0\n4.0\n1.0\n2.0\n2.0\n2.0\n1.0\nMulti\nTrue\n\n\n\n\n58500 rows × 16 columns\n\n\n\n\nThe number of indiviuals in this df are\n\n\nprint(\"The number of indiviuals in this df are\", df.shape[0])\n\nThe number of indiviuals in this df are 58500\n\n\n\nThe proportion of employed individuals are\n\n\nemp = df[df['label'] == True][[\"label\"]].size # 29703\ntotal = df['label'].size # 58500\nemp_prop = emp / total\nprint(\"The proportion of employed indiviuals are\", emp_prop)\n\nThe proportion of employed indiviuals are 0.5077435897435898\n\n\n\nThe population of each group\n\n\nrelabeled.groupby('race')[['label']].aggregate('sum')\n\n/var/folders/t8/bssz0nhj0h94fjkhfd648d280000gn/T/ipykernel_33206/1987727674.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  relabeled.groupby('race')[['label']].aggregate('sum')\n\n\n\n\n\n\n\n\n\nlabel\n\n\nrace\n\n\n\n\n\nAsian\n2399\n\n\nBlack\n1529\n\n\nMulti\n2478\n\n\nN. American\n52\n\n\nNPI\n15\n\n\nOther\n1284\n\n\nSPAA\n26\n\n\nWhite\n21920\n\n\n\n\n\n\n\nBased on this, I might consider whether groups with less than 30 labels provide an adequate sample size.\n\nThe proportion of employed people in each group are:\n\n\nrelabeled.groupby('race')[['label']].aggregate('mean')\n\n/var/folders/t8/bssz0nhj0h94fjkhfd648d280000gn/T/ipykernel_33206/802482079.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  relabeled.groupby('race')[['label']].aggregate('mean')\n\n\n\n\n\n\n\n\n\nlabel\n\n\nrace\n\n\n\n\n\nAsian\n0.540315\n\n\nBlack\n0.473668\n\n\nMulti\n0.456270\n\n\nN. American\n0.530612\n\n\nNPI\n0.625000\n\n\nOther\n0.486364\n\n\nSPAA\n0.520000\n\n\nWhite\n0.514687\n\n\n\n\n\n\n\n\nIntersectional Trends\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize = (14, 4))\n\ncolors = [\"#1f77b4\", \"#ff7f0e\"]\nplt1 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"SEX\", palette=colors, ax=ax[0])\nplt1.set_xlabel(\"Race\")\nplt1.set_ylabel(\"Employment\")\nhandles, _ = plt1.get_legend_handles_labels()\nplt1.legend(handles=handles, title='Sex', labels=['Male', 'Female'])\n\nplt2 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"DIS\", palette=colors, ax=ax[1])\nplt2.set_xlabel(\"Race\")\nplt2.set_ylabel(\"Employment\")\nhandles, _ = plt2.get_legend_handles_labels()\nplt2.legend(handles=handles, title='Disability', labels=['Has disability', 'No disability'])\nplt.show()\n\n\n\n\n\n\n\n\nIn this we see intersectionality between employment based on the sex of the person or whether they have a disability. As we can see in both graphs generally male and able bodied people are more likely to be employed. Multiple factors can contribute to this desparity. One is that women and people with disabilities are discriminated against by employers. Another is also considering how many of them are applying for jobs. For women, even with more of them joining the workforce, they may be more likely to doing childcare at home. Because disabilites is such a broad it’s hard which may be capable of working and those that aren’t. This probably a big factor why relatively few are employed. Something I noticed is that for Black Americans, Native Americans and SPAA more females are employed than males.\n\nplt3 = sns.barplot(relabeled, x = \"race\", y = \"label\", hue=\"CIT\")\nplt3.set_xlabel(\"Race\")\nplt3.set_ylabel(\"Employment\")\nhandles, _ = plt3.get_legend_handles_labels()\nstatus = ['Born in US', 'Born in US Territories', 'Born Abroad', 'US by naturalization', 'Not a citizen']\nplt3.legend(handles=handles, title='Citizenship', labels=status, loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we see that for people born in the U.S or it’s territories have lower percentage employment than people born abroad or by naturalization. I suspect this is might be due to the smaller populations and people to who move here are more likely to have specific work purposes.\n\n\n\nThe decision tree classifier was one of the recommedations and scikit learn said it was easy to interpret. In order to find the best depth for I used GridSearchCV which cross validated the best paramaters.\n\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n\nparam_grid = {\n    'max_depth': [1, 3, 5, 10]\n}\n\nmodel = DecisionTreeClassifier() \ngrid = GridSearchCV(model, param_grid, scoring=\"accuracy\")\ngrid.fit(X_train, y_train)\n\nprint(\"Best params\", grid.best_params_)\n\nbest_dtree = grid.best_estimator_\n\nbest_dtree.predict(X_train)\nbest_dtree.score(X_train, y_train)\n\nBest params {'max_depth': 10}\n\n\n0.8346666666666667\n\n\nThe best depth was 10 and gave us a solid accuracy.\n\n\n\n\npred = best_dtree.predict(X_test)\nscore = best_dtree.score(X_test, y_test)\n\nprint(\"Test score\", score)\n\nTest score 0.821892520169561"
  },
  {
    "objectID": "posts/blog3/index.html#bias-audit",
    "href": "posts/blog3/index.html#bias-audit",
    "title": "Auditing Bias",
    "section": "",
    "text": "The overall accuracy:\n\n(pred == y_test).mean()\n\nnp.float64(0.821892520169561)\n\n\nHere we calculate the PPV, the false negative and false positive rate\n\ntn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n\nppv = tp / (tp + fp)\nfnr = fn / (fn + tp)\nfpr = tp / (tp + tn)\n\nprint(\"Overall PPV:\", ppv)\nprint(\"Overall false negative rate:\", fnr)\nprint(\"Overall false positive rate:\", fpr)\n\nOverall PPV: 0.7938385705483673\nOverall false negative rate: 0.12639001898562516\nOverall false positive rate: 0.5358955161800183\n\n\nBased on this the model is pretty good at predicting who’s not employed but seems to overestimate the amount of employed.\n\n\n\nThis is the accuracy by group:\n\ngroups = df[\"race\"].unique() # Need race as numbers in order to compare with group_test\naudit = pd.DataFrame(groups, columns=[\"race\"])\n\naccuracies = []\nfor group in groups:\n   accuracy = (pred == y_test)[group_test == group].mean()\n   accuracies.append(accuracy)\n\naudit[\"accuracy\"] = accuracies\naudit = convert_race(audit)\naudit\n\n\n\n\n\n\n\n\nrace\naccuracy\n\n\n\n\n0\nWhite\n0.824150\n\n\n3\nBlack\n0.799747\n\n\n7\nN. American\n0.812500\n\n\n6\nSPAA\n1.000000\n\n\n4\nAsian\n0.821859\n\n\n5\nNPI\n0.500000\n\n\n1\nOther\n0.807092\n\n\n2\nMulti\n0.825095\n\n\n\n\n\n\n\nBasides NPI and SPAA, the accuracies are rather similar to each other.\nThis is the PPV, the false negative and false positive rate by group:\n\nppvs = []\ntprs = []\nfnrs = []\nfprs = []\n\nfor group in groups:\n    tp = int(0)\n    fp = int(0)\n    tn = int(0)\n    fn = int(0)\n    for n, m, grp in zip(y_test, pred, group_test):\n        if(grp == group):\n            if m == n:\n                if n == True:\n                    tp += 1\n                if n == False:\n                    tn += 1\n            if m != n:\n                if n == True:\n                    fn += 1\n                if n == False:\n                    fp += 1\n    ppv = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n    ppvs.append(ppv)\n    fnr = fn / (fn + tp)  if (fn + tp) &gt; 0 else 0\n    fnrs.append(fnr)\n    fpr = fp / (fp + tn) if (fp + tn) &gt; 0 else 0 \n    fprs.append(fpr)\n    tpr = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n    tprs.append(tpr)\n    \n\naudit[\"ppv\"] = ppvs\naudit[\"tpr\"] = tprs\naudit[\"fpr\"] = fprs\naudit[\"fnr\"] = fnrs\naudit\n\n\n\n\n\n\n\n\nrace\naccuracy\nppv\ntpr\nfpr\nfnr\n\n\n\n\n0\nWhite\n0.824150\n0.802322\n0.868922\n0.222350\n0.131078\n\n\n3\nBlack\n0.799747\n0.750630\n0.889552\n0.267568\n0.110448\n\n\n7\nN. American\n0.812500\n0.769345\n0.873311\n0.214385\n0.126689\n\n\n6\nSPAA\n1.000000\n0.739631\n0.877049\n0.267139\n0.122951\n\n\n4\nAsian\n0.821859\n0.807588\n0.901664\n0.283433\n0.098336\n\n\n5\nNPI\n0.500000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n1\nOther\n0.807092\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\nMulti\n0.825095\n0.625000\n1.000000\n0.272727\n0.000000\n\n\n\n\n\n\n\n\n\n\nUsing code adapted from from Machine Learning Master and sklearn documentation, I used a sci-kit learn’s calibration curve to diagnose the calibration:\n\nfrom sklearn.calibration import calibration_curve \n\ny_prob = best_dtree.predict_proba(X_test)[:, 1] # Gets the probability of positive values\nprob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n\navgs = []\nactuals = []\nfor group in groups:\n    # P(Y = 1 | S = {0,1}, R = r) \n    avg_pred_prob = y_prob[group_test == group].mean()\n    avgs.append(avg_pred_prob)\n    actual_employment_rate = (y_test == 1)[group_test == group].mean()\n    actuals.append(actual_employment_rate)\n\ncalibrate= pd.DataFrame(groups, columns=[\"race\"])\ncalibrate[\"employed\"] = actuals\ncalibrate[\"avg_predicted_probs\"] = avgs\nprint(calibrate)\n\ncal = plt\ncal.plot([0, 1], [0, 1], linestyle='--') # Actual values\ncal.plot(prob_pred, prob_true, marker='.') # Predicted values\ncal.title(\"Model Calibration\")\ncal.xlabel(\"Average Probability\")\ncal.ylabel(\"Fraction of Positive\")\ncal.show()\n\n   race  employed  avg_predicted_probs\n0     1  0.509466             0.510189\n1     8  0.475177             0.486277\n2     9  0.450190             0.449267\n3     2  0.463878             0.482898\n4     6  0.568847             0.571642\n5     7  0.500000             0.336508\n6     5  0.000000             0.094724\n7     3  0.312500             0.424606\n\n\n\n\n\n\n\n\n\nThe above graph and table shows that my model is nearly calibrated, meaning that the model usually predicts probabilites that are the same as the real probabilities.\nApproximate error balance rate: Looking at the table dataframe above we can see that the model does not meet approximate error rate balance for groups. The groups differ in true and false positive rates. The code below double checks.\n\nfor i, row1 in audit.iterrows():\n    equal = True;\n    race1 = row1['race']\n    tpr1 = row1['tpr']\n    fpr1 = row1['fpr']\n    for j, row2 in audit.iterrows():\n        race2 = row2['race']\n        tpr2 = row2['tpr']\n        fpr2 = row2['fpr']\n        if(tpr1 != tpr2 or fpr1 != fpr2):\n            equal = False\n            print(f\"{race1} did not have an equal TPR or FPR as {race2}\")\n            break\n    if(equal != True):\n        break\n\nWhite did not have an equal TPR or FPR as Black\n\n\nStatistical parity:\n\nprobs = []\nfor group in groups:\n    prob = (pred == True)[group_test == group].mean()\n    probs.append(prob)\n\nparity= pd.DataFrame(groups, columns=[\"race\"])\nparity[\"prob\"] = probs\nparity\n\n\n\n\n\n\n\n\nrace\nprob\n\n\n\n\n0\n1\n0.551757\n\n\n1\n8\n0.563121\n\n\n2\n9\n0.511027\n\n\n3\n2\n0.550063\n\n\n4\n6\n0.635112\n\n\n5\n7\n0.000000\n\n\n6\n5\n0.000000\n\n\n7\n3\n0.500000\n\n\n\n\n\n\n\nThe model does not meet statistical parity which means not all groups have an equal change of achieving favorable odds. Therefore we can assume that the probability of predicting employment is not independent of race.\n\n\n\nAdd prevalance to data table\n\naudit[\"p\"] = (1 + (audit[\"tpr\"] / audit[\"fpr\"]) * ((1 - audit[\"ppv\"])/(audit[\"ppv\"]))) ** -1\naudit[\"p\"] = audit[\"p\"].fillna(0)\naudit\nprint(audit)\n\n          race  accuracy       ppv       tpr       fpr       fnr         p\n0        White  0.824338  0.802492  0.869107  0.222158  0.130893  0.509466\n3        Black  0.798479  0.752525  0.889552  0.264865  0.110448  0.475177\n7  N. American  0.843750  0.769001  0.871622  0.214385  0.128378  0.450190\n6         SPAA  1.000000  0.739030  0.874317  0.267139  0.125683  0.463878\n4        Asian  0.821859  0.807588  0.901664  0.283433  0.098336  0.568847\n5          NPI  0.500000  0.000000  0.000000  0.000000  1.000000  0.000000\n1        Other  0.808511  0.000000  0.000000  0.000000  0.000000  0.000000\n2        Multi  0.824335  0.666667  1.000000  0.227273  0.000000  0.312500\n\n\nHere I plot the feasibility of FPR and FNR for Black and White groups.\n\nimport seaborn as sns\n\n# Filter to only include Black and White groups\nfiltered = audit[audit[\"race\"].isin([\"Black\", \"White\"])]\n\norange_color = \"#E69F00\"\nblack_color = \"#000000\"\n\n# A cleaner table\nfeasible = filtered[[\"race\", \"fpr\", \"fnr\", \"p\"]].copy() \nlines = []\n\n# Make fixed ppv based on the black ppv\nfixed_ppv = filtered.loc[filtered[\"race\"] == \"Black\", \"ppv\"].values[0]\nfnr_range = np.linspace(0, 1, 100)\n\n# Compute feasible FPR for different FNR values\nfor i, row in feasible.iterrows():\n    race = row[\"race\"]\n    p = row[\"p\"]  \n    \n    fprs = (p / (1 - p)) * ((1 - fixed_ppv) / fixed_ppv) * (1 - fnr_range)\n\n    for fnr, fpr in zip(fnr_range, fprs):\n        lines.append({\"race\": race, \"fnr\": fnr, \"fpr\": fpr})\n\nlines_df = pd.DataFrame(lines)\n\nplt.figure(figsize=(7, 5))\nsns.set_style(\"whitegrid\")\n\n# Plot observed (fnr, fpr)\nfor i, row in feasible.iterrows():\n    color = orange_color if row[\"race\"] == \"White\" else black_color\n    plt.scatter(row[\"fnr\"], row[\"fpr\"], color=color)\n\n# Plot feasible (fnr, fpr) line\nfor race, color in zip([\"White\", \"Black\"], [orange_color, black_color]):\n    line = lines_df[lines_df[\"race\"] == race]\n    plt.plot(line[\"fnr\"], line[\"fpr\"], color=color)\n\nplt.xlabel(\"False Negative Rate\")\nplt.ylabel(\"False Positive Rate\")\nplt.title(\"Feasible (FNR, FPR) combinations\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on this plot, to get equal false positive rates we would need to reduce \\(\\mathrm{FNR}_w\\) by about 0.04."
  },
  {
    "objectID": "posts/blog3/index.html#concluding-discussion",
    "href": "posts/blog3/index.html#concluding-discussion",
    "title": "Auditing Bias",
    "section": "",
    "text": "If we assume that these institutions don’t already have employment data, then it could be used by finacial and government institutions. For example, a bank may use employment status prediction to determine whether to give a loan to someone. The government could use its prediction to determine where they need to improve in providing employment opportunities. Landlords could use this information to decide if they want to accept a tenant or how much they would to charge them.\nSince my model proved to not be fair, then commercial or government industries would be misguided in their decisions. This algorithm may be used to unreasonably target certain populations with advertisements. Some people may be missclassified which would either disallow them from certian opportunities or stop them from getting the help they need.\nMy model failed tests of statistical parity and error rate balance which means that it does display problematic biases.\nI think transparency might be an issue, if the algorithm is not shared to the public as people would unknownigly be judged by a algorithm. I’m unsure if this bias but smaller populations have very weird data points that probably don’t accurate reflect what’s going on. I feel like there could be some social bias or judgement as result of this algorithm, especially for people who aren’t employed or predicted to not be employed."
  },
  {
    "objectID": "posts/blog2/index.html",
    "href": "posts/blog2/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "import pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train = df_train[df_train[\"person_income\"] &lt; 1e6] # Remove the really high outlier\ndf_train = df_train[df_train[\"person_age\"] &lt;= 100] # Remove the really high outlier\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\np1 = sns.scatterplot(df_train, x=\"person_age\", y=\"person_emp_length\", hue= \"loan_intent\")\n\n\n\n\n\n\n\n\n\n\np2 = sns.histplot(df_train, x=\"person_income\", y=\"loan_percent_income\", bins=50)\n\n\n\n\n\n\n\n\n\np4 = sns.barplot(df_train, x=\"loan_status\", y=\"person_income\")\n\n\n\n\n\n\n\n\n\np5 = sns.scatterplot(df_train, x=\"person_age\", y=\"loan_amnt\", hue=\"loan_intent\")\n\n\n\n\n\n\n\n\n\np6 = sns.boxplot(df_train, x=\"loan_intent\", y=\"loan_amnt\", hue=\"loan_intent\", legend=False)\n\n\n\n\n\n\n\n\n\np7 = sns.scatterplot(df_train, y=\"cb_person_default_on_file\", x=\"person_income\")\n\n\n\n\n\n\n\n\n\nsns.FacetGrid(df_train, col= \"loan_intent\")"
  },
  {
    "objectID": "posts/blog2/index.html#data-visualization",
    "href": "posts/blog2/index.html#data-visualization",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\np1 = sns.scatterplot(df_train, x=\"person_age\", y=\"person_emp_length\", hue= \"loan_intent\")\n\n\n\n\n\n\n\n\n\n\np2 = sns.histplot(df_train, x=\"person_income\", y=\"loan_percent_income\", bins=50)\n\n\n\n\n\n\n\n\n\np4 = sns.barplot(df_train, x=\"loan_status\", y=\"person_income\")\n\n\n\n\n\n\n\n\n\np5 = sns.scatterplot(df_train, x=\"person_age\", y=\"loan_amnt\", hue=\"loan_intent\")\n\n\n\n\n\n\n\n\n\np6 = sns.boxplot(df_train, x=\"loan_intent\", y=\"loan_amnt\", hue=\"loan_intent\", legend=False)\n\n\n\n\n\n\n\n\n\np7 = sns.scatterplot(df_train, y=\"cb_person_default_on_file\", x=\"person_income\")\n\n\n\n\n\n\n\n\n\nsns.FacetGrid(df_train, col= \"loan_intent\")"
  },
  {
    "objectID": "posts/blog4/index.html",
    "href": "posts/blog4/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Link to perceptron source code.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nCells 1 to 7 has been directly adapted from Prof. Phil’s code. Cell 8 is slightly modified from the original code. The code below in the sets up the necessary functions to implement a perceptron algorithm.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X: torch.Tensor, y: torch.Tensor, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w: torch.Tensor, x_min: int, x_max: int, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\nThis tests the Perceptron to see if the loss does reach 0.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\n\n\n\n\nThis plots the decision boundary and the data points over time.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    \n    # Making sure we don't run out of axes\n    if current_ax &gt;= len(axarr.ravel()):\n        print(\"Ran out of axes to plot. Stopping early.\")\n        break\n    \n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    \n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\nRan out of axes to plot. Stopping early.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nfig, axarr = plt.subplots(1, 2, figsize=(12, 6))\ncurrent_ax = 0\n\nloss = 1\nscore_vec = [] \n\nwhile loss &gt; 0 and len(score_vec) &lt;= 1000:\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    score = p.score(X).mean()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    \n    if local_loss &gt; 0:\n        loss = p.loss(X, y).item()\n        score = p.score(X).mean()\n        score_vec.append(score)\n\n# plot the score over iterations\naxarr[0].plot(range(len(score_vec)), score_vec, color=\"steelblue\", label=\"Score\")\naxarr[0].set_title(\"Score vs. Iterations\")\naxarr[0].set_xlabel(\"Iteration\")\naxarr[0].set_ylabel(\"Score\")\n\n# Plot the final decision boundary\nplot_perceptron_data(X, y, axarr[1])\ndraw_line(p.w, x_min=-1, x_max=2, ax=axarr[1], color=\"black\")\naxarr[1].scatter(X[i, 0], X[i, 1], color=\"black\", facecolors=\"none\", edgecolors=\"black\")\naxarr[1].set_title(f\"Final Decision Boundary (Loss = {loss:.3f})\")\naxarr[1].set(xlim=(-1, 2), ylim=(-1, 2))\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims=5)\n\ntorch.Size([300, 6])\n\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss = 1\nscore_vec = [] \n\nwhile loss &gt; 0 and len(score_vec) &lt;= 1000:\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    prev_length = len(score_vec)\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    score = p.score(X).mean()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    \n    if local_loss &gt; 0:\n        loss = p.loss(X, y).item()\n        score = p.score(X).mean()\n        score_vec.append(score)\n    \n    if(len(score_vec) != prev_length):\n        print(f\"Iteration {len(score_vec)}: Loss = {loss:.3f}, Score = {score:.3f}\")\n\nIteration 1: Loss = 0.027, Score = 0.699\nIteration 2: Loss = 0.247, Score = -0.777\nIteration 3: Loss = 0.103, Score = 2.712\nIteration 4: Loss = 0.003, Score = 1.483\nIteration 5: Loss = 0.000, Score = -0.228"
  },
  {
    "objectID": "posts/blog5/index.html",
    "href": "posts/blog5/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Link to logistic source code.\nThis notebook implements a logistic regression model with a gradient descent optimizer. The model is trained on a synthetic dataset, and in where it is optimized at each iteration and also updates loss. There are four experiments conducted. The first experiment tested the model on with vanilla gradient descent plotting the loss per iteration and a decision boundary. The second experiment compared the loss per iterations between the model when using the vanilla descent and when using momentum descent. The third experiment was to overfit the model to the training data and compare it to the accuracy of the model on the test data. The fourth experiment was to test the model on a heart disease prediction dateset. The dataset was split into training, validation, and test data. The model was trained on the training data and the loss computed for both training and validation. The model was then evaluated on the test data, and the accuracy was reported.\n\n%load_ext autoreload\n%autoreload 2\n\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\n\n\nimport numpy as np\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nThis code was adapted from previous notes.\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, X, y, x_min, x_max, **kwargs):\n    fig, ax = plt.subplots(1, 1)\n    plot_classification_data(X, y, ax)\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\n\n\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\nfor _ in range(100):\n    \n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n    \n    # add other stuff to e.g. keep track of the loss over time. \n    opt.step(X, y, alpha = 0.1, beta = 0)\n\n  \nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\ndraw_line(opt.model.w, X, y, x_min = -0.5, x_max = 1.5, color = \"black\", linestyle = \"dashed\")\n\n\n\n\n\n\n\n\nThe code above implements a vanilla gradient descent with logistic regression. It is run through a training loop, while keeping track of the loss and storing it in an array called loss_vec. Using loss_vec, I plot a graph showing the loss over gradient iterations. The second graph shows the decision boundary of the data. - Loss: This appears to start at ~0.67 and decreases to ~0.33 over the course of 100 iterations. - Decision Boundary: The decision boundary appears to correctly classify the data.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport torch\n\n# Vanilla gradient descent\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec1 = []\niterations = 3000\n\nfor _ in range(iterations):\n    loss = LR.loss(X, y)\n    loss_vec1.append(loss)\n    opt.step(X, y, alpha=0.1, beta=0.0)  # baseline\n\n# Momentum version\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec2 = []\nfor _ in range(iterations):\n    loss = LR.loss(X, y)\n    loss_vec2.append(loss)\n    opt.step(X, y, alpha=0.01, beta=0.5)  # momentum version\n\n\nplt.plot(torch.arange(1, len(loss_vec1) + 1), loss_vec1, color=\"black\", label=\"alpha=0.1, beta=0.0\")\nplt.plot(torch.arange(1, len(loss_vec2) + 1), loss_vec2, color=\"orange\", label=\"alpha=0.01, beta=0.5\")\n\nplt.semilogx()\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Loss (binary cross entropy)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe code above plots the loss over iterations for both vanilla gradient descent and momentum gradient descent (alpha = 0.01, beta = 0.5) over the same amount of iterations. The momentum gradient descent currently appears to converge slower than the vanilla gradient descent. This may be due to not identifying the correct parameters for the momentum gradient descent.\n\n\n\n\nX_train, y_train = classification_data(n_points= 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points= 50, noise = 0.5, p_dims = 100)\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(50):\n    \n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n    \n\n\n# Calculate training accuracy\ntrain_predictions = LR.predict(X_train)\ntrain_accuracy = (train_predictions == y_train).float().mean().item()\nprint(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n\n# Calculate testing accuracy\ntest_predictions = LR.predict(X_test)\ntest_accuracy = (test_predictions == y_test).float().mean().item()\nprint(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")\n\nTraining Accuracy: 100.00%\nTesting Accuracy: 92.00%\n\n\nHere both the training accuracy and the testing accuracy for their relative sets. The training accuracy was able to achieve 100% accuracy, while the testing accuracy was only able to achieve 92% accuracy. This indicates that the model is overfitting to the training data, though it still has a pretty good accuracy on the test data.\n\n\n\n\nimport kagglehub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Download dataset from Kaggle\npath = kagglehub.dataset_download(\"shantanugarg274/heart-prediction-dataset-quantum\")\nprint(\"Path to dataset files:\", path)\n\ndata_path = path + \"/Heart Prediction Quantum Dataset.csv\"\ndf = pd.read_csv(data_path)\nprint(df.head())\n\nWarning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.11).\nPath to dataset files: /Users/emmanueltowner/.cache/kagglehub/datasets/shantanugarg274/heart-prediction-dataset-quantum/versions/1\n   Age  Gender  BloodPressure  Cholesterol  HeartRate  QuantumPatternFeature  \\\n0   68       1            105          191        107               8.362241   \n1   58       0             97          249         89               9.249002   \n2   44       0             93          190         82               7.942542   \n3   72       1             93          183        101               6.495155   \n4   37       0            145          166        103               7.653900   \n\n   HeartDisease  \n0             1  \n1             0  \n2             1  \n3             1  \n4             1  \n\n\n\n\nX_data = df.drop(\"HeartDisease\", axis=1).values\ny_data = df[\"HeartDisease\"].values\n\nX_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.4, random_state=42)\n\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Convert the numpy arrays to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\nX_val   = torch.tensor(X_val, dtype=torch.float32)\ny_val   = torch.tensor(y_val, dtype=torch.float32)\nX_test  = torch.tensor(X_test, dtype=torch.float32)\ny_test  = torch.tensor(y_test, dtype=torch.float32)\n\n\n\nprint(\"X_train:\", X_train[0:5])\nprint(\"y_train:\", y_train[0:5])\nprint(\"X_val:\", X_val[0:5])\nprint(\"y_val:\", y_val[0:5])\n\n\nX_train: tensor([[ 30.0000,   1.0000, 109.0000, 246.0000,  62.0000,   7.6140],\n        [ 69.0000,   1.0000, 157.0000, 161.0000, 118.0000,   7.8521],\n        [ 35.0000,   0.0000, 118.0000, 208.0000,  61.0000,   8.1145],\n        [ 42.0000,   0.0000, 102.0000, 201.0000,  71.0000,   7.1339],\n        [ 58.0000,   0.0000, 133.0000, 211.0000,  99.0000,   8.9405]])\ny_train: tensor([1., 1., 1., 1., 0.])\nX_val: tensor([[ 37.0000,   1.0000, 111.0000, 231.0000,  73.0000,   8.6168],\n        [ 73.0000,   1.0000, 174.0000, 161.0000,  99.0000,   7.9447],\n        [ 51.0000,   1.0000, 148.0000, 199.0000,  76.0000,   8.8898],\n        [ 57.0000,   0.0000,  97.0000, 188.0000, 103.0000,   7.9620],\n        [ 64.0000,   0.0000, 164.0000, 188.0000, 109.0000,   8.3512]])\ny_val: tensor([1., 1., 0., 1., 0.])\n\n\n\nimport matplotlib.pyplot as plt\nimport torch\n\n# Vanilla gradient descent\nLRV = LogisticRegression() \noptv = GradientDescentOptimizer(LRV)\n\nv_loss_train = []\nv_loss_val = []\n\niterations = 100\n\nfor _ in range(iterations):\n    train_loss = LRV.loss(X_train, y_train)\n    val_loss = LRV.loss(X_val, y_val)\n    v_loss_train.append(train_loss)\n    v_loss_val.append(val_loss)\n    optv.step(X_train, y_train, alpha=0.1, beta=0.0)  # baseline\n\n# Momentum version\nLRM = LogisticRegression() \noptm = GradientDescentOptimizer(LRM)\n\nm_loss_train = []\nm_loss_val = []\nfor _ in range(iterations):\n    train_loss = LRM.loss(X_train, y_train)\n    val_loss = LRM.loss(X_val, y_val)\n    m_loss_train.append(train_loss)\n    m_loss_val.append(val_loss)\n    optm.step(X_train, y_train, alpha=0.01, beta=0.9)  # momentum version\n\n\nplt.plot(torch.arange(1, iterations + 1), v_loss_train, color=\"black\", label=\"Vanilla (alpha=0.1, beta=0.0)\")\nplt.plot(torch.arange(1, iterations + 1), m_loss_train, color=\"orange\", label=\"Momentum (alpha=0.01, beta=0.5)\")\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Training Loss (binary cross entropy)\")\nplt.legend()\nplt.title(\"Training Loss\")\n\nText(0.5, 1.0, 'Training Loss')\n\n\n\n\n\n\n\n\n\n\nplt.plot(torch.arange(1, iterations + 1), v_loss_val, color=\"black\", label=\"Vanilla (alpha=0.1, beta=0.0)\")\nplt.plot(torch.arange(1, iterations + 1), m_loss_val, color=\"orange\", label=\"Momentum (alpha=0.01, beta=0.5)\")\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Validation Loss (binary cross entropy)\")\nplt.legend()\nplt.title(\"Validation Loss\")\n\nText(0.5, 1.0, 'Validation Loss')"
  },
  {
    "objectID": "posts/blog5/index.html#abstract",
    "href": "posts/blog5/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Link to logistic source code.\nThis notebook implements a logistic regression model with a gradient descent optimizer. The model is trained on a synthetic dataset, and in where it is optimized at each iteration and also updates loss. There are four experiments conducted. The first experiment tested the model on with vanilla gradient descent plotting the loss per iteration and a decision boundary. The second experiment compared the loss per iterations between the model when using the vanilla descent and when using momentum descent. The third experiment was to overfit the model to the training data and compare it to the accuracy of the model on the test data. The fourth experiment was to test the model on a heart disease prediction dateset. The dataset was split into training, validation, and test data. The model was trained on the training data and the loss computed for both training and validation. The model was then evaluated on the test data, and the accuracy was reported.\n\n%load_ext autoreload\n%autoreload 2\n\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\n\n\nimport numpy as np\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nThis code was adapted from previous notes.\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, X, y, x_min, x_max, **kwargs):\n    fig, ax = plt.subplots(1, 1)\n    plot_classification_data(X, y, ax)\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    l = ax.plot(x, y, **kwargs)"
  },
  {
    "objectID": "posts/blog5/index.html#experiments",
    "href": "posts/blog5/index.html#experiments",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "LR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\nfor _ in range(100):\n    \n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n    \n    # add other stuff to e.g. keep track of the loss over time. \n    opt.step(X, y, alpha = 0.1, beta = 0)\n\n  \nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\ndraw_line(opt.model.w, X, y, x_min = -0.5, x_max = 1.5, color = \"black\", linestyle = \"dashed\")\n\n\n\n\n\n\n\n\nThe code above implements a vanilla gradient descent with logistic regression. It is run through a training loop, while keeping track of the loss and storing it in an array called loss_vec. Using loss_vec, I plot a graph showing the loss over gradient iterations. The second graph shows the decision boundary of the data. - Loss: This appears to start at ~0.67 and decreases to ~0.33 over the course of 100 iterations. - Decision Boundary: The decision boundary appears to correctly classify the data.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport torch\n\n# Vanilla gradient descent\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec1 = []\niterations = 3000\n\nfor _ in range(iterations):\n    loss = LR.loss(X, y)\n    loss_vec1.append(loss)\n    opt.step(X, y, alpha=0.1, beta=0.0)  # baseline\n\n# Momentum version\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec2 = []\nfor _ in range(iterations):\n    loss = LR.loss(X, y)\n    loss_vec2.append(loss)\n    opt.step(X, y, alpha=0.01, beta=0.5)  # momentum version\n\n\nplt.plot(torch.arange(1, len(loss_vec1) + 1), loss_vec1, color=\"black\", label=\"alpha=0.1, beta=0.0\")\nplt.plot(torch.arange(1, len(loss_vec2) + 1), loss_vec2, color=\"orange\", label=\"alpha=0.01, beta=0.5\")\n\nplt.semilogx()\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Loss (binary cross entropy)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe code above plots the loss over iterations for both vanilla gradient descent and momentum gradient descent (alpha = 0.01, beta = 0.5) over the same amount of iterations. The momentum gradient descent currently appears to converge slower than the vanilla gradient descent. This may be due to not identifying the correct parameters for the momentum gradient descent.\n\n\n\n\nX_train, y_train = classification_data(n_points= 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points= 50, noise = 0.5, p_dims = 100)\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(50):\n    \n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n    \n\n\n# Calculate training accuracy\ntrain_predictions = LR.predict(X_train)\ntrain_accuracy = (train_predictions == y_train).float().mean().item()\nprint(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n\n# Calculate testing accuracy\ntest_predictions = LR.predict(X_test)\ntest_accuracy = (test_predictions == y_test).float().mean().item()\nprint(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")\n\nTraining Accuracy: 100.00%\nTesting Accuracy: 92.00%\n\n\nHere both the training accuracy and the testing accuracy for their relative sets. The training accuracy was able to achieve 100% accuracy, while the testing accuracy was only able to achieve 92% accuracy. This indicates that the model is overfitting to the training data, though it still has a pretty good accuracy on the test data.\n\n\n\n\nimport kagglehub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Download dataset from Kaggle\npath = kagglehub.dataset_download(\"shantanugarg274/heart-prediction-dataset-quantum\")\nprint(\"Path to dataset files:\", path)\n\ndata_path = path + \"/Heart Prediction Quantum Dataset.csv\"\ndf = pd.read_csv(data_path)\nprint(df.head())\n\nWarning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.11).\nPath to dataset files: /Users/emmanueltowner/.cache/kagglehub/datasets/shantanugarg274/heart-prediction-dataset-quantum/versions/1\n   Age  Gender  BloodPressure  Cholesterol  HeartRate  QuantumPatternFeature  \\\n0   68       1            105          191        107               8.362241   \n1   58       0             97          249         89               9.249002   \n2   44       0             93          190         82               7.942542   \n3   72       1             93          183        101               6.495155   \n4   37       0            145          166        103               7.653900   \n\n   HeartDisease  \n0             1  \n1             0  \n2             1  \n3             1  \n4             1  \n\n\n\n\nX_data = df.drop(\"HeartDisease\", axis=1).values\ny_data = df[\"HeartDisease\"].values\n\nX_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.4, random_state=42)\n\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Convert the numpy arrays to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\nX_val   = torch.tensor(X_val, dtype=torch.float32)\ny_val   = torch.tensor(y_val, dtype=torch.float32)\nX_test  = torch.tensor(X_test, dtype=torch.float32)\ny_test  = torch.tensor(y_test, dtype=torch.float32)\n\n\n\nprint(\"X_train:\", X_train[0:5])\nprint(\"y_train:\", y_train[0:5])\nprint(\"X_val:\", X_val[0:5])\nprint(\"y_val:\", y_val[0:5])\n\n\nX_train: tensor([[ 30.0000,   1.0000, 109.0000, 246.0000,  62.0000,   7.6140],\n        [ 69.0000,   1.0000, 157.0000, 161.0000, 118.0000,   7.8521],\n        [ 35.0000,   0.0000, 118.0000, 208.0000,  61.0000,   8.1145],\n        [ 42.0000,   0.0000, 102.0000, 201.0000,  71.0000,   7.1339],\n        [ 58.0000,   0.0000, 133.0000, 211.0000,  99.0000,   8.9405]])\ny_train: tensor([1., 1., 1., 1., 0.])\nX_val: tensor([[ 37.0000,   1.0000, 111.0000, 231.0000,  73.0000,   8.6168],\n        [ 73.0000,   1.0000, 174.0000, 161.0000,  99.0000,   7.9447],\n        [ 51.0000,   1.0000, 148.0000, 199.0000,  76.0000,   8.8898],\n        [ 57.0000,   0.0000,  97.0000, 188.0000, 103.0000,   7.9620],\n        [ 64.0000,   0.0000, 164.0000, 188.0000, 109.0000,   8.3512]])\ny_val: tensor([1., 1., 0., 1., 0.])\n\n\n\nimport matplotlib.pyplot as plt\nimport torch\n\n# Vanilla gradient descent\nLRV = LogisticRegression() \noptv = GradientDescentOptimizer(LRV)\n\nv_loss_train = []\nv_loss_val = []\n\niterations = 100\n\nfor _ in range(iterations):\n    train_loss = LRV.loss(X_train, y_train)\n    val_loss = LRV.loss(X_val, y_val)\n    v_loss_train.append(train_loss)\n    v_loss_val.append(val_loss)\n    optv.step(X_train, y_train, alpha=0.1, beta=0.0)  # baseline\n\n# Momentum version\nLRM = LogisticRegression() \noptm = GradientDescentOptimizer(LRM)\n\nm_loss_train = []\nm_loss_val = []\nfor _ in range(iterations):\n    train_loss = LRM.loss(X_train, y_train)\n    val_loss = LRM.loss(X_val, y_val)\n    m_loss_train.append(train_loss)\n    m_loss_val.append(val_loss)\n    optm.step(X_train, y_train, alpha=0.01, beta=0.9)  # momentum version\n\n\nplt.plot(torch.arange(1, iterations + 1), v_loss_train, color=\"black\", label=\"Vanilla (alpha=0.1, beta=0.0)\")\nplt.plot(torch.arange(1, iterations + 1), m_loss_train, color=\"orange\", label=\"Momentum (alpha=0.01, beta=0.5)\")\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Training Loss (binary cross entropy)\")\nplt.legend()\nplt.title(\"Training Loss\")\n\nText(0.5, 1.0, 'Training Loss')\n\n\n\n\n\n\n\n\n\n\nplt.plot(torch.arange(1, iterations + 1), v_loss_val, color=\"black\", label=\"Vanilla (alpha=0.1, beta=0.0)\")\nplt.plot(torch.arange(1, iterations + 1), m_loss_val, color=\"orange\", label=\"Momentum (alpha=0.01, beta=0.5)\")\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Validation Loss (binary cross entropy)\")\nplt.legend()\nplt.title(\"Validation Loss\")\n\nText(0.5, 1.0, 'Validation Loss')"
  },
  {
    "objectID": "posts/blog4/index.html#experiment-1",
    "href": "posts/blog4/index.html#experiment-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "This plots the decision boundary and the data points over time.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    \n    # Making sure we don't run out of axes\n    if current_ax &gt;= len(axarr.ravel()):\n        print(\"Ran out of axes to plot. Stopping early.\")\n        break\n    \n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    \n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\nRan out of axes to plot. Stopping early."
  },
  {
    "objectID": "posts/blog4/index.html#experiment-2",
    "href": "posts/blog4/index.html#experiment-2",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "torch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nfig, axarr = plt.subplots(1, 2, figsize=(12, 6))\ncurrent_ax = 0\n\nloss = 1\nscore_vec = [] \n\nwhile loss &gt; 0 and len(score_vec) &lt;= 1000:\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    score = p.score(X).mean()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    \n    if local_loss &gt; 0:\n        loss = p.loss(X, y).item()\n        score = p.score(X).mean()\n        score_vec.append(score)\n\n# plot the score over iterations\naxarr[0].plot(range(len(score_vec)), score_vec, color=\"steelblue\", label=\"Score\")\naxarr[0].set_title(\"Score vs. Iterations\")\naxarr[0].set_xlabel(\"Iteration\")\naxarr[0].set_ylabel(\"Score\")\n\n# Plot the final decision boundary\nplot_perceptron_data(X, y, axarr[1])\ndraw_line(p.w, x_min=-1, x_max=2, ax=axarr[1], color=\"black\")\naxarr[1].scatter(X[i, 0], X[i, 1], color=\"black\", facecolors=\"none\", edgecolors=\"black\")\naxarr[1].set_title(f\"Final Decision Boundary (Loss = {loss:.3f})\")\naxarr[1].set(xlim=(-1, 2), ylim=(-1, 2))\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/blog4/index.html#experiment-3",
    "href": "posts/blog4/index.html#experiment-3",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "X, y = perceptron_data(n_points = 300, noise = 0.2, p_dims=5)\n\ntorch.Size([300, 6])\n\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss = 1\nscore_vec = [] \n\nwhile loss &gt; 0 and len(score_vec) &lt;= 1000:\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    prev_length = len(score_vec)\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    score = p.score(X).mean()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    \n    if local_loss &gt; 0:\n        loss = p.loss(X, y).item()\n        score = p.score(X).mean()\n        score_vec.append(score)\n    \n    if(len(score_vec) != prev_length):\n        print(f\"Iteration {len(score_vec)}: Loss = {loss:.3f}, Score = {score:.3f}\")\n\nIteration 1: Loss = 0.027, Score = 0.699\nIteration 2: Loss = 0.247, Score = -0.777\nIteration 3: Loss = 0.103, Score = 2.712\nIteration 4: Loss = 0.003, Score = 1.483\nIteration 5: Loss = 0.000, Score = -0.228"
  },
  {
    "objectID": "posts/blog6/index.html",
    "href": "posts/blog6/index.html",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "In this blog post, I explore deep learning technique of overfitting and the resulting double descent. First, I created the a linear regression model and an overparameterized optimizer that could fit the model to the data. For the first visualization, I tested model prediction after having it fitted to the data. Then I used my model to calculate the number of corruptions in a image. To measure model performance, I calculated mean squared error (MSE) and graphed how it changes as the number features increased. In this visualization, I observed double descent on the testing set. I found the optimal number of features was beyond the interpolation threshold at 188 features and a MSE of 280.\n\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nimport torch\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X: torch.Tensor):\n        return self.activation(X @ self.u + self.b)\n\n\n\n\n\nIf p &gt; n then (X^T X)^{-1} is not invertible. The matrix is rank deficient.\n\n\n\n\nfrom MyLinearRegression import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nphi = RandomFeatures(n_features= 100)\nphi.fit(X)\nX_train_features = phi.transform(X)\n\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\nopt.fit(X_train_features, y)\npred = LR.predict(X_train_features)\nline = pred.numpy() \n\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.plot(X, line, color='lightblue', label=\"Predictions\")\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n\n\n\nn_features = 200\n\ntrain_loss_vec = []\ntest_loss_vec = []\n\nn = X_train.size()[0] \nfor i in range(n_features):\n    phi = RandomFeatures(n_features = i, activation = square)\n    phi.fit(X_train)\n    \n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n    \n    LR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(LR)\n    opt.fit(X_train_phi, y_train)\n \n    train_loss = LR.loss(X_train_phi, y_train).item()\n    test_loss = LR.loss(X_test_phi, y_test).item()\n\n    train_loss_vec.append(train_loss)\n    test_loss_vec.append(test_loss)\n   \nfig, ax = plt.subplots(1, 2, figsize=(14, 6))  \nax[0].scatter(range(n_features), train_loss_vec, color='darkgrey')\nax[0].set_yscale('log')\nax[0].axvline(n, color='black', linewidth = 1)\nax[0].set_xlabel(\"Number of features\")\nax[0].set_ylabel(\"Mean Squared Error (Training)\")\n\nax[1].scatter(range(n_features), test_loss_vec, color='darkred')\nax[1].set_yscale('log')\nax[1].axvline(n, color='black', linewidth = 1)\nax[1].set_xlabel(\"Number of features\")\nax[1].set_ylabel(\"Mean Squared Error (Testing)\")\n\nbest_idx = torch.tensor(test_loss_vec).argmin().item() # convert vector to tensor and get index of minimum\nbest_features = best_idx + 1 \nbest_loss = test_loss_vec[best_idx]\n\nprint(f\"Lowest test loss: {best_loss:.4f} at {best_features} features.\")\n\nLowest test loss: 279.7244 at 188 features.\n\n\n\n\n\n\n\n\n\nThe best error rate for the testing set was around 280 at 188 features. This was after the interpolation threshold.\n\n\n\n\nMy most important finding was the that for the best mse is after the interpolation threshold. For the testing set it was 280 at 188 features. This blog post was my first time to make a fit function for a machine learning model. I also improved my overall graphing skills like a graphing lines on scatter plots and for the interpolation methods."
  },
  {
    "objectID": "posts/blog6/index.html#abstract",
    "href": "posts/blog6/index.html#abstract",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "In this blog post, I explore deep learning technique of overfitting and the resulting double descent. First, I created the a linear regression model and an overparameterized optimizer that could fit the model to the data. For the first visualization, I tested model prediction after having it fitted to the data. Then I used my model to calculate the number of corruptions in a image. To measure model performance, I calculated mean squared error (MSE) and graphed how it changes as the number features increased. In this visualization, I observed double descent on the testing set. I found the optimal number of features was beyond the interpolation threshold at 188 features and a MSE of 280.\n\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nimport torch\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X: torch.Tensor):\n        return self.activation(X @ self.u + self.b)"
  },
  {
    "objectID": "posts/blog6/index.html#part-0",
    "href": "posts/blog6/index.html#part-0",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "If p &gt; n then (X^T X)^{-1} is not invertible. The matrix is rank deficient."
  },
  {
    "objectID": "posts/blog6/index.html#testing-mylinearregression-model",
    "href": "posts/blog6/index.html#testing-mylinearregression-model",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "from MyLinearRegression import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nphi = RandomFeatures(n_features= 100)\nphi.fit(X)\nX_train_features = phi.transform(X)\n\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\nopt.fit(X_train_features, y)\npred = LR.predict(X_train_features)\nline = pred.numpy() \n\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.plot(X, line, color='lightblue', label=\"Predictions\")"
  },
  {
    "objectID": "posts/blog6/index.html#corrupted-flower-images",
    "href": "posts/blog6/index.html#corrupted-flower-images",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "from sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n\n\n\nn_features = 200\n\ntrain_loss_vec = []\ntest_loss_vec = []\n\nn = X_train.size()[0] \nfor i in range(n_features):\n    phi = RandomFeatures(n_features = i, activation = square)\n    phi.fit(X_train)\n    \n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n    \n    LR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(LR)\n    opt.fit(X_train_phi, y_train)\n \n    train_loss = LR.loss(X_train_phi, y_train).item()\n    test_loss = LR.loss(X_test_phi, y_test).item()\n\n    train_loss_vec.append(train_loss)\n    test_loss_vec.append(test_loss)\n   \nfig, ax = plt.subplots(1, 2, figsize=(14, 6))  \nax[0].scatter(range(n_features), train_loss_vec, color='darkgrey')\nax[0].set_yscale('log')\nax[0].axvline(n, color='black', linewidth = 1)\nax[0].set_xlabel(\"Number of features\")\nax[0].set_ylabel(\"Mean Squared Error (Training)\")\n\nax[1].scatter(range(n_features), test_loss_vec, color='darkred')\nax[1].set_yscale('log')\nax[1].axvline(n, color='black', linewidth = 1)\nax[1].set_xlabel(\"Number of features\")\nax[1].set_ylabel(\"Mean Squared Error (Testing)\")\n\nbest_idx = torch.tensor(test_loss_vec).argmin().item() # convert vector to tensor and get index of minimum\nbest_features = best_idx + 1 \nbest_loss = test_loss_vec[best_idx]\n\nprint(f\"Lowest test loss: {best_loss:.4f} at {best_features} features.\")\n\nLowest test loss: 279.7244 at 188 features.\n\n\n\n\n\n\n\n\n\nThe best error rate for the testing set was around 280 at 188 features. This was after the interpolation threshold."
  },
  {
    "objectID": "posts/blog6/index.html#discussion",
    "href": "posts/blog6/index.html#discussion",
    "title": "Overfitting and Double Descents",
    "section": "",
    "text": "My most important finding was the that for the best mse is after the interpolation threshold. For the testing set it was 280 at 188 features. This blog post was my first time to make a fit function for a machine learning model. I also improved my overall graphing skills like a graphing lines on scatter plots and for the interpolation methods."
  },
  {
    "objectID": "posts/blog4/index.html#abstract",
    "href": "posts/blog4/index.html#abstract",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Link to perceptron source code.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nCells 1 to 7 has been directly adapted from Prof. Phil’s code. Cell 8 is slightly modified from the original code. The code below in the sets up the necessary functions to implement a perceptron algorithm.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X: torch.Tensor, y: torch.Tensor, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w: torch.Tensor, x_min: int, x_max: int, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\nThis tests the Perceptron to see if the loss does reach 0.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")"
  }
]